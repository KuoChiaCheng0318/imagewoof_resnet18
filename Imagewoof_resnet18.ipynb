{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FellowShipAI_Imagewoof3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "axNfBPMhYQLo",
        "outputId": "adca7050-1124-499d-e252-ac3a89884ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gGI-DZqWuTJ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2,preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#locate the file path\n",
        "im_size = 224\n",
        "train_x = []\n",
        "train_y= []\n",
        "val_x = []\n",
        "val_y= []\n",
        "dict_label={}\n",
        "import os \n",
        "trainfolder_list = os.listdir(\"/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/train\")\n",
        "valfolder_list = os.listdir(\"/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/val\")\n",
        "i=0\n",
        "for folder in trainfolder_list:\n",
        "  dict_label[folder] = i\n",
        "  i+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "O-YbGeP-acUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read training data\n",
        "for folder in tqdm(trainfolder_list):\n",
        "  for filename in os.listdir('/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/train/'+folder):\n",
        "    img = cv2.resize(cv2.imread('/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/train/'+folder+'/'+filename,cv2.IMREAD_COLOR),((im_size,im_size)))\n",
        "    img_array = preprocess_input(np.expand_dims(np.array(img[...,::-1].astype(np.float32)).copy(), axis=0))\n",
        "    train_x.append(img_array.reshape(3,im_size,im_size))\n",
        "    train_y.append(dict_label[folder])"
      ],
      "metadata": {
        "id": "S7TYxOldtgbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7209f6-445a-4838-fcff-cc7a12a89fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:03<00:00, 12.33s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read validation data\n",
        "for folder in tqdm(valfolder_list):\n",
        "  for filename in os.listdir('/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/val/'+folder):\n",
        "    img = cv2.resize(cv2.imread('/content/gdrive/MyDrive/FindCareer/FellowshipAI/imagewoof-320/val/'+folder+'/'+filename,cv2.IMREAD_COLOR),((im_size,im_size)))\n",
        "    img_array = preprocess_input(np.expand_dims(np.array(img[...,::-1].astype(np.float32)).copy(), axis=0))\n",
        "    val_x.append(img_array.reshape(3,im_size,im_size))\n",
        "    val_y.append(dict_label[folder])"
      ],
      "metadata": {
        "id": "z6WQEvZDx_SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b80ea33-02b8-48f5-c585-3c9cb7905ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EG2ik09XFOGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just checking the data\n",
        "plt.imshow(train_x[150].reshape(224,224,3))\n",
        "plt.show()\n",
        "print(train_y[150])\n",
        "print(dict_label)\n",
        "\n",
        "print(\"images-size:\", train_x[0].shape)\n",
        "print(len(train_y))\n",
        "print(len(val_y))\n",
        "print(len(test_y))\n",
        "print(val_y)"
      ],
      "metadata": {
        "id": "GzQ8OiiynEpQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "0469c782-70b9-430e-f38c-06600bb729bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "{'n02089973': 0, 'n02115641': 1, 'n02111889': 2, 'n02093754': 3, 'n02099601': 4, 'n02105641': 5, 'n02096294': 6, 'n02087394': 7, 'n02086240': 8, 'n02088364': 9}\n",
            "images-size: (3, 224, 224)\n",
            "9971\n",
            "500\n",
            "2493\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "batchSize = 32\n",
        "dataset=list(zip(train_x, train_y))\n",
        "trainloader = DataLoader(dataset, batch_size = batchSize, shuffle=True)"
      ],
      "metadata": {
        "id": "VwsuKEOIR5m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validataset=list(zip(val_x, val_y))\n",
        "valiloader = DataLoader(validataset, batch_size = batchSize, shuffle=True)"
      ],
      "metadata": {
        "id": "WXqGCckhF9sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset=list(zip(test_x, test_y))\n",
        "testloader = DataLoader(testset, batch_size = batchSize, shuffle=True)"
      ],
      "metadata": {
        "id": "LkCjAlZOFtjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import *\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net = models.resnet18(pretrained=True)\n",
        "net = net.cuda() if device else net\n",
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-uHSAN0IUl9",
        "outputId": "ed913f7c-0245-4790-bdc7-2b415aef7204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "def accuracy(out, labels):\n",
        "  _,pred = torch.max(out, dim=1)\n",
        "  return torch.sum(pred==labels).item()\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 128)\n",
        "net.fc = net.fc.cuda() if torch.cuda.is_available() else net.fc"
      ],
      "metadata": {
        "id": "RGs-JnlmIXd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 200\n",
        "print_every = 10\n",
        "valid_loss_min = np.Inf\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "total_step = len(trainloader)\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total=0\n",
        "  print(f'Epoch {epoch}\\n')\n",
        "  for batch_idx, (data_, target_) in enumerate(trainloader):\n",
        "    data_, target_ = data_.to(device), target_.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(data_)\n",
        "    loss = criterion(outputs, target_)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    _,pred = torch.max(outputs, dim=1)\n",
        "    correct += torch.sum(pred==target_).item()\n",
        "    total += target_.size(0)\n",
        "    if (batch_idx) % 20 == 0:\n",
        "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "              .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "  train_acc.append(100 * correct / total)\n",
        "  train_loss.append(running_loss/total_step)\n",
        "  print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "  batch_loss = 0\n",
        "  total_t=0\n",
        "  correct_t=0\n",
        "  with torch.no_grad():\n",
        "    net.eval()\n",
        "    for data_t, target_t in (testloader):\n",
        "      data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "      outputs_t = net(data_t)\n",
        "      loss_t = criterion(outputs_t, target_t)\n",
        "      batch_loss += loss_t.item()\n",
        "      _,pred_t = torch.max(outputs_t, dim=1)\n",
        "      correct_t += torch.sum(pred_t==target_t).item()\n",
        "      total_t += target_t.size(0)\n",
        "    val_acc.append(100 * correct_t/total_t)\n",
        "    val_loss.append(batch_loss/len(testloader))\n",
        "    network_learned = batch_loss < valid_loss_min\n",
        "    print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "    if network_learned:\n",
        "      valid_loss_min = batch_loss\n",
        "      torch.save(net.state_dict(), 'resnet.pt')\n",
        "      print('Improvement-Detected, save-model')\n",
        "\n",
        "\n",
        "  net.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQKwxpJGIacA",
        "outputId": "18b73709-1d33-4440-e800-0058ded577dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "\n",
            "Epoch [1/200], Step [0/312], Loss: 5.2945\n",
            "Epoch [1/200], Step [20/312], Loss: 5.2177\n",
            "Epoch [1/200], Step [40/312], Loss: 5.0701\n",
            "Epoch [1/200], Step [60/312], Loss: 5.1168\n",
            "Epoch [1/200], Step [80/312], Loss: 5.1485\n",
            "Epoch [1/200], Step [100/312], Loss: 5.1567\n",
            "Epoch [1/200], Step [120/312], Loss: 5.0575\n",
            "Epoch [1/200], Step [140/312], Loss: 5.1182\n",
            "Epoch [1/200], Step [160/312], Loss: 5.3125\n",
            "Epoch [1/200], Step [180/312], Loss: 5.0191\n",
            "Epoch [1/200], Step [200/312], Loss: 5.1787\n",
            "Epoch [1/200], Step [220/312], Loss: 5.0047\n",
            "Epoch [1/200], Step [240/312], Loss: 5.0725\n",
            "Epoch [1/200], Step [260/312], Loss: 5.0997\n",
            "Epoch [1/200], Step [280/312], Loss: 5.2080\n",
            "Epoch [1/200], Step [300/312], Loss: 4.9834\n",
            "\n",
            "train-loss: 5.1032, train-acc: 0.2206\n",
            "validation loss: 5.0082, validation acc: 0.4813\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 2\n",
            "\n",
            "Epoch [2/200], Step [0/312], Loss: 5.0730\n",
            "Epoch [2/200], Step [20/312], Loss: 4.8301\n",
            "Epoch [2/200], Step [40/312], Loss: 4.8943\n",
            "Epoch [2/200], Step [60/312], Loss: 4.9916\n",
            "Epoch [2/200], Step [80/312], Loss: 5.0898\n",
            "Epoch [2/200], Step [100/312], Loss: 4.9159\n",
            "Epoch [2/200], Step [120/312], Loss: 4.9604\n",
            "Epoch [2/200], Step [140/312], Loss: 5.0017\n",
            "Epoch [2/200], Step [160/312], Loss: 4.8191\n",
            "Epoch [2/200], Step [180/312], Loss: 4.9980\n",
            "Epoch [2/200], Step [200/312], Loss: 4.9156\n",
            "Epoch [2/200], Step [220/312], Loss: 4.8459\n",
            "Epoch [2/200], Step [240/312], Loss: 4.7125\n",
            "Epoch [2/200], Step [260/312], Loss: 4.8236\n",
            "Epoch [2/200], Step [280/312], Loss: 4.7720\n",
            "Epoch [2/200], Step [300/312], Loss: 4.7647\n",
            "\n",
            "train-loss: 4.9994, train-acc: 1.9055\n",
            "validation loss: 4.9092, validation acc: 4.7734\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 3\n",
            "\n",
            "Epoch [3/200], Step [0/312], Loss: 4.8815\n",
            "Epoch [3/200], Step [20/312], Loss: 4.6747\n",
            "Epoch [3/200], Step [40/312], Loss: 4.8179\n",
            "Epoch [3/200], Step [60/312], Loss: 4.5073\n",
            "Epoch [3/200], Step [80/312], Loss: 4.7630\n",
            "Epoch [3/200], Step [100/312], Loss: 4.8134\n",
            "Epoch [3/200], Step [120/312], Loss: 4.5645\n",
            "Epoch [3/200], Step [140/312], Loss: 4.8515\n",
            "Epoch [3/200], Step [160/312], Loss: 4.7804\n",
            "Epoch [3/200], Step [180/312], Loss: 4.6670\n",
            "Epoch [3/200], Step [200/312], Loss: 4.7506\n",
            "Epoch [3/200], Step [220/312], Loss: 4.7130\n",
            "Epoch [3/200], Step [240/312], Loss: 4.4626\n",
            "Epoch [3/200], Step [260/312], Loss: 4.3190\n",
            "Epoch [3/200], Step [280/312], Loss: 4.4975\n",
            "Epoch [3/200], Step [300/312], Loss: 4.3197\n",
            "\n",
            "train-loss: 4.8865, train-acc: 8.9961\n",
            "validation loss: 4.7896, validation acc: 13.2772\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 4\n",
            "\n",
            "Epoch [4/200], Step [0/312], Loss: 4.5529\n",
            "Epoch [4/200], Step [20/312], Loss: 4.6668\n",
            "Epoch [4/200], Step [40/312], Loss: 4.4448\n",
            "Epoch [4/200], Step [60/312], Loss: 4.8288\n",
            "Epoch [4/200], Step [80/312], Loss: 4.2674\n",
            "Epoch [4/200], Step [100/312], Loss: 4.4970\n",
            "Epoch [4/200], Step [120/312], Loss: 4.6341\n",
            "Epoch [4/200], Step [140/312], Loss: 4.6515\n",
            "Epoch [4/200], Step [160/312], Loss: 4.3864\n",
            "Epoch [4/200], Step [180/312], Loss: 4.4945\n",
            "Epoch [4/200], Step [200/312], Loss: 4.2776\n",
            "Epoch [4/200], Step [220/312], Loss: 4.5120\n",
            "Epoch [4/200], Step [240/312], Loss: 4.3524\n",
            "Epoch [4/200], Step [260/312], Loss: 4.5879\n",
            "Epoch [4/200], Step [280/312], Loss: 4.0086\n",
            "Epoch [4/200], Step [300/312], Loss: 4.3648\n",
            "\n",
            "train-loss: 4.7683, train-acc: 16.1167\n",
            "validation loss: 4.6699, validation acc: 18.9731\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 5\n",
            "\n",
            "Epoch [5/200], Step [0/312], Loss: 4.2556\n",
            "Epoch [5/200], Step [20/312], Loss: 4.4438\n",
            "Epoch [5/200], Step [40/312], Loss: 3.9130\n",
            "Epoch [5/200], Step [60/312], Loss: 4.3039\n",
            "Epoch [5/200], Step [80/312], Loss: 4.4473\n",
            "Epoch [5/200], Step [100/312], Loss: 4.0886\n",
            "Epoch [5/200], Step [120/312], Loss: 3.8938\n",
            "Epoch [5/200], Step [140/312], Loss: 3.7303\n",
            "Epoch [5/200], Step [160/312], Loss: 4.4503\n",
            "Epoch [5/200], Step [180/312], Loss: 4.8882\n",
            "Epoch [5/200], Step [200/312], Loss: 4.5601\n",
            "Epoch [5/200], Step [220/312], Loss: 3.8682\n",
            "Epoch [5/200], Step [240/312], Loss: 4.1549\n",
            "Epoch [5/200], Step [260/312], Loss: 4.1416\n",
            "Epoch [5/200], Step [280/312], Loss: 4.3495\n",
            "Epoch [5/200], Step [300/312], Loss: 3.8093\n",
            "\n",
            "train-loss: 4.6517, train-acc: 22.2746\n",
            "validation loss: 4.5589, validation acc: 24.3482\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 6\n",
            "\n",
            "Epoch [6/200], Step [0/312], Loss: 3.9070\n",
            "Epoch [6/200], Step [20/312], Loss: 4.0859\n",
            "Epoch [6/200], Step [40/312], Loss: 4.1368\n",
            "Epoch [6/200], Step [60/312], Loss: 4.6256\n",
            "Epoch [6/200], Step [80/312], Loss: 3.8671\n",
            "Epoch [6/200], Step [100/312], Loss: 4.0241\n",
            "Epoch [6/200], Step [120/312], Loss: 3.6159\n",
            "Epoch [6/200], Step [140/312], Loss: 3.8178\n",
            "Epoch [6/200], Step [160/312], Loss: 3.8659\n",
            "Epoch [6/200], Step [180/312], Loss: 3.9111\n",
            "Epoch [6/200], Step [200/312], Loss: 3.6110\n",
            "Epoch [6/200], Step [220/312], Loss: 3.6498\n",
            "Epoch [6/200], Step [240/312], Loss: 4.0359\n",
            "Epoch [6/200], Step [260/312], Loss: 3.7168\n",
            "Epoch [6/200], Step [280/312], Loss: 3.8959\n",
            "Epoch [6/200], Step [300/312], Loss: 3.8314\n",
            "\n",
            "train-loss: 4.5418, train-acc: 26.3464\n",
            "validation loss: 4.4558, validation acc: 27.5170\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 7\n",
            "\n",
            "Epoch [7/200], Step [0/312], Loss: 3.9334\n",
            "Epoch [7/200], Step [20/312], Loss: 3.6020\n",
            "Epoch [7/200], Step [40/312], Loss: 4.2071\n",
            "Epoch [7/200], Step [60/312], Loss: 3.5090\n",
            "Epoch [7/200], Step [80/312], Loss: 3.5133\n",
            "Epoch [7/200], Step [100/312], Loss: 3.4842\n",
            "Epoch [7/200], Step [120/312], Loss: 3.6341\n",
            "Epoch [7/200], Step [140/312], Loss: 4.0955\n",
            "Epoch [7/200], Step [160/312], Loss: 3.6118\n",
            "Epoch [7/200], Step [180/312], Loss: 3.6093\n",
            "Epoch [7/200], Step [200/312], Loss: 3.7974\n",
            "Epoch [7/200], Step [220/312], Loss: 3.9557\n",
            "Epoch [7/200], Step [240/312], Loss: 3.4400\n",
            "Epoch [7/200], Step [260/312], Loss: 3.8871\n",
            "Epoch [7/200], Step [280/312], Loss: 3.5396\n",
            "Epoch [7/200], Step [300/312], Loss: 3.9000\n",
            "\n",
            "train-loss: 4.4398, train-acc: 29.9368\n",
            "validation loss: 4.3622, validation acc: 31.0068\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 8\n",
            "\n",
            "Epoch [8/200], Step [0/312], Loss: 3.4874\n",
            "Epoch [8/200], Step [20/312], Loss: 3.7414\n",
            "Epoch [8/200], Step [40/312], Loss: 4.0254\n",
            "Epoch [8/200], Step [60/312], Loss: 4.3311\n",
            "Epoch [8/200], Step [80/312], Loss: 3.5504\n",
            "Epoch [8/200], Step [100/312], Loss: 4.1558\n",
            "Epoch [8/200], Step [120/312], Loss: 4.1998\n",
            "Epoch [8/200], Step [140/312], Loss: 3.4233\n",
            "Epoch [8/200], Step [160/312], Loss: 3.9806\n",
            "Epoch [8/200], Step [180/312], Loss: 3.2156\n",
            "Epoch [8/200], Step [200/312], Loss: 3.5038\n",
            "Epoch [8/200], Step [220/312], Loss: 3.8369\n",
            "Epoch [8/200], Step [240/312], Loss: 3.7989\n",
            "Epoch [8/200], Step [260/312], Loss: 3.4730\n",
            "Epoch [8/200], Step [280/312], Loss: 3.5659\n",
            "Epoch [8/200], Step [300/312], Loss: 3.8735\n",
            "\n",
            "train-loss: 4.3438, train-acc: 32.7951\n",
            "validation loss: 4.2742, validation acc: 33.0124\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 9\n",
            "\n",
            "Epoch [9/200], Step [0/312], Loss: 3.4272\n",
            "Epoch [9/200], Step [20/312], Loss: 3.6141\n",
            "Epoch [9/200], Step [40/312], Loss: 3.4453\n",
            "Epoch [9/200], Step [60/312], Loss: 3.7792\n",
            "Epoch [9/200], Step [80/312], Loss: 4.1746\n",
            "Epoch [9/200], Step [100/312], Loss: 3.2977\n",
            "Epoch [9/200], Step [120/312], Loss: 4.2738\n",
            "Epoch [9/200], Step [140/312], Loss: 3.2428\n",
            "Epoch [9/200], Step [160/312], Loss: 4.2315\n",
            "Epoch [9/200], Step [180/312], Loss: 3.1097\n",
            "Epoch [9/200], Step [200/312], Loss: 3.3468\n",
            "Epoch [9/200], Step [220/312], Loss: 4.1469\n",
            "Epoch [9/200], Step [240/312], Loss: 2.9325\n",
            "Epoch [9/200], Step [260/312], Loss: 3.3431\n",
            "Epoch [9/200], Step [280/312], Loss: 3.5230\n",
            "Epoch [9/200], Step [300/312], Loss: 3.4735\n",
            "\n",
            "train-loss: 4.2536, train-acc: 34.1891\n",
            "validation loss: 4.1934, validation acc: 33.3333\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 10\n",
            "\n",
            "Epoch [10/200], Step [0/312], Loss: 3.0423\n",
            "Epoch [10/200], Step [20/312], Loss: 3.3796\n",
            "Epoch [10/200], Step [40/312], Loss: 3.4330\n",
            "Epoch [10/200], Step [60/312], Loss: 3.6247\n",
            "Epoch [10/200], Step [80/312], Loss: 3.2620\n",
            "Epoch [10/200], Step [100/312], Loss: 3.8319\n",
            "Epoch [10/200], Step [120/312], Loss: 3.9807\n",
            "Epoch [10/200], Step [140/312], Loss: 3.7850\n",
            "Epoch [10/200], Step [160/312], Loss: 3.3957\n",
            "Epoch [10/200], Step [180/312], Loss: 3.1630\n",
            "Epoch [10/200], Step [200/312], Loss: 3.1158\n",
            "Epoch [10/200], Step [220/312], Loss: 3.5576\n",
            "Epoch [10/200], Step [240/312], Loss: 3.4083\n",
            "Epoch [10/200], Step [260/312], Loss: 3.4636\n",
            "Epoch [10/200], Step [280/312], Loss: 3.4420\n",
            "Epoch [10/200], Step [300/312], Loss: 3.6915\n",
            "\n",
            "train-loss: 4.1696, train-acc: 35.4027\n",
            "validation loss: 4.1182, validation acc: 34.2158\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 11\n",
            "\n",
            "Epoch [11/200], Step [0/312], Loss: 3.8210\n",
            "Epoch [11/200], Step [20/312], Loss: 3.4550\n",
            "Epoch [11/200], Step [40/312], Loss: 3.5088\n",
            "Epoch [11/200], Step [60/312], Loss: 3.3653\n",
            "Epoch [11/200], Step [80/312], Loss: 3.4490\n",
            "Epoch [11/200], Step [100/312], Loss: 3.4642\n",
            "Epoch [11/200], Step [120/312], Loss: 3.7913\n",
            "Epoch [11/200], Step [140/312], Loss: 3.1545\n",
            "Epoch [11/200], Step [160/312], Loss: 3.3049\n",
            "Epoch [11/200], Step [180/312], Loss: 3.3136\n",
            "Epoch [11/200], Step [200/312], Loss: 3.2092\n",
            "Epoch [11/200], Step [220/312], Loss: 3.4088\n",
            "Epoch [11/200], Step [240/312], Loss: 2.5920\n",
            "Epoch [11/200], Step [260/312], Loss: 4.1155\n",
            "Epoch [11/200], Step [280/312], Loss: 3.3868\n",
            "Epoch [11/200], Step [300/312], Loss: 2.9300\n",
            "\n",
            "train-loss: 4.0890, train-acc: 37.1176\n",
            "validation loss: 4.0505, validation acc: 34.8175\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 12\n",
            "\n",
            "Epoch [12/200], Step [0/312], Loss: 3.4178\n",
            "Epoch [12/200], Step [20/312], Loss: 3.1929\n",
            "Epoch [12/200], Step [40/312], Loss: 2.9561\n",
            "Epoch [12/200], Step [60/312], Loss: 3.5983\n",
            "Epoch [12/200], Step [80/312], Loss: 3.6796\n",
            "Epoch [12/200], Step [100/312], Loss: 3.4917\n",
            "Epoch [12/200], Step [120/312], Loss: 3.0560\n",
            "Epoch [12/200], Step [140/312], Loss: 3.7033\n",
            "Epoch [12/200], Step [160/312], Loss: 3.3442\n",
            "Epoch [12/200], Step [180/312], Loss: 2.8989\n",
            "Epoch [12/200], Step [200/312], Loss: 3.2005\n",
            "Epoch [12/200], Step [220/312], Loss: 3.3102\n",
            "Epoch [12/200], Step [240/312], Loss: 2.4867\n",
            "Epoch [12/200], Step [260/312], Loss: 3.3355\n",
            "Epoch [12/200], Step [280/312], Loss: 2.9544\n",
            "Epoch [12/200], Step [300/312], Loss: 2.7995\n",
            "\n",
            "train-loss: 4.0135, train-acc: 38.1607\n",
            "validation loss: 3.9851, validation acc: 36.5022\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 13\n",
            "\n",
            "Epoch [13/200], Step [0/312], Loss: 3.4150\n",
            "Epoch [13/200], Step [20/312], Loss: 3.1408\n",
            "Epoch [13/200], Step [40/312], Loss: 3.4125\n",
            "Epoch [13/200], Step [60/312], Loss: 2.3952\n",
            "Epoch [13/200], Step [80/312], Loss: 2.5963\n",
            "Epoch [13/200], Step [100/312], Loss: 3.2339\n",
            "Epoch [13/200], Step [120/312], Loss: 2.3947\n",
            "Epoch [13/200], Step [140/312], Loss: 2.6699\n",
            "Epoch [13/200], Step [160/312], Loss: 2.7519\n",
            "Epoch [13/200], Step [180/312], Loss: 3.2887\n",
            "Epoch [13/200], Step [200/312], Loss: 2.5478\n",
            "Epoch [13/200], Step [220/312], Loss: 2.6077\n",
            "Epoch [13/200], Step [240/312], Loss: 2.6146\n",
            "Epoch [13/200], Step [260/312], Loss: 3.2460\n",
            "Epoch [13/200], Step [280/312], Loss: 2.8456\n",
            "Epoch [13/200], Step [300/312], Loss: 2.7567\n",
            "\n",
            "train-loss: 3.9410, train-acc: 39.8456\n",
            "validation loss: 3.9243, validation acc: 37.0638\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 14\n",
            "\n",
            "Epoch [14/200], Step [0/312], Loss: 2.5837\n",
            "Epoch [14/200], Step [20/312], Loss: 2.7876\n",
            "Epoch [14/200], Step [40/312], Loss: 2.9205\n",
            "Epoch [14/200], Step [60/312], Loss: 3.3927\n",
            "Epoch [14/200], Step [80/312], Loss: 2.9943\n",
            "Epoch [14/200], Step [100/312], Loss: 2.9511\n",
            "Epoch [14/200], Step [120/312], Loss: 2.6321\n",
            "Epoch [14/200], Step [140/312], Loss: 3.2239\n",
            "Epoch [14/200], Step [160/312], Loss: 2.7209\n",
            "Epoch [14/200], Step [180/312], Loss: 2.7956\n",
            "Epoch [14/200], Step [200/312], Loss: 2.5582\n",
            "Epoch [14/200], Step [220/312], Loss: 2.6221\n",
            "Epoch [14/200], Step [240/312], Loss: 2.4204\n",
            "Epoch [14/200], Step [260/312], Loss: 3.0383\n",
            "Epoch [14/200], Step [280/312], Loss: 3.3920\n",
            "Epoch [14/200], Step [300/312], Loss: 3.1483\n",
            "\n",
            "train-loss: 3.8727, train-acc: 41.0691\n",
            "validation loss: 3.8681, validation acc: 38.2270\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 15\n",
            "\n",
            "Epoch [15/200], Step [0/312], Loss: 3.3842\n",
            "Epoch [15/200], Step [20/312], Loss: 3.0368\n",
            "Epoch [15/200], Step [40/312], Loss: 2.5001\n",
            "Epoch [15/200], Step [60/312], Loss: 3.4250\n",
            "Epoch [15/200], Step [80/312], Loss: 3.0707\n",
            "Epoch [15/200], Step [100/312], Loss: 2.9760\n",
            "Epoch [15/200], Step [120/312], Loss: 2.7592\n",
            "Epoch [15/200], Step [140/312], Loss: 3.1155\n",
            "Epoch [15/200], Step [160/312], Loss: 2.7271\n",
            "Epoch [15/200], Step [180/312], Loss: 2.8901\n",
            "Epoch [15/200], Step [200/312], Loss: 3.3702\n",
            "Epoch [15/200], Step [220/312], Loss: 2.8583\n",
            "Epoch [15/200], Step [240/312], Loss: 2.7304\n",
            "Epoch [15/200], Step [260/312], Loss: 2.7363\n",
            "Epoch [15/200], Step [280/312], Loss: 2.2427\n",
            "Epoch [15/200], Step [300/312], Loss: 3.1857\n",
            "\n",
            "train-loss: 3.8078, train-acc: 42.3027\n",
            "validation loss: 3.8161, validation acc: 38.1468\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 16\n",
            "\n",
            "Epoch [16/200], Step [0/312], Loss: 3.1031\n",
            "Epoch [16/200], Step [20/312], Loss: 2.5301\n",
            "Epoch [16/200], Step [40/312], Loss: 2.5789\n",
            "Epoch [16/200], Step [60/312], Loss: 2.7428\n",
            "Epoch [16/200], Step [80/312], Loss: 2.9223\n",
            "Epoch [16/200], Step [100/312], Loss: 2.6665\n",
            "Epoch [16/200], Step [120/312], Loss: 3.0424\n",
            "Epoch [16/200], Step [140/312], Loss: 3.4221\n",
            "Epoch [16/200], Step [160/312], Loss: 2.7565\n",
            "Epoch [16/200], Step [180/312], Loss: 3.3691\n",
            "Epoch [16/200], Step [200/312], Loss: 2.8769\n",
            "Epoch [16/200], Step [220/312], Loss: 2.4708\n",
            "Epoch [16/200], Step [240/312], Loss: 3.1504\n",
            "Epoch [16/200], Step [260/312], Loss: 3.0132\n",
            "Epoch [16/200], Step [280/312], Loss: 3.2342\n",
            "Epoch [16/200], Step [300/312], Loss: 3.0538\n",
            "\n",
            "train-loss: 3.7459, train-acc: 43.6666\n",
            "validation loss: 3.7671, validation acc: 39.1496\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 17\n",
            "\n",
            "Epoch [17/200], Step [0/312], Loss: 2.6584\n",
            "Epoch [17/200], Step [20/312], Loss: 2.5609\n",
            "Epoch [17/200], Step [40/312], Loss: 3.1225\n",
            "Epoch [17/200], Step [60/312], Loss: 2.6057\n",
            "Epoch [17/200], Step [80/312], Loss: 2.5380\n",
            "Epoch [17/200], Step [100/312], Loss: 3.2666\n",
            "Epoch [17/200], Step [120/312], Loss: 2.7404\n",
            "Epoch [17/200], Step [140/312], Loss: 2.6453\n",
            "Epoch [17/200], Step [160/312], Loss: 2.8724\n",
            "Epoch [17/200], Step [180/312], Loss: 2.8900\n",
            "Epoch [17/200], Step [200/312], Loss: 2.6770\n",
            "Epoch [17/200], Step [220/312], Loss: 3.1889\n",
            "Epoch [17/200], Step [240/312], Loss: 2.4769\n",
            "Epoch [17/200], Step [260/312], Loss: 2.5844\n",
            "Epoch [17/200], Step [280/312], Loss: 2.2946\n",
            "Epoch [17/200], Step [300/312], Loss: 2.6699\n",
            "\n",
            "train-loss: 3.6855, train-acc: 45.0005\n",
            "validation loss: 3.7198, validation acc: 40.1524\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 18\n",
            "\n",
            "Epoch [18/200], Step [0/312], Loss: 2.6753\n",
            "Epoch [18/200], Step [20/312], Loss: 3.2332\n",
            "Epoch [18/200], Step [40/312], Loss: 2.5844\n",
            "Epoch [18/200], Step [60/312], Loss: 2.7959\n",
            "Epoch [18/200], Step [80/312], Loss: 3.4629\n",
            "Epoch [18/200], Step [100/312], Loss: 2.6305\n",
            "Epoch [18/200], Step [120/312], Loss: 2.5926\n",
            "Epoch [18/200], Step [140/312], Loss: 1.9664\n",
            "Epoch [18/200], Step [160/312], Loss: 2.7094\n",
            "Epoch [18/200], Step [180/312], Loss: 2.7880\n",
            "Epoch [18/200], Step [200/312], Loss: 2.3794\n",
            "Epoch [18/200], Step [220/312], Loss: 2.2571\n",
            "Epoch [18/200], Step [240/312], Loss: 2.3598\n",
            "Epoch [18/200], Step [260/312], Loss: 2.8576\n",
            "Epoch [18/200], Step [280/312], Loss: 2.4361\n",
            "Epoch [18/200], Step [300/312], Loss: 2.1579\n",
            "\n",
            "train-loss: 3.6282, train-acc: 45.9834\n",
            "validation loss: 3.6755, validation acc: 40.1123\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 19\n",
            "\n",
            "Epoch [19/200], Step [0/312], Loss: 2.4415\n",
            "Epoch [19/200], Step [20/312], Loss: 2.3523\n",
            "Epoch [19/200], Step [40/312], Loss: 2.4129\n",
            "Epoch [19/200], Step [60/312], Loss: 2.2995\n",
            "Epoch [19/200], Step [80/312], Loss: 2.4638\n",
            "Epoch [19/200], Step [100/312], Loss: 2.2611\n",
            "Epoch [19/200], Step [120/312], Loss: 2.0050\n",
            "Epoch [19/200], Step [140/312], Loss: 2.3963\n",
            "Epoch [19/200], Step [160/312], Loss: 2.8420\n",
            "Epoch [19/200], Step [180/312], Loss: 2.9416\n",
            "Epoch [19/200], Step [200/312], Loss: 2.4915\n",
            "Epoch [19/200], Step [220/312], Loss: 2.9246\n",
            "Epoch [19/200], Step [240/312], Loss: 2.5319\n",
            "Epoch [19/200], Step [260/312], Loss: 2.3561\n",
            "Epoch [19/200], Step [280/312], Loss: 2.6184\n",
            "Epoch [19/200], Step [300/312], Loss: 2.7754\n",
            "\n",
            "train-loss: 3.5723, train-acc: 47.3975\n",
            "validation loss: 3.6351, validation acc: 41.2355\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 20\n",
            "\n",
            "Epoch [20/200], Step [0/312], Loss: 2.3815\n",
            "Epoch [20/200], Step [20/312], Loss: 3.1048\n",
            "Epoch [20/200], Step [40/312], Loss: 2.4881\n",
            "Epoch [20/200], Step [60/312], Loss: 2.7441\n",
            "Epoch [20/200], Step [80/312], Loss: 2.0676\n",
            "Epoch [20/200], Step [100/312], Loss: 2.1234\n",
            "Epoch [20/200], Step [120/312], Loss: 2.4721\n",
            "Epoch [20/200], Step [140/312], Loss: 2.3891\n",
            "Epoch [20/200], Step [160/312], Loss: 2.2989\n",
            "Epoch [20/200], Step [180/312], Loss: 2.2012\n",
            "Epoch [20/200], Step [200/312], Loss: 2.9006\n",
            "Epoch [20/200], Step [220/312], Loss: 2.6176\n",
            "Epoch [20/200], Step [240/312], Loss: 2.6490\n",
            "Epoch [20/200], Step [260/312], Loss: 2.0517\n",
            "Epoch [20/200], Step [280/312], Loss: 2.4572\n",
            "Epoch [20/200], Step [300/312], Loss: 2.4544\n",
            "\n",
            "train-loss: 3.5187, train-acc: 48.4505\n",
            "validation loss: 3.5957, validation acc: 41.1151\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 21\n",
            "\n",
            "Epoch [21/200], Step [0/312], Loss: 2.4866\n",
            "Epoch [21/200], Step [20/312], Loss: 2.4935\n",
            "Epoch [21/200], Step [40/312], Loss: 2.3315\n",
            "Epoch [21/200], Step [60/312], Loss: 2.9742\n",
            "Epoch [21/200], Step [80/312], Loss: 1.9930\n",
            "Epoch [21/200], Step [100/312], Loss: 2.4120\n",
            "Epoch [21/200], Step [120/312], Loss: 2.1554\n",
            "Epoch [21/200], Step [140/312], Loss: 1.9539\n",
            "Epoch [21/200], Step [160/312], Loss: 2.1854\n",
            "Epoch [21/200], Step [180/312], Loss: 2.2192\n",
            "Epoch [21/200], Step [200/312], Loss: 3.1440\n",
            "Epoch [21/200], Step [220/312], Loss: 2.7839\n",
            "Epoch [21/200], Step [240/312], Loss: 2.5912\n",
            "Epoch [21/200], Step [260/312], Loss: 2.4282\n",
            "Epoch [21/200], Step [280/312], Loss: 2.4785\n",
            "Epoch [21/200], Step [300/312], Loss: 2.7447\n",
            "\n",
            "train-loss: 3.4662, train-acc: 49.9649\n",
            "validation loss: 3.5594, validation acc: 41.3959\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 22\n",
            "\n",
            "Epoch [22/200], Step [0/312], Loss: 2.4455\n",
            "Epoch [22/200], Step [20/312], Loss: 2.3151\n",
            "Epoch [22/200], Step [40/312], Loss: 2.4630\n",
            "Epoch [22/200], Step [60/312], Loss: 2.0015\n",
            "Epoch [22/200], Step [80/312], Loss: 2.4681\n",
            "Epoch [22/200], Step [100/312], Loss: 2.1537\n",
            "Epoch [22/200], Step [120/312], Loss: 2.3793\n",
            "Epoch [22/200], Step [140/312], Loss: 2.7895\n",
            "Epoch [22/200], Step [160/312], Loss: 2.6180\n",
            "Epoch [22/200], Step [180/312], Loss: 2.5805\n",
            "Epoch [22/200], Step [200/312], Loss: 2.1176\n",
            "Epoch [22/200], Step [220/312], Loss: 2.2772\n",
            "Epoch [22/200], Step [240/312], Loss: 2.1057\n",
            "Epoch [22/200], Step [260/312], Loss: 1.8636\n",
            "Epoch [22/200], Step [280/312], Loss: 2.8819\n",
            "Epoch [22/200], Step [300/312], Loss: 2.4207\n",
            "\n",
            "train-loss: 3.4158, train-acc: 50.9177\n",
            "validation loss: 3.5254, validation acc: 41.7168\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 23\n",
            "\n",
            "Epoch [23/200], Step [0/312], Loss: 1.8175\n",
            "Epoch [23/200], Step [20/312], Loss: 2.0952\n",
            "Epoch [23/200], Step [40/312], Loss: 2.6688\n",
            "Epoch [23/200], Step [60/312], Loss: 2.0463\n",
            "Epoch [23/200], Step [80/312], Loss: 1.8950\n",
            "Epoch [23/200], Step [100/312], Loss: 2.4427\n",
            "Epoch [23/200], Step [120/312], Loss: 2.2975\n",
            "Epoch [23/200], Step [140/312], Loss: 2.7186\n",
            "Epoch [23/200], Step [160/312], Loss: 2.1252\n",
            "Epoch [23/200], Step [180/312], Loss: 2.2376\n",
            "Epoch [23/200], Step [200/312], Loss: 2.4097\n",
            "Epoch [23/200], Step [220/312], Loss: 2.1957\n",
            "Epoch [23/200], Step [240/312], Loss: 2.7579\n",
            "Epoch [23/200], Step [260/312], Loss: 2.0591\n",
            "Epoch [23/200], Step [280/312], Loss: 2.0604\n",
            "Epoch [23/200], Step [300/312], Loss: 2.2030\n",
            "\n",
            "train-loss: 3.3664, train-acc: 52.0309\n",
            "validation loss: 3.4921, validation acc: 42.2784\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 24\n",
            "\n",
            "Epoch [24/200], Step [0/312], Loss: 2.2790\n",
            "Epoch [24/200], Step [20/312], Loss: 2.0700\n",
            "Epoch [24/200], Step [40/312], Loss: 2.5847\n",
            "Epoch [24/200], Step [60/312], Loss: 1.9910\n",
            "Epoch [24/200], Step [80/312], Loss: 2.7176\n",
            "Epoch [24/200], Step [100/312], Loss: 1.8716\n",
            "Epoch [24/200], Step [120/312], Loss: 2.3132\n",
            "Epoch [24/200], Step [140/312], Loss: 2.2407\n",
            "Epoch [24/200], Step [160/312], Loss: 2.2510\n",
            "Epoch [24/200], Step [180/312], Loss: 2.1388\n",
            "Epoch [24/200], Step [200/312], Loss: 2.4070\n",
            "Epoch [24/200], Step [220/312], Loss: 2.0739\n",
            "Epoch [24/200], Step [240/312], Loss: 2.1497\n",
            "Epoch [24/200], Step [260/312], Loss: 2.3848\n",
            "Epoch [24/200], Step [280/312], Loss: 2.4457\n",
            "Epoch [24/200], Step [300/312], Loss: 2.0754\n",
            "\n",
            "train-loss: 3.3182, train-acc: 53.4149\n",
            "validation loss: 3.4608, validation acc: 42.7597\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 25\n",
            "\n",
            "Epoch [25/200], Step [0/312], Loss: 2.1201\n",
            "Epoch [25/200], Step [20/312], Loss: 2.1588\n",
            "Epoch [25/200], Step [40/312], Loss: 2.2695\n",
            "Epoch [25/200], Step [60/312], Loss: 1.7158\n",
            "Epoch [25/200], Step [80/312], Loss: 2.3226\n",
            "Epoch [25/200], Step [100/312], Loss: 2.3862\n",
            "Epoch [25/200], Step [120/312], Loss: 2.6327\n",
            "Epoch [25/200], Step [140/312], Loss: 2.0618\n",
            "Epoch [25/200], Step [160/312], Loss: 1.7716\n",
            "Epoch [25/200], Step [180/312], Loss: 1.9779\n",
            "Epoch [25/200], Step [200/312], Loss: 2.4029\n",
            "Epoch [25/200], Step [220/312], Loss: 2.0232\n",
            "Epoch [25/200], Step [240/312], Loss: 2.1566\n",
            "Epoch [25/200], Step [260/312], Loss: 2.7820\n",
            "Epoch [25/200], Step [280/312], Loss: 2.0187\n",
            "Epoch [25/200], Step [300/312], Loss: 1.7153\n",
            "\n",
            "train-loss: 3.2713, train-acc: 54.7688\n",
            "validation loss: 3.4313, validation acc: 42.8801\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 26\n",
            "\n",
            "Epoch [26/200], Step [0/312], Loss: 2.6022\n",
            "Epoch [26/200], Step [20/312], Loss: 1.7201\n",
            "Epoch [26/200], Step [40/312], Loss: 1.7631\n",
            "Epoch [26/200], Step [60/312], Loss: 1.9384\n",
            "Epoch [26/200], Step [80/312], Loss: 2.3895\n",
            "Epoch [26/200], Step [100/312], Loss: 2.1277\n",
            "Epoch [26/200], Step [120/312], Loss: 1.8595\n",
            "Epoch [26/200], Step [140/312], Loss: 2.0680\n",
            "Epoch [26/200], Step [160/312], Loss: 1.6898\n",
            "Epoch [26/200], Step [180/312], Loss: 2.3504\n",
            "Epoch [26/200], Step [200/312], Loss: 2.1168\n",
            "Epoch [26/200], Step [220/312], Loss: 2.3370\n",
            "Epoch [26/200], Step [240/312], Loss: 1.9110\n",
            "Epoch [26/200], Step [260/312], Loss: 2.4847\n",
            "Epoch [26/200], Step [280/312], Loss: 2.6980\n",
            "Epoch [26/200], Step [300/312], Loss: 2.0630\n",
            "\n",
            "train-loss: 3.2255, train-acc: 56.3033\n",
            "validation loss: 3.4044, validation acc: 42.9603\n",
            "\n",
            "Epoch 27\n",
            "\n",
            "Epoch [27/200], Step [0/312], Loss: 1.9193\n",
            "Epoch [27/200], Step [20/312], Loss: 1.8454\n",
            "Epoch [27/200], Step [40/312], Loss: 1.9325\n",
            "Epoch [27/200], Step [60/312], Loss: 1.6567\n",
            "Epoch [27/200], Step [80/312], Loss: 1.8800\n",
            "Epoch [27/200], Step [100/312], Loss: 2.3517\n",
            "Epoch [27/200], Step [120/312], Loss: 2.0200\n",
            "Epoch [27/200], Step [140/312], Loss: 1.7033\n",
            "Epoch [27/200], Step [160/312], Loss: 1.9739\n",
            "Epoch [27/200], Step [180/312], Loss: 1.9525\n",
            "Epoch [27/200], Step [200/312], Loss: 2.2576\n",
            "Epoch [27/200], Step [220/312], Loss: 1.8286\n",
            "Epoch [27/200], Step [240/312], Loss: 3.0828\n",
            "Epoch [27/200], Step [260/312], Loss: 1.6809\n",
            "Epoch [27/200], Step [280/312], Loss: 1.6430\n",
            "Epoch [27/200], Step [300/312], Loss: 1.8098\n",
            "\n",
            "train-loss: 3.1805, train-acc: 57.3463\n",
            "validation loss: 3.3782, validation acc: 42.9603\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 28\n",
            "\n",
            "Epoch [28/200], Step [0/312], Loss: 1.7446\n",
            "Epoch [28/200], Step [20/312], Loss: 1.4208\n",
            "Epoch [28/200], Step [40/312], Loss: 2.1710\n",
            "Epoch [28/200], Step [60/312], Loss: 1.8659\n",
            "Epoch [28/200], Step [80/312], Loss: 2.0308\n",
            "Epoch [28/200], Step [100/312], Loss: 1.8141\n",
            "Epoch [28/200], Step [120/312], Loss: 2.1739\n",
            "Epoch [28/200], Step [140/312], Loss: 1.9129\n",
            "Epoch [28/200], Step [160/312], Loss: 2.2982\n",
            "Epoch [28/200], Step [180/312], Loss: 1.6161\n",
            "Epoch [28/200], Step [200/312], Loss: 1.3868\n",
            "Epoch [28/200], Step [220/312], Loss: 1.6307\n",
            "Epoch [28/200], Step [240/312], Loss: 1.8940\n",
            "Epoch [28/200], Step [260/312], Loss: 1.4720\n",
            "Epoch [28/200], Step [280/312], Loss: 1.9318\n",
            "Epoch [28/200], Step [300/312], Loss: 1.7067\n",
            "\n",
            "train-loss: 3.1360, train-acc: 59.2518\n",
            "validation loss: 3.3534, validation acc: 42.9202\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 29\n",
            "\n",
            "Epoch [29/200], Step [0/312], Loss: 1.9443\n",
            "Epoch [29/200], Step [20/312], Loss: 1.7401\n",
            "Epoch [29/200], Step [40/312], Loss: 1.5601\n",
            "Epoch [29/200], Step [60/312], Loss: 1.7998\n",
            "Epoch [29/200], Step [80/312], Loss: 1.8557\n",
            "Epoch [29/200], Step [100/312], Loss: 1.7596\n",
            "Epoch [29/200], Step [120/312], Loss: 1.9747\n",
            "Epoch [29/200], Step [140/312], Loss: 1.7112\n",
            "Epoch [29/200], Step [160/312], Loss: 1.5155\n",
            "Epoch [29/200], Step [180/312], Loss: 1.7813\n",
            "Epoch [29/200], Step [200/312], Loss: 1.5877\n",
            "Epoch [29/200], Step [220/312], Loss: 1.9864\n",
            "Epoch [29/200], Step [240/312], Loss: 1.6651\n",
            "Epoch [29/200], Step [260/312], Loss: 1.6325\n",
            "Epoch [29/200], Step [280/312], Loss: 1.9639\n",
            "Epoch [29/200], Step [300/312], Loss: 1.7505\n",
            "\n",
            "train-loss: 3.0923, train-acc: 60.8565\n",
            "validation loss: 3.3307, validation acc: 43.0405\n",
            "\n",
            "Epoch 30\n",
            "\n",
            "Epoch [30/200], Step [0/312], Loss: 1.4890\n",
            "Epoch [30/200], Step [20/312], Loss: 1.7990\n",
            "Epoch [30/200], Step [40/312], Loss: 1.4765\n",
            "Epoch [30/200], Step [60/312], Loss: 1.3773\n",
            "Epoch [30/200], Step [80/312], Loss: 1.9818\n",
            "Epoch [30/200], Step [100/312], Loss: 1.4220\n",
            "Epoch [30/200], Step [120/312], Loss: 1.9594\n",
            "Epoch [30/200], Step [140/312], Loss: 1.7465\n",
            "Epoch [30/200], Step [160/312], Loss: 2.1210\n",
            "Epoch [30/200], Step [180/312], Loss: 1.4497\n",
            "Epoch [30/200], Step [200/312], Loss: 1.5441\n",
            "Epoch [30/200], Step [220/312], Loss: 1.6907\n",
            "Epoch [30/200], Step [240/312], Loss: 1.7666\n",
            "Epoch [30/200], Step [260/312], Loss: 2.1312\n",
            "Epoch [30/200], Step [280/312], Loss: 1.5968\n",
            "Epoch [30/200], Step [300/312], Loss: 1.6312\n",
            "\n",
            "train-loss: 3.0489, train-acc: 62.1904\n",
            "validation loss: 3.3091, validation acc: 43.6021\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 31\n",
            "\n",
            "Epoch [31/200], Step [0/312], Loss: 1.7891\n",
            "Epoch [31/200], Step [20/312], Loss: 1.7092\n",
            "Epoch [31/200], Step [40/312], Loss: 1.8618\n",
            "Epoch [31/200], Step [60/312], Loss: 1.7465\n",
            "Epoch [31/200], Step [80/312], Loss: 1.5879\n",
            "Epoch [31/200], Step [100/312], Loss: 2.1534\n",
            "Epoch [31/200], Step [120/312], Loss: 1.7797\n",
            "Epoch [31/200], Step [140/312], Loss: 2.0290\n",
            "Epoch [31/200], Step [160/312], Loss: 1.5172\n",
            "Epoch [31/200], Step [180/312], Loss: 1.6172\n",
            "Epoch [31/200], Step [200/312], Loss: 1.5123\n",
            "Epoch [31/200], Step [220/312], Loss: 1.4779\n",
            "Epoch [31/200], Step [240/312], Loss: 1.3707\n",
            "Epoch [31/200], Step [260/312], Loss: 1.7808\n",
            "Epoch [31/200], Step [280/312], Loss: 1.3529\n",
            "Epoch [31/200], Step [300/312], Loss: 2.1855\n",
            "\n",
            "train-loss: 3.0061, train-acc: 63.9956\n",
            "validation loss: 3.2882, validation acc: 43.8026\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 32\n",
            "\n",
            "Epoch [32/200], Step [0/312], Loss: 1.1840\n",
            "Epoch [32/200], Step [20/312], Loss: 2.0647\n",
            "Epoch [32/200], Step [40/312], Loss: 1.9253\n",
            "Epoch [32/200], Step [60/312], Loss: 1.2564\n",
            "Epoch [32/200], Step [80/312], Loss: 1.1606\n",
            "Epoch [32/200], Step [100/312], Loss: 1.5336\n",
            "Epoch [32/200], Step [120/312], Loss: 1.5495\n",
            "Epoch [32/200], Step [140/312], Loss: 1.9678\n",
            "Epoch [32/200], Step [160/312], Loss: 1.9104\n",
            "Epoch [32/200], Step [180/312], Loss: 1.4644\n",
            "Epoch [32/200], Step [200/312], Loss: 1.5792\n",
            "Epoch [32/200], Step [220/312], Loss: 2.0080\n",
            "Epoch [32/200], Step [240/312], Loss: 1.5321\n",
            "Epoch [32/200], Step [260/312], Loss: 1.6574\n",
            "Epoch [32/200], Step [280/312], Loss: 1.7417\n",
            "Epoch [32/200], Step [300/312], Loss: 1.4531\n",
            "\n",
            "train-loss: 2.9641, train-acc: 65.5902\n",
            "validation loss: 3.2690, validation acc: 43.1609\n",
            "\n",
            "Epoch 33\n",
            "\n",
            "Epoch [33/200], Step [0/312], Loss: 1.3254\n",
            "Epoch [33/200], Step [20/312], Loss: 1.5798\n",
            "Epoch [33/200], Step [40/312], Loss: 1.7179\n",
            "Epoch [33/200], Step [60/312], Loss: 1.9047\n",
            "Epoch [33/200], Step [80/312], Loss: 1.4285\n",
            "Epoch [33/200], Step [100/312], Loss: 1.4317\n",
            "Epoch [33/200], Step [120/312], Loss: 1.4534\n",
            "Epoch [33/200], Step [140/312], Loss: 1.8677\n",
            "Epoch [33/200], Step [160/312], Loss: 1.0595\n",
            "Epoch [33/200], Step [180/312], Loss: 1.4399\n",
            "Epoch [33/200], Step [200/312], Loss: 2.0255\n",
            "Epoch [33/200], Step [220/312], Loss: 1.8831\n",
            "Epoch [33/200], Step [240/312], Loss: 1.7676\n",
            "Epoch [33/200], Step [260/312], Loss: 1.9766\n",
            "Epoch [33/200], Step [280/312], Loss: 1.7970\n",
            "Epoch [33/200], Step [300/312], Loss: 1.4460\n",
            "\n",
            "train-loss: 2.9220, train-acc: 67.9270\n",
            "validation loss: 3.2506, validation acc: 43.4416\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 34\n",
            "\n",
            "Epoch [34/200], Step [0/312], Loss: 1.8203\n",
            "Epoch [34/200], Step [20/312], Loss: 1.0703\n",
            "Epoch [34/200], Step [40/312], Loss: 1.0107\n",
            "Epoch [34/200], Step [60/312], Loss: 1.5416\n",
            "Epoch [34/200], Step [80/312], Loss: 1.4544\n",
            "Epoch [34/200], Step [100/312], Loss: 1.4920\n",
            "Epoch [34/200], Step [120/312], Loss: 1.2886\n",
            "Epoch [34/200], Step [140/312], Loss: 1.8337\n",
            "Epoch [34/200], Step [160/312], Loss: 1.3025\n",
            "Epoch [34/200], Step [180/312], Loss: 1.8909\n",
            "Epoch [34/200], Step [200/312], Loss: 1.5389\n",
            "Epoch [34/200], Step [220/312], Loss: 1.5384\n",
            "Epoch [34/200], Step [240/312], Loss: 1.4992\n",
            "Epoch [34/200], Step [260/312], Loss: 1.7193\n",
            "Epoch [34/200], Step [280/312], Loss: 1.3751\n",
            "Epoch [34/200], Step [300/312], Loss: 1.5766\n",
            "\n",
            "train-loss: 2.8806, train-acc: 69.0603\n",
            "validation loss: 3.2332, validation acc: 42.9603\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 35\n",
            "\n",
            "Epoch [35/200], Step [0/312], Loss: 1.4598\n",
            "Epoch [35/200], Step [20/312], Loss: 1.0839\n",
            "Epoch [35/200], Step [40/312], Loss: 1.1510\n",
            "Epoch [35/200], Step [60/312], Loss: 1.2925\n",
            "Epoch [35/200], Step [80/312], Loss: 1.3779\n",
            "Epoch [35/200], Step [100/312], Loss: 1.3382\n",
            "Epoch [35/200], Step [120/312], Loss: 1.2479\n",
            "Epoch [35/200], Step [140/312], Loss: 1.1569\n",
            "Epoch [35/200], Step [160/312], Loss: 1.9789\n",
            "Epoch [35/200], Step [180/312], Loss: 1.3951\n",
            "Epoch [35/200], Step [200/312], Loss: 1.3660\n",
            "Epoch [35/200], Step [220/312], Loss: 1.2438\n",
            "Epoch [35/200], Step [240/312], Loss: 1.5447\n",
            "Epoch [35/200], Step [260/312], Loss: 1.3914\n",
            "Epoch [35/200], Step [280/312], Loss: 1.2749\n",
            "Epoch [35/200], Step [300/312], Loss: 1.2557\n",
            "\n",
            "train-loss: 2.8396, train-acc: 71.3469\n",
            "validation loss: 3.2176, validation acc: 43.7224\n",
            "\n",
            "Epoch 36\n",
            "\n",
            "Epoch [36/200], Step [0/312], Loss: 1.2453\n",
            "Epoch [36/200], Step [20/312], Loss: 1.3215\n",
            "Epoch [36/200], Step [40/312], Loss: 1.0203\n",
            "Epoch [36/200], Step [60/312], Loss: 1.2301\n",
            "Epoch [36/200], Step [80/312], Loss: 1.5182\n",
            "Epoch [36/200], Step [100/312], Loss: 1.0336\n",
            "Epoch [36/200], Step [120/312], Loss: 1.3433\n",
            "Epoch [36/200], Step [140/312], Loss: 1.2885\n",
            "Epoch [36/200], Step [160/312], Loss: 1.7677\n",
            "Epoch [36/200], Step [180/312], Loss: 1.2665\n",
            "Epoch [36/200], Step [200/312], Loss: 1.2831\n",
            "Epoch [36/200], Step [220/312], Loss: 1.4186\n",
            "Epoch [36/200], Step [240/312], Loss: 1.5402\n",
            "Epoch [36/200], Step [260/312], Loss: 1.4258\n",
            "Epoch [36/200], Step [280/312], Loss: 1.1287\n",
            "Epoch [36/200], Step [300/312], Loss: 1.5749\n",
            "\n",
            "train-loss: 2.7989, train-acc: 73.0117\n",
            "validation loss: 3.2024, validation acc: 43.0004\n",
            "\n",
            "Epoch 37\n",
            "\n",
            "Epoch [37/200], Step [0/312], Loss: 1.1669\n",
            "Epoch [37/200], Step [20/312], Loss: 1.5023\n",
            "Epoch [37/200], Step [40/312], Loss: 1.3489\n",
            "Epoch [37/200], Step [60/312], Loss: 1.1209\n",
            "Epoch [37/200], Step [80/312], Loss: 1.4229\n",
            "Epoch [37/200], Step [100/312], Loss: 1.3808\n",
            "Epoch [37/200], Step [120/312], Loss: 1.8572\n",
            "Epoch [37/200], Step [140/312], Loss: 1.0707\n",
            "Epoch [37/200], Step [160/312], Loss: 1.1713\n",
            "Epoch [37/200], Step [180/312], Loss: 1.1228\n",
            "Epoch [37/200], Step [200/312], Loss: 1.0858\n",
            "Epoch [37/200], Step [220/312], Loss: 1.1646\n",
            "Epoch [37/200], Step [240/312], Loss: 2.1591\n",
            "Epoch [37/200], Step [260/312], Loss: 1.0679\n",
            "Epoch [37/200], Step [280/312], Loss: 1.3692\n",
            "Epoch [37/200], Step [300/312], Loss: 1.2757\n",
            "\n",
            "train-loss: 2.7584, train-acc: 74.5061\n",
            "validation loss: 3.1886, validation acc: 43.1207\n",
            "\n",
            "Epoch 38\n",
            "\n",
            "Epoch [38/200], Step [0/312], Loss: 1.2132\n",
            "Epoch [38/200], Step [20/312], Loss: 1.2743\n",
            "Epoch [38/200], Step [40/312], Loss: 1.1204\n",
            "Epoch [38/200], Step [60/312], Loss: 0.9954\n",
            "Epoch [38/200], Step [80/312], Loss: 0.9871\n",
            "Epoch [38/200], Step [100/312], Loss: 1.6773\n",
            "Epoch [38/200], Step [120/312], Loss: 1.1357\n",
            "Epoch [38/200], Step [140/312], Loss: 1.0307\n",
            "Epoch [38/200], Step [160/312], Loss: 1.4463\n",
            "Epoch [38/200], Step [180/312], Loss: 1.2643\n",
            "Epoch [38/200], Step [200/312], Loss: 1.5188\n",
            "Epoch [38/200], Step [220/312], Loss: 1.4677\n",
            "Epoch [38/200], Step [240/312], Loss: 1.4538\n",
            "Epoch [38/200], Step [260/312], Loss: 1.5347\n",
            "Epoch [38/200], Step [280/312], Loss: 1.4960\n",
            "Epoch [38/200], Step [300/312], Loss: 1.0499\n",
            "\n",
            "train-loss: 2.7183, train-acc: 76.2210\n",
            "validation loss: 3.1758, validation acc: 43.2010\n",
            "\n",
            "Epoch 39\n",
            "\n",
            "Epoch [39/200], Step [0/312], Loss: 1.3396\n",
            "Epoch [39/200], Step [20/312], Loss: 1.2626\n",
            "Epoch [39/200], Step [40/312], Loss: 1.6080\n",
            "Epoch [39/200], Step [60/312], Loss: 1.3849\n",
            "Epoch [39/200], Step [80/312], Loss: 1.4447\n",
            "Epoch [39/200], Step [100/312], Loss: 1.2085\n",
            "Epoch [39/200], Step [120/312], Loss: 1.2807\n",
            "Epoch [39/200], Step [140/312], Loss: 1.1249\n",
            "Epoch [39/200], Step [160/312], Loss: 0.9730\n",
            "Epoch [39/200], Step [180/312], Loss: 1.3414\n",
            "Epoch [39/200], Step [200/312], Loss: 0.9308\n",
            "Epoch [39/200], Step [220/312], Loss: 0.9294\n",
            "Epoch [39/200], Step [240/312], Loss: 1.3088\n",
            "Epoch [39/200], Step [260/312], Loss: 1.1042\n",
            "Epoch [39/200], Step [280/312], Loss: 1.0575\n",
            "Epoch [39/200], Step [300/312], Loss: 1.1520\n",
            "\n",
            "train-loss: 2.6780, train-acc: 78.8186\n",
            "validation loss: 3.1638, validation acc: 43.4416\n",
            "\n",
            "Epoch 40\n",
            "\n",
            "Epoch [40/200], Step [0/312], Loss: 0.8524\n",
            "Epoch [40/200], Step [20/312], Loss: 1.0886\n",
            "Epoch [40/200], Step [40/312], Loss: 0.8754\n",
            "Epoch [40/200], Step [60/312], Loss: 0.7741\n",
            "Epoch [40/200], Step [80/312], Loss: 0.7410\n",
            "Epoch [40/200], Step [100/312], Loss: 1.0045\n",
            "Epoch [40/200], Step [120/312], Loss: 0.9478\n",
            "Epoch [40/200], Step [140/312], Loss: 1.3432\n",
            "Epoch [40/200], Step [160/312], Loss: 1.0444\n",
            "Epoch [40/200], Step [180/312], Loss: 1.0683\n",
            "Epoch [40/200], Step [200/312], Loss: 0.9320\n",
            "Epoch [40/200], Step [220/312], Loss: 1.4074\n",
            "Epoch [40/200], Step [240/312], Loss: 1.0332\n",
            "Epoch [40/200], Step [260/312], Loss: 1.2074\n",
            "Epoch [40/200], Step [280/312], Loss: 1.1975\n",
            "Epoch [40/200], Step [300/312], Loss: 1.4754\n",
            "\n",
            "train-loss: 2.6384, train-acc: 80.1023\n",
            "validation loss: 3.1527, validation acc: 43.4416\n",
            "\n",
            "Epoch 41\n",
            "\n",
            "Epoch [41/200], Step [0/312], Loss: 1.0642\n",
            "Epoch [41/200], Step [20/312], Loss: 1.2079\n",
            "Epoch [41/200], Step [40/312], Loss: 0.6447\n",
            "Epoch [41/200], Step [60/312], Loss: 0.6188\n",
            "Epoch [41/200], Step [80/312], Loss: 1.1082\n",
            "Epoch [41/200], Step [100/312], Loss: 0.9766\n",
            "Epoch [41/200], Step [120/312], Loss: 0.9545\n",
            "Epoch [41/200], Step [140/312], Loss: 1.0159\n",
            "Epoch [41/200], Step [160/312], Loss: 0.9338\n",
            "Epoch [41/200], Step [180/312], Loss: 1.0863\n",
            "Epoch [41/200], Step [200/312], Loss: 0.9223\n",
            "Epoch [41/200], Step [220/312], Loss: 1.1467\n",
            "Epoch [41/200], Step [240/312], Loss: 0.6513\n",
            "Epoch [41/200], Step [260/312], Loss: 0.6897\n",
            "Epoch [41/200], Step [280/312], Loss: 1.1617\n",
            "Epoch [41/200], Step [300/312], Loss: 0.8424\n",
            "\n",
            "train-loss: 2.5989, train-acc: 82.0580\n",
            "validation loss: 3.1423, validation acc: 42.1982\n",
            "\n",
            "Epoch 42\n",
            "\n",
            "Epoch [42/200], Step [0/312], Loss: 1.2132\n",
            "Epoch [42/200], Step [20/312], Loss: 0.6881\n",
            "Epoch [42/200], Step [40/312], Loss: 0.7739\n",
            "Epoch [42/200], Step [60/312], Loss: 0.8476\n",
            "Epoch [42/200], Step [80/312], Loss: 0.8638\n",
            "Epoch [42/200], Step [100/312], Loss: 1.0229\n",
            "Epoch [42/200], Step [120/312], Loss: 0.9857\n",
            "Epoch [42/200], Step [140/312], Loss: 0.8335\n",
            "Epoch [42/200], Step [160/312], Loss: 1.1970\n",
            "Epoch [42/200], Step [180/312], Loss: 0.9273\n",
            "Epoch [42/200], Step [200/312], Loss: 1.0112\n",
            "Epoch [42/200], Step [220/312], Loss: 1.3278\n",
            "Epoch [42/200], Step [240/312], Loss: 1.2876\n",
            "Epoch [42/200], Step [260/312], Loss: 1.0962\n",
            "Epoch [42/200], Step [280/312], Loss: 0.6043\n",
            "Epoch [42/200], Step [300/312], Loss: 1.2783\n",
            "\n",
            "train-loss: 2.5601, train-acc: 83.0709\n",
            "validation loss: 3.1332, validation acc: 42.7998\n",
            "\n",
            "Epoch 43\n",
            "\n",
            "Epoch [43/200], Step [0/312], Loss: 0.8299\n",
            "Epoch [43/200], Step [20/312], Loss: 0.8615\n",
            "Epoch [43/200], Step [40/312], Loss: 0.9208\n",
            "Epoch [43/200], Step [60/312], Loss: 0.5136\n",
            "Epoch [43/200], Step [80/312], Loss: 0.9482\n",
            "Epoch [43/200], Step [100/312], Loss: 0.7914\n",
            "Epoch [43/200], Step [120/312], Loss: 1.0646\n",
            "Epoch [43/200], Step [140/312], Loss: 0.6621\n",
            "Epoch [43/200], Step [160/312], Loss: 0.7026\n",
            "Epoch [43/200], Step [180/312], Loss: 1.0616\n",
            "Epoch [43/200], Step [200/312], Loss: 0.6087\n",
            "Epoch [43/200], Step [220/312], Loss: 0.7396\n",
            "Epoch [43/200], Step [240/312], Loss: 0.9012\n",
            "Epoch [43/200], Step [260/312], Loss: 0.6864\n",
            "Epoch [43/200], Step [280/312], Loss: 0.8411\n",
            "Epoch [43/200], Step [300/312], Loss: 0.7357\n",
            "\n",
            "train-loss: 2.5215, train-acc: 85.3977\n",
            "validation loss: 3.1249, validation acc: 41.3558\n",
            "\n",
            "Epoch 44\n",
            "\n",
            "Epoch [44/200], Step [0/312], Loss: 0.9480\n",
            "Epoch [44/200], Step [20/312], Loss: 0.6116\n",
            "Epoch [44/200], Step [40/312], Loss: 0.6434\n",
            "Epoch [44/200], Step [60/312], Loss: 0.7060\n",
            "Epoch [44/200], Step [80/312], Loss: 1.1501\n",
            "Epoch [44/200], Step [100/312], Loss: 0.6704\n",
            "Epoch [44/200], Step [120/312], Loss: 0.5184\n",
            "Epoch [44/200], Step [140/312], Loss: 0.8733\n",
            "Epoch [44/200], Step [160/312], Loss: 0.6815\n",
            "Epoch [44/200], Step [180/312], Loss: 0.9028\n",
            "Epoch [44/200], Step [200/312], Loss: 0.9053\n",
            "Epoch [44/200], Step [220/312], Loss: 0.6546\n",
            "Epoch [44/200], Step [240/312], Loss: 0.6956\n",
            "Epoch [44/200], Step [260/312], Loss: 0.6693\n",
            "Epoch [44/200], Step [280/312], Loss: 0.9887\n",
            "Epoch [44/200], Step [300/312], Loss: 0.9499\n",
            "\n",
            "train-loss: 2.4832, train-acc: 86.8920\n",
            "validation loss: 3.1171, validation acc: 41.5564\n",
            "\n",
            "Epoch 45\n",
            "\n",
            "Epoch [45/200], Step [0/312], Loss: 0.6058\n",
            "Epoch [45/200], Step [20/312], Loss: 0.4850\n",
            "Epoch [45/200], Step [40/312], Loss: 0.6939\n",
            "Epoch [45/200], Step [60/312], Loss: 0.7634\n",
            "Epoch [45/200], Step [80/312], Loss: 0.8686\n",
            "Epoch [45/200], Step [100/312], Loss: 0.8753\n",
            "Epoch [45/200], Step [120/312], Loss: 0.7933\n",
            "Epoch [45/200], Step [140/312], Loss: 0.7450\n",
            "Epoch [45/200], Step [160/312], Loss: 0.7903\n",
            "Epoch [45/200], Step [180/312], Loss: 0.7231\n",
            "Epoch [45/200], Step [200/312], Loss: 0.8440\n",
            "Epoch [45/200], Step [220/312], Loss: 0.6569\n",
            "Epoch [45/200], Step [240/312], Loss: 1.0309\n",
            "Epoch [45/200], Step [260/312], Loss: 0.5886\n",
            "Epoch [45/200], Step [280/312], Loss: 0.7709\n",
            "Epoch [45/200], Step [300/312], Loss: 1.0558\n",
            "\n",
            "train-loss: 2.4454, train-acc: 88.0554\n",
            "validation loss: 3.1102, validation acc: 41.8371\n",
            "\n",
            "Epoch 46\n",
            "\n",
            "Epoch [46/200], Step [0/312], Loss: 0.7591\n",
            "Epoch [46/200], Step [20/312], Loss: 0.6205\n",
            "Epoch [46/200], Step [40/312], Loss: 0.9738\n",
            "Epoch [46/200], Step [60/312], Loss: 0.6407\n",
            "Epoch [46/200], Step [80/312], Loss: 1.0609\n",
            "Epoch [46/200], Step [100/312], Loss: 0.5423\n",
            "Epoch [46/200], Step [120/312], Loss: 0.6914\n",
            "Epoch [46/200], Step [140/312], Loss: 0.9090\n",
            "Epoch [46/200], Step [160/312], Loss: 0.8400\n",
            "Epoch [46/200], Step [180/312], Loss: 0.7175\n",
            "Epoch [46/200], Step [200/312], Loss: 1.0965\n",
            "Epoch [46/200], Step [220/312], Loss: 0.6886\n",
            "Epoch [46/200], Step [240/312], Loss: 0.4667\n",
            "Epoch [46/200], Step [260/312], Loss: 0.8061\n",
            "Epoch [46/200], Step [280/312], Loss: 0.6752\n",
            "Epoch [46/200], Step [300/312], Loss: 0.8388\n",
            "\n",
            "train-loss: 2.4084, train-acc: 89.0182\n",
            "validation loss: 3.1031, validation acc: 41.8371\n",
            "\n",
            "Epoch 47\n",
            "\n",
            "Epoch [47/200], Step [0/312], Loss: 0.5978\n",
            "Epoch [47/200], Step [20/312], Loss: 0.5325\n",
            "Epoch [47/200], Step [40/312], Loss: 1.0440\n",
            "Epoch [47/200], Step [60/312], Loss: 0.9148\n",
            "Epoch [47/200], Step [80/312], Loss: 0.9001\n",
            "Epoch [47/200], Step [100/312], Loss: 0.6148\n",
            "Epoch [47/200], Step [120/312], Loss: 0.5777\n",
            "Epoch [47/200], Step [140/312], Loss: 0.6843\n",
            "Epoch [47/200], Step [160/312], Loss: 0.5239\n",
            "Epoch [47/200], Step [180/312], Loss: 0.7087\n",
            "Epoch [47/200], Step [200/312], Loss: 0.9735\n",
            "Epoch [47/200], Step [220/312], Loss: 0.6618\n",
            "Epoch [47/200], Step [240/312], Loss: 0.7045\n",
            "Epoch [47/200], Step [260/312], Loss: 0.5957\n",
            "Epoch [47/200], Step [280/312], Loss: 0.6578\n",
            "Epoch [47/200], Step [300/312], Loss: 1.1157\n",
            "\n",
            "train-loss: 2.3717, train-acc: 90.6328\n",
            "validation loss: 3.0969, validation acc: 42.1580\n",
            "\n",
            "Epoch 48\n",
            "\n",
            "Epoch [48/200], Step [0/312], Loss: 0.6405\n",
            "Epoch [48/200], Step [20/312], Loss: 0.6298\n",
            "Epoch [48/200], Step [40/312], Loss: 0.6987\n",
            "Epoch [48/200], Step [60/312], Loss: 0.5776\n",
            "Epoch [48/200], Step [80/312], Loss: 0.6713\n",
            "Epoch [48/200], Step [100/312], Loss: 0.6701\n",
            "Epoch [48/200], Step [120/312], Loss: 0.6113\n",
            "Epoch [48/200], Step [140/312], Loss: 0.5721\n",
            "Epoch [48/200], Step [160/312], Loss: 0.7864\n",
            "Epoch [48/200], Step [180/312], Loss: 0.6826\n",
            "Epoch [48/200], Step [200/312], Loss: 0.4525\n",
            "Epoch [48/200], Step [220/312], Loss: 0.6173\n",
            "Epoch [48/200], Step [240/312], Loss: 0.6229\n",
            "Epoch [48/200], Step [260/312], Loss: 0.3834\n",
            "Epoch [48/200], Step [280/312], Loss: 0.5851\n",
            "Epoch [48/200], Step [300/312], Loss: 0.4662\n",
            "\n",
            "train-loss: 2.3353, train-acc: 91.8464\n",
            "validation loss: 3.0913, validation acc: 41.1151\n",
            "\n",
            "Epoch 49\n",
            "\n",
            "Epoch [49/200], Step [0/312], Loss: 0.6306\n",
            "Epoch [49/200], Step [20/312], Loss: 0.6168\n",
            "Epoch [49/200], Step [40/312], Loss: 0.7081\n",
            "Epoch [49/200], Step [60/312], Loss: 0.3450\n",
            "Epoch [49/200], Step [80/312], Loss: 0.5459\n",
            "Epoch [49/200], Step [100/312], Loss: 0.6081\n",
            "Epoch [49/200], Step [120/312], Loss: 0.5900\n",
            "Epoch [49/200], Step [140/312], Loss: 0.9827\n",
            "Epoch [49/200], Step [160/312], Loss: 0.5769\n",
            "Epoch [49/200], Step [180/312], Loss: 0.8235\n",
            "Epoch [49/200], Step [200/312], Loss: 0.5389\n",
            "Epoch [49/200], Step [220/312], Loss: 0.6835\n",
            "Epoch [49/200], Step [240/312], Loss: 0.6600\n",
            "Epoch [49/200], Step [260/312], Loss: 0.3523\n",
            "Epoch [49/200], Step [280/312], Loss: 0.4890\n",
            "Epoch [49/200], Step [300/312], Loss: 0.3252\n",
            "\n",
            "train-loss: 2.2996, train-acc: 92.8593\n",
            "validation loss: 3.0862, validation acc: 40.7942\n",
            "\n",
            "Epoch 50\n",
            "\n",
            "Epoch [50/200], Step [0/312], Loss: 0.6709\n",
            "Epoch [50/200], Step [20/312], Loss: 0.4351\n",
            "Epoch [50/200], Step [40/312], Loss: 0.3687\n",
            "Epoch [50/200], Step [60/312], Loss: 0.6541\n",
            "Epoch [50/200], Step [80/312], Loss: 0.5093\n",
            "Epoch [50/200], Step [100/312], Loss: 0.7271\n",
            "Epoch [50/200], Step [120/312], Loss: 0.6971\n",
            "Epoch [50/200], Step [140/312], Loss: 0.2778\n",
            "Epoch [50/200], Step [160/312], Loss: 0.3403\n",
            "Epoch [50/200], Step [180/312], Loss: 0.5002\n",
            "Epoch [50/200], Step [200/312], Loss: 0.5632\n",
            "Epoch [50/200], Step [220/312], Loss: 0.6168\n",
            "Epoch [50/200], Step [240/312], Loss: 0.5801\n",
            "Epoch [50/200], Step [260/312], Loss: 0.5449\n",
            "Epoch [50/200], Step [280/312], Loss: 0.6745\n",
            "Epoch [50/200], Step [300/312], Loss: 0.4431\n",
            "\n",
            "train-loss: 2.2646, train-acc: 93.5112\n",
            "validation loss: 3.0819, validation acc: 40.3530\n",
            "\n",
            "Epoch 51\n",
            "\n",
            "Epoch [51/200], Step [0/312], Loss: 0.5697\n",
            "Epoch [51/200], Step [20/312], Loss: 0.3624\n",
            "Epoch [51/200], Step [40/312], Loss: 0.5993\n",
            "Epoch [51/200], Step [60/312], Loss: 0.6188\n",
            "Epoch [51/200], Step [80/312], Loss: 0.9563\n",
            "Epoch [51/200], Step [100/312], Loss: 0.4316\n",
            "Epoch [51/200], Step [120/312], Loss: 0.4950\n",
            "Epoch [51/200], Step [140/312], Loss: 0.5476\n",
            "Epoch [51/200], Step [160/312], Loss: 0.3753\n",
            "Epoch [51/200], Step [180/312], Loss: 0.4678\n",
            "Epoch [51/200], Step [200/312], Loss: 0.4927\n",
            "Epoch [51/200], Step [220/312], Loss: 0.5775\n",
            "Epoch [51/200], Step [240/312], Loss: 0.3685\n",
            "Epoch [51/200], Step [260/312], Loss: 0.7145\n",
            "Epoch [51/200], Step [280/312], Loss: 0.5304\n",
            "Epoch [51/200], Step [300/312], Loss: 0.4930\n",
            "\n",
            "train-loss: 2.2302, train-acc: 94.2935\n",
            "validation loss: 3.0782, validation acc: 41.3157\n",
            "\n",
            "Epoch 52\n",
            "\n",
            "Epoch [52/200], Step [0/312], Loss: 0.6323\n",
            "Epoch [52/200], Step [20/312], Loss: 0.2767\n",
            "Epoch [52/200], Step [40/312], Loss: 0.3648\n",
            "Epoch [52/200], Step [60/312], Loss: 0.6127\n",
            "Epoch [52/200], Step [80/312], Loss: 0.7351\n",
            "Epoch [52/200], Step [100/312], Loss: 0.2825\n",
            "Epoch [52/200], Step [120/312], Loss: 0.5643\n",
            "Epoch [52/200], Step [140/312], Loss: 0.4058\n",
            "Epoch [52/200], Step [160/312], Loss: 0.4902\n",
            "Epoch [52/200], Step [180/312], Loss: 0.3782\n",
            "Epoch [52/200], Step [200/312], Loss: 0.3241\n",
            "Epoch [52/200], Step [220/312], Loss: 0.4677\n",
            "Epoch [52/200], Step [240/312], Loss: 0.4679\n",
            "Epoch [52/200], Step [260/312], Loss: 0.5157\n",
            "Epoch [52/200], Step [280/312], Loss: 0.3738\n",
            "Epoch [52/200], Step [300/312], Loss: 0.4991\n",
            "\n",
            "train-loss: 2.1963, train-acc: 95.2462\n",
            "validation loss: 3.0746, validation acc: 40.9948\n",
            "\n",
            "Epoch 53\n",
            "\n",
            "Epoch [53/200], Step [0/312], Loss: 0.4117\n",
            "Epoch [53/200], Step [20/312], Loss: 0.4997\n",
            "Epoch [53/200], Step [40/312], Loss: 0.4615\n",
            "Epoch [53/200], Step [60/312], Loss: 0.5165\n",
            "Epoch [53/200], Step [80/312], Loss: 0.4642\n",
            "Epoch [53/200], Step [100/312], Loss: 0.5211\n",
            "Epoch [53/200], Step [120/312], Loss: 0.3035\n",
            "Epoch [53/200], Step [140/312], Loss: 0.3606\n",
            "Epoch [53/200], Step [160/312], Loss: 0.5075\n",
            "Epoch [53/200], Step [180/312], Loss: 0.3597\n",
            "Epoch [53/200], Step [200/312], Loss: 0.4085\n",
            "Epoch [53/200], Step [220/312], Loss: 0.5043\n",
            "Epoch [53/200], Step [240/312], Loss: 0.4444\n",
            "Epoch [53/200], Step [260/312], Loss: 0.3815\n",
            "Epoch [53/200], Step [280/312], Loss: 0.3524\n",
            "Epoch [53/200], Step [300/312], Loss: 0.4793\n",
            "\n",
            "train-loss: 2.1633, train-acc: 95.3164\n",
            "validation loss: 3.0707, validation acc: 40.5535\n",
            "\n",
            "Epoch 54\n",
            "\n",
            "Epoch [54/200], Step [0/312], Loss: 0.3190\n",
            "Epoch [54/200], Step [20/312], Loss: 0.3796\n",
            "Epoch [54/200], Step [40/312], Loss: 0.4309\n",
            "Epoch [54/200], Step [60/312], Loss: 0.4674\n",
            "Epoch [54/200], Step [80/312], Loss: 0.4076\n",
            "Epoch [54/200], Step [100/312], Loss: 0.3473\n",
            "Epoch [54/200], Step [120/312], Loss: 0.5314\n",
            "Epoch [54/200], Step [140/312], Loss: 0.4395\n",
            "Epoch [54/200], Step [160/312], Loss: 0.2673\n",
            "Epoch [54/200], Step [180/312], Loss: 0.4923\n",
            "Epoch [54/200], Step [200/312], Loss: 0.2834\n",
            "Epoch [54/200], Step [220/312], Loss: 0.4546\n",
            "Epoch [54/200], Step [240/312], Loss: 0.7432\n",
            "Epoch [54/200], Step [260/312], Loss: 0.5649\n",
            "Epoch [54/200], Step [280/312], Loss: 0.5227\n",
            "Epoch [54/200], Step [300/312], Loss: 0.5775\n",
            "\n",
            "train-loss: 2.1306, train-acc: 96.6001\n",
            "validation loss: 3.0673, validation acc: 40.5134\n",
            "\n",
            "Epoch 55\n",
            "\n",
            "Epoch [55/200], Step [0/312], Loss: 0.4412\n",
            "Epoch [55/200], Step [20/312], Loss: 0.2453\n",
            "Epoch [55/200], Step [40/312], Loss: 0.2747\n",
            "Epoch [55/200], Step [60/312], Loss: 0.3505\n",
            "Epoch [55/200], Step [80/312], Loss: 0.2908\n",
            "Epoch [55/200], Step [100/312], Loss: 0.3107\n",
            "Epoch [55/200], Step [120/312], Loss: 0.4993\n",
            "Epoch [55/200], Step [140/312], Loss: 0.2668\n",
            "Epoch [55/200], Step [160/312], Loss: 0.4818\n",
            "Epoch [55/200], Step [180/312], Loss: 0.3925\n",
            "Epoch [55/200], Step [200/312], Loss: 0.2999\n",
            "Epoch [55/200], Step [220/312], Loss: 0.2537\n",
            "Epoch [55/200], Step [240/312], Loss: 0.3361\n",
            "Epoch [55/200], Step [260/312], Loss: 0.5350\n",
            "Epoch [55/200], Step [280/312], Loss: 0.3753\n",
            "Epoch [55/200], Step [300/312], Loss: 0.4399\n",
            "\n",
            "train-loss: 2.0986, train-acc: 96.8308\n",
            "validation loss: 3.0641, validation acc: 40.7942\n",
            "\n",
            "Epoch 56\n",
            "\n",
            "Epoch [56/200], Step [0/312], Loss: 0.2339\n",
            "Epoch [56/200], Step [20/312], Loss: 0.3475\n",
            "Epoch [56/200], Step [40/312], Loss: 0.2871\n",
            "Epoch [56/200], Step [60/312], Loss: 0.5380\n",
            "Epoch [56/200], Step [80/312], Loss: 0.4152\n",
            "Epoch [56/200], Step [100/312], Loss: 0.2945\n",
            "Epoch [56/200], Step [120/312], Loss: 0.3487\n",
            "Epoch [56/200], Step [140/312], Loss: 0.2311\n",
            "Epoch [56/200], Step [160/312], Loss: 0.2911\n",
            "Epoch [56/200], Step [180/312], Loss: 0.2847\n",
            "Epoch [56/200], Step [200/312], Loss: 0.3860\n",
            "Epoch [56/200], Step [220/312], Loss: 0.2542\n",
            "Epoch [56/200], Step [240/312], Loss: 0.2784\n",
            "Epoch [56/200], Step [260/312], Loss: 0.1859\n",
            "Epoch [56/200], Step [280/312], Loss: 0.1625\n",
            "Epoch [56/200], Step [300/312], Loss: 0.4713\n",
            "\n",
            "train-loss: 2.0674, train-acc: 97.4025\n",
            "validation loss: 3.0617, validation acc: 40.0321\n",
            "\n",
            "Epoch 57\n",
            "\n",
            "Epoch [57/200], Step [0/312], Loss: 0.3300\n",
            "Epoch [57/200], Step [20/312], Loss: 0.3090\n",
            "Epoch [57/200], Step [40/312], Loss: 0.7740\n",
            "Epoch [57/200], Step [60/312], Loss: 0.3292\n",
            "Epoch [57/200], Step [80/312], Loss: 0.2279\n",
            "Epoch [57/200], Step [100/312], Loss: 0.3078\n",
            "Epoch [57/200], Step [120/312], Loss: 0.3012\n",
            "Epoch [57/200], Step [140/312], Loss: 0.2864\n",
            "Epoch [57/200], Step [160/312], Loss: 0.4612\n",
            "Epoch [57/200], Step [180/312], Loss: 0.1822\n",
            "Epoch [57/200], Step [200/312], Loss: 0.3485\n",
            "Epoch [57/200], Step [220/312], Loss: 0.3149\n",
            "Epoch [57/200], Step [240/312], Loss: 0.4809\n",
            "Epoch [57/200], Step [260/312], Loss: 0.4962\n",
            "Epoch [57/200], Step [280/312], Loss: 0.3192\n",
            "Epoch [57/200], Step [300/312], Loss: 0.2200\n",
            "\n",
            "train-loss: 2.0369, train-acc: 97.3423\n",
            "validation loss: 3.0595, validation acc: 40.4733\n",
            "\n",
            "Epoch 58\n",
            "\n",
            "Epoch [58/200], Step [0/312], Loss: 0.2030\n",
            "Epoch [58/200], Step [20/312], Loss: 0.3970\n",
            "Epoch [58/200], Step [40/312], Loss: 0.4790\n",
            "Epoch [58/200], Step [60/312], Loss: 0.4262\n",
            "Epoch [58/200], Step [80/312], Loss: 0.4113\n",
            "Epoch [58/200], Step [100/312], Loss: 0.2169\n",
            "Epoch [58/200], Step [120/312], Loss: 0.2470\n",
            "Epoch [58/200], Step [140/312], Loss: 0.2706\n",
            "Epoch [58/200], Step [160/312], Loss: 0.2292\n",
            "Epoch [58/200], Step [180/312], Loss: 0.3516\n",
            "Epoch [58/200], Step [200/312], Loss: 0.2121\n",
            "Epoch [58/200], Step [220/312], Loss: 0.2433\n",
            "Epoch [58/200], Step [240/312], Loss: 0.4412\n",
            "Epoch [58/200], Step [260/312], Loss: 0.2890\n",
            "Epoch [58/200], Step [280/312], Loss: 0.2667\n",
            "Epoch [58/200], Step [300/312], Loss: 0.2508\n",
            "\n",
            "train-loss: 2.0072, train-acc: 97.8437\n",
            "validation loss: 3.0575, validation acc: 40.3530\n",
            "\n",
            "Epoch 59\n",
            "\n",
            "Epoch [59/200], Step [0/312], Loss: 0.2084\n",
            "Epoch [59/200], Step [20/312], Loss: 0.1616\n",
            "Epoch [59/200], Step [40/312], Loss: 0.2635\n",
            "Epoch [59/200], Step [60/312], Loss: 0.3415\n",
            "Epoch [59/200], Step [80/312], Loss: 0.2687\n",
            "Epoch [59/200], Step [100/312], Loss: 0.3719\n",
            "Epoch [59/200], Step [120/312], Loss: 0.4166\n",
            "Epoch [59/200], Step [140/312], Loss: 0.1479\n",
            "Epoch [59/200], Step [160/312], Loss: 0.3388\n",
            "Epoch [59/200], Step [180/312], Loss: 0.2934\n",
            "Epoch [59/200], Step [200/312], Loss: 0.2668\n",
            "Epoch [59/200], Step [220/312], Loss: 0.2096\n",
            "Epoch [59/200], Step [240/312], Loss: 0.2179\n",
            "Epoch [59/200], Step [260/312], Loss: 0.2812\n",
            "Epoch [59/200], Step [280/312], Loss: 0.3480\n",
            "Epoch [59/200], Step [300/312], Loss: 0.4437\n",
            "\n",
            "train-loss: 1.9781, train-acc: 98.0243\n",
            "validation loss: 3.0558, validation acc: 40.0321\n",
            "\n",
            "Epoch 60\n",
            "\n",
            "Epoch [60/200], Step [0/312], Loss: 0.3905\n",
            "Epoch [60/200], Step [20/312], Loss: 0.2507\n",
            "Epoch [60/200], Step [40/312], Loss: 0.1572\n",
            "Epoch [60/200], Step [60/312], Loss: 0.3174\n",
            "Epoch [60/200], Step [80/312], Loss: 0.3500\n",
            "Epoch [60/200], Step [100/312], Loss: 0.3617\n",
            "Epoch [60/200], Step [120/312], Loss: 0.6216\n",
            "Epoch [60/200], Step [140/312], Loss: 0.2757\n",
            "Epoch [60/200], Step [160/312], Loss: 0.2762\n",
            "Epoch [60/200], Step [180/312], Loss: 0.3033\n",
            "Epoch [60/200], Step [200/312], Loss: 0.2793\n",
            "Epoch [60/200], Step [220/312], Loss: 0.2881\n",
            "Epoch [60/200], Step [240/312], Loss: 0.3117\n",
            "Epoch [60/200], Step [260/312], Loss: 0.2692\n",
            "Epoch [60/200], Step [280/312], Loss: 0.2520\n",
            "Epoch [60/200], Step [300/312], Loss: 0.2255\n",
            "\n",
            "train-loss: 1.9496, train-acc: 98.3151\n",
            "validation loss: 3.0541, validation acc: 40.0321\n",
            "\n",
            "Epoch 61\n",
            "\n",
            "Epoch [61/200], Step [0/312], Loss: 0.1055\n",
            "Epoch [61/200], Step [20/312], Loss: 0.2680\n",
            "Epoch [61/200], Step [40/312], Loss: 0.2243\n",
            "Epoch [61/200], Step [60/312], Loss: 0.2417\n",
            "Epoch [61/200], Step [80/312], Loss: 0.2032\n",
            "Epoch [61/200], Step [100/312], Loss: 0.2337\n",
            "Epoch [61/200], Step [120/312], Loss: 0.1145\n",
            "Epoch [61/200], Step [140/312], Loss: 0.1831\n",
            "Epoch [61/200], Step [160/312], Loss: 0.3097\n",
            "Epoch [61/200], Step [180/312], Loss: 0.2034\n",
            "Epoch [61/200], Step [200/312], Loss: 0.2318\n",
            "Epoch [61/200], Step [220/312], Loss: 0.9729\n",
            "Epoch [61/200], Step [240/312], Loss: 0.2313\n",
            "Epoch [61/200], Step [260/312], Loss: 0.2284\n",
            "Epoch [61/200], Step [280/312], Loss: 0.1621\n",
            "Epoch [61/200], Step [300/312], Loss: 0.3427\n",
            "\n",
            "train-loss: 1.9219, train-acc: 98.3352\n",
            "validation loss: 3.0526, validation acc: 40.3129\n",
            "\n",
            "Epoch 62\n",
            "\n",
            "Epoch [62/200], Step [0/312], Loss: 0.2253\n",
            "Epoch [62/200], Step [20/312], Loss: 0.1622\n",
            "Epoch [62/200], Step [40/312], Loss: 0.1625\n",
            "Epoch [62/200], Step [60/312], Loss: 0.2279\n",
            "Epoch [62/200], Step [80/312], Loss: 0.2617\n",
            "Epoch [62/200], Step [100/312], Loss: 0.1516\n",
            "Epoch [62/200], Step [120/312], Loss: 0.2444\n",
            "Epoch [62/200], Step [140/312], Loss: 0.2825\n",
            "Epoch [62/200], Step [160/312], Loss: 0.2166\n",
            "Epoch [62/200], Step [180/312], Loss: 0.1680\n",
            "Epoch [62/200], Step [200/312], Loss: 0.2205\n",
            "Epoch [62/200], Step [220/312], Loss: 0.2860\n",
            "Epoch [62/200], Step [240/312], Loss: 0.1676\n",
            "Epoch [62/200], Step [260/312], Loss: 0.4206\n",
            "Epoch [62/200], Step [280/312], Loss: 0.2219\n",
            "Epoch [62/200], Step [300/312], Loss: 0.1322\n",
            "\n",
            "train-loss: 1.8946, train-acc: 98.7664\n",
            "validation loss: 3.0513, validation acc: 39.8315\n",
            "\n",
            "Epoch 63\n",
            "\n",
            "Epoch [63/200], Step [0/312], Loss: 0.2674\n",
            "Epoch [63/200], Step [20/312], Loss: 0.1261\n",
            "Epoch [63/200], Step [40/312], Loss: 0.1644\n",
            "Epoch [63/200], Step [60/312], Loss: 0.2861\n",
            "Epoch [63/200], Step [80/312], Loss: 0.2545\n",
            "Epoch [63/200], Step [100/312], Loss: 0.1866\n",
            "Epoch [63/200], Step [120/312], Loss: 0.2031\n",
            "Epoch [63/200], Step [140/312], Loss: 0.1298\n",
            "Epoch [63/200], Step [160/312], Loss: 0.2171\n",
            "Epoch [63/200], Step [180/312], Loss: 0.2090\n",
            "Epoch [63/200], Step [200/312], Loss: 0.1367\n",
            "Epoch [63/200], Step [220/312], Loss: 0.1628\n",
            "Epoch [63/200], Step [240/312], Loss: 0.2176\n",
            "Epoch [63/200], Step [260/312], Loss: 0.1834\n",
            "Epoch [63/200], Step [280/312], Loss: 0.1932\n",
            "Epoch [63/200], Step [300/312], Loss: 0.1409\n",
            "\n",
            "train-loss: 1.8681, train-acc: 99.0171\n",
            "validation loss: 3.0500, validation acc: 40.3931\n",
            "\n",
            "Epoch 64\n",
            "\n",
            "Epoch [64/200], Step [0/312], Loss: 0.1632\n",
            "Epoch [64/200], Step [20/312], Loss: 0.2347\n",
            "Epoch [64/200], Step [40/312], Loss: 0.5084\n",
            "Epoch [64/200], Step [60/312], Loss: 0.1492\n",
            "Epoch [64/200], Step [80/312], Loss: 0.1779\n",
            "Epoch [64/200], Step [100/312], Loss: 0.3058\n",
            "Epoch [64/200], Step [120/312], Loss: 0.3205\n",
            "Epoch [64/200], Step [140/312], Loss: 0.2036\n",
            "Epoch [64/200], Step [160/312], Loss: 0.1650\n",
            "Epoch [64/200], Step [180/312], Loss: 0.1678\n",
            "Epoch [64/200], Step [200/312], Loss: 0.1462\n",
            "Epoch [64/200], Step [220/312], Loss: 0.1539\n",
            "Epoch [64/200], Step [240/312], Loss: 0.1564\n",
            "Epoch [64/200], Step [260/312], Loss: 0.3961\n",
            "Epoch [64/200], Step [280/312], Loss: 0.1801\n",
            "Epoch [64/200], Step [300/312], Loss: 0.2365\n",
            "\n",
            "train-loss: 1.8423, train-acc: 98.9269\n",
            "validation loss: 3.0491, validation acc: 39.8716\n",
            "\n",
            "Epoch 65\n",
            "\n",
            "Epoch [65/200], Step [0/312], Loss: 0.1122\n",
            "Epoch [65/200], Step [20/312], Loss: 0.1118\n",
            "Epoch [65/200], Step [40/312], Loss: 0.2447\n",
            "Epoch [65/200], Step [60/312], Loss: 0.1940\n",
            "Epoch [65/200], Step [80/312], Loss: 0.2985\n",
            "Epoch [65/200], Step [100/312], Loss: 0.1310\n",
            "Epoch [65/200], Step [120/312], Loss: 0.1205\n",
            "Epoch [65/200], Step [140/312], Loss: 0.4195\n",
            "Epoch [65/200], Step [160/312], Loss: 0.2086\n",
            "Epoch [65/200], Step [180/312], Loss: 0.2050\n",
            "Epoch [65/200], Step [200/312], Loss: 0.1818\n",
            "Epoch [65/200], Step [220/312], Loss: 0.2218\n",
            "Epoch [65/200], Step [240/312], Loss: 0.1707\n",
            "Epoch [65/200], Step [260/312], Loss: 0.2127\n",
            "Epoch [65/200], Step [280/312], Loss: 0.1569\n",
            "Epoch [65/200], Step [300/312], Loss: 0.2897\n",
            "\n",
            "train-loss: 1.8170, train-acc: 99.1776\n",
            "validation loss: 3.0485, validation acc: 39.7513\n",
            "\n",
            "Epoch 66\n",
            "\n",
            "Epoch [66/200], Step [0/312], Loss: 0.1715\n",
            "Epoch [66/200], Step [20/312], Loss: 0.1455\n",
            "Epoch [66/200], Step [40/312], Loss: 0.2129\n",
            "Epoch [66/200], Step [60/312], Loss: 0.1278\n",
            "Epoch [66/200], Step [80/312], Loss: 0.1243\n",
            "Epoch [66/200], Step [100/312], Loss: 0.1858\n",
            "Epoch [66/200], Step [120/312], Loss: 0.1387\n",
            "Epoch [66/200], Step [140/312], Loss: 0.1834\n",
            "Epoch [66/200], Step [160/312], Loss: 0.1144\n",
            "Epoch [66/200], Step [180/312], Loss: 0.1467\n",
            "Epoch [66/200], Step [200/312], Loss: 0.2372\n",
            "Epoch [66/200], Step [220/312], Loss: 0.2570\n",
            "Epoch [66/200], Step [240/312], Loss: 0.2870\n",
            "Epoch [66/200], Step [260/312], Loss: 0.2637\n",
            "Epoch [66/200], Step [280/312], Loss: 0.1287\n",
            "Epoch [66/200], Step [300/312], Loss: 0.2092\n",
            "\n",
            "train-loss: 1.7923, train-acc: 99.3180\n",
            "validation loss: 3.0477, validation acc: 40.5535\n",
            "\n",
            "Epoch 67\n",
            "\n",
            "Epoch [67/200], Step [0/312], Loss: 0.1510\n",
            "Epoch [67/200], Step [20/312], Loss: 0.1713\n",
            "Epoch [67/200], Step [40/312], Loss: 0.1342\n",
            "Epoch [67/200], Step [60/312], Loss: 0.1389\n",
            "Epoch [67/200], Step [80/312], Loss: 0.2339\n",
            "Epoch [67/200], Step [100/312], Loss: 0.1773\n",
            "Epoch [67/200], Step [120/312], Loss: 0.1287\n",
            "Epoch [67/200], Step [140/312], Loss: 0.2935\n",
            "Epoch [67/200], Step [160/312], Loss: 0.1792\n",
            "Epoch [67/200], Step [180/312], Loss: 0.1180\n",
            "Epoch [67/200], Step [200/312], Loss: 0.1709\n",
            "Epoch [67/200], Step [220/312], Loss: 0.1724\n",
            "Epoch [67/200], Step [240/312], Loss: 0.1184\n",
            "Epoch [67/200], Step [260/312], Loss: 0.2341\n",
            "Epoch [67/200], Step [280/312], Loss: 0.1790\n",
            "Epoch [67/200], Step [300/312], Loss: 0.3373\n",
            "\n",
            "train-loss: 1.7683, train-acc: 99.2779\n",
            "validation loss: 3.0470, validation acc: 39.5507\n",
            "\n",
            "Epoch 68\n",
            "\n",
            "Epoch [68/200], Step [0/312], Loss: 0.1269\n",
            "Epoch [68/200], Step [20/312], Loss: 0.1367\n",
            "Epoch [68/200], Step [40/312], Loss: 0.2002\n",
            "Epoch [68/200], Step [60/312], Loss: 0.0880\n",
            "Epoch [68/200], Step [80/312], Loss: 0.1473\n",
            "Epoch [68/200], Step [100/312], Loss: 0.1838\n",
            "Epoch [68/200], Step [120/312], Loss: 0.2105\n",
            "Epoch [68/200], Step [140/312], Loss: 0.3563\n",
            "Epoch [68/200], Step [160/312], Loss: 0.1118\n",
            "Epoch [68/200], Step [180/312], Loss: 0.1249\n",
            "Epoch [68/200], Step [200/312], Loss: 0.0948\n",
            "Epoch [68/200], Step [220/312], Loss: 0.1976\n",
            "Epoch [68/200], Step [240/312], Loss: 0.1146\n",
            "Epoch [68/200], Step [260/312], Loss: 0.0867\n",
            "Epoch [68/200], Step [280/312], Loss: 0.1280\n",
            "Epoch [68/200], Step [300/312], Loss: 0.1832\n",
            "\n",
            "train-loss: 1.7447, train-acc: 99.4183\n",
            "validation loss: 3.0465, validation acc: 39.9118\n",
            "\n",
            "Epoch 69\n",
            "\n",
            "Epoch [69/200], Step [0/312], Loss: 0.2280\n",
            "Epoch [69/200], Step [20/312], Loss: 0.1771\n",
            "Epoch [69/200], Step [40/312], Loss: 0.1441\n",
            "Epoch [69/200], Step [60/312], Loss: 0.0777\n",
            "Epoch [69/200], Step [80/312], Loss: 0.0700\n",
            "Epoch [69/200], Step [100/312], Loss: 0.0939\n",
            "Epoch [69/200], Step [120/312], Loss: 0.1317\n",
            "Epoch [69/200], Step [140/312], Loss: 0.1147\n",
            "Epoch [69/200], Step [160/312], Loss: 0.1018\n",
            "Epoch [69/200], Step [180/312], Loss: 0.1222\n",
            "Epoch [69/200], Step [200/312], Loss: 0.1600\n",
            "Epoch [69/200], Step [220/312], Loss: 0.1780\n",
            "Epoch [69/200], Step [240/312], Loss: 0.1615\n",
            "Epoch [69/200], Step [260/312], Loss: 0.1568\n",
            "Epoch [69/200], Step [280/312], Loss: 0.2050\n",
            "Epoch [69/200], Step [300/312], Loss: 0.0877\n",
            "\n",
            "train-loss: 1.7218, train-acc: 99.2980\n",
            "validation loss: 3.0464, validation acc: 39.9519\n",
            "\n",
            "Epoch 70\n",
            "\n",
            "Epoch [70/200], Step [0/312], Loss: 0.1121\n",
            "Epoch [70/200], Step [20/312], Loss: 0.1881\n",
            "Epoch [70/200], Step [40/312], Loss: 0.1143\n",
            "Epoch [70/200], Step [60/312], Loss: 0.0916\n",
            "Epoch [70/200], Step [80/312], Loss: 0.1469\n",
            "Epoch [70/200], Step [100/312], Loss: 0.1197\n",
            "Epoch [70/200], Step [120/312], Loss: 0.0976\n",
            "Epoch [70/200], Step [140/312], Loss: 0.1561\n",
            "Epoch [70/200], Step [160/312], Loss: 0.1556\n",
            "Epoch [70/200], Step [180/312], Loss: 0.2827\n",
            "Epoch [70/200], Step [200/312], Loss: 0.0943\n",
            "Epoch [70/200], Step [220/312], Loss: 0.2594\n",
            "Epoch [70/200], Step [240/312], Loss: 0.1379\n",
            "Epoch [70/200], Step [260/312], Loss: 0.2232\n",
            "Epoch [70/200], Step [280/312], Loss: 0.0854\n",
            "Epoch [70/200], Step [300/312], Loss: 0.1617\n",
            "\n",
            "train-loss: 1.6995, train-acc: 99.4183\n",
            "validation loss: 3.0458, validation acc: 39.8716\n",
            "\n",
            "Epoch 71\n",
            "\n",
            "Epoch [71/200], Step [0/312], Loss: 0.1955\n",
            "Epoch [71/200], Step [20/312], Loss: 0.1284\n",
            "Epoch [71/200], Step [40/312], Loss: 0.1453\n",
            "Epoch [71/200], Step [60/312], Loss: 0.0967\n",
            "Epoch [71/200], Step [80/312], Loss: 0.1645\n",
            "Epoch [71/200], Step [100/312], Loss: 0.0845\n",
            "Epoch [71/200], Step [120/312], Loss: 0.2831\n",
            "Epoch [71/200], Step [140/312], Loss: 0.2163\n",
            "Epoch [71/200], Step [160/312], Loss: 0.1572\n",
            "Epoch [71/200], Step [180/312], Loss: 0.1699\n",
            "Epoch [71/200], Step [200/312], Loss: 0.0764\n",
            "Epoch [71/200], Step [220/312], Loss: 0.1560\n",
            "Epoch [71/200], Step [240/312], Loss: 0.1720\n",
            "Epoch [71/200], Step [260/312], Loss: 0.1849\n",
            "Epoch [71/200], Step [280/312], Loss: 0.1497\n",
            "Epoch [71/200], Step [300/312], Loss: 0.2028\n",
            "\n",
            "train-loss: 1.6778, train-acc: 99.3381\n",
            "validation loss: 3.0453, validation acc: 40.3530\n",
            "\n",
            "Epoch 72\n",
            "\n",
            "Epoch [72/200], Step [0/312], Loss: 0.1704\n",
            "Epoch [72/200], Step [20/312], Loss: 0.1423\n",
            "Epoch [72/200], Step [40/312], Loss: 0.1035\n",
            "Epoch [72/200], Step [60/312], Loss: 0.5156\n",
            "Epoch [72/200], Step [80/312], Loss: 0.2226\n",
            "Epoch [72/200], Step [100/312], Loss: 0.1627\n",
            "Epoch [72/200], Step [120/312], Loss: 0.1940\n",
            "Epoch [72/200], Step [140/312], Loss: 0.2308\n",
            "Epoch [72/200], Step [160/312], Loss: 0.1453\n",
            "Epoch [72/200], Step [180/312], Loss: 0.0856\n",
            "Epoch [72/200], Step [200/312], Loss: 0.1100\n",
            "Epoch [72/200], Step [220/312], Loss: 0.1889\n",
            "Epoch [72/200], Step [240/312], Loss: 0.1627\n",
            "Epoch [72/200], Step [260/312], Loss: 0.3137\n",
            "Epoch [72/200], Step [280/312], Loss: 0.1199\n",
            "Epoch [72/200], Step [300/312], Loss: 0.1389\n",
            "\n",
            "train-loss: 1.6566, train-acc: 99.3581\n",
            "validation loss: 3.0451, validation acc: 40.1524\n",
            "\n",
            "Epoch 73\n",
            "\n",
            "Epoch [73/200], Step [0/312], Loss: 0.1012\n",
            "Epoch [73/200], Step [20/312], Loss: 0.1234\n",
            "Epoch [73/200], Step [40/312], Loss: 0.1041\n",
            "Epoch [73/200], Step [60/312], Loss: 0.1012\n",
            "Epoch [73/200], Step [80/312], Loss: 0.1978\n",
            "Epoch [73/200], Step [100/312], Loss: 0.2671\n",
            "Epoch [73/200], Step [120/312], Loss: 0.2824\n",
            "Epoch [73/200], Step [140/312], Loss: 0.0830\n",
            "Epoch [73/200], Step [160/312], Loss: 0.0978\n",
            "Epoch [73/200], Step [180/312], Loss: 0.1132\n",
            "Epoch [73/200], Step [200/312], Loss: 0.1448\n",
            "Epoch [73/200], Step [220/312], Loss: 0.1084\n",
            "Epoch [73/200], Step [240/312], Loss: 0.0866\n",
            "Epoch [73/200], Step [260/312], Loss: 0.1197\n",
            "Epoch [73/200], Step [280/312], Loss: 0.2359\n",
            "Epoch [73/200], Step [300/312], Loss: 0.1863\n",
            "\n",
            "train-loss: 1.6359, train-acc: 99.4584\n",
            "validation loss: 3.0450, validation acc: 40.1524\n",
            "\n",
            "Epoch 74\n",
            "\n",
            "Epoch [74/200], Step [0/312], Loss: 0.1529\n",
            "Epoch [74/200], Step [20/312], Loss: 0.1153\n",
            "Epoch [74/200], Step [40/312], Loss: 0.1412\n",
            "Epoch [74/200], Step [60/312], Loss: 0.1727\n",
            "Epoch [74/200], Step [80/312], Loss: 0.1164\n",
            "Epoch [74/200], Step [100/312], Loss: 0.1038\n",
            "Epoch [74/200], Step [120/312], Loss: 0.1097\n",
            "Epoch [74/200], Step [140/312], Loss: 0.1214\n",
            "Epoch [74/200], Step [160/312], Loss: 0.1542\n",
            "Epoch [74/200], Step [180/312], Loss: 0.1192\n",
            "Epoch [74/200], Step [200/312], Loss: 0.1040\n",
            "Epoch [74/200], Step [220/312], Loss: 0.0944\n",
            "Epoch [74/200], Step [240/312], Loss: 0.4913\n",
            "Epoch [74/200], Step [260/312], Loss: 0.1393\n",
            "Epoch [74/200], Step [280/312], Loss: 0.0732\n",
            "Epoch [74/200], Step [300/312], Loss: 0.0954\n",
            "\n",
            "train-loss: 1.6157, train-acc: 99.4885\n",
            "validation loss: 3.0451, validation acc: 39.3903\n",
            "\n",
            "Epoch 75\n",
            "\n",
            "Epoch [75/200], Step [0/312], Loss: 0.1844\n",
            "Epoch [75/200], Step [20/312], Loss: 0.0615\n",
            "Epoch [75/200], Step [40/312], Loss: 0.0768\n",
            "Epoch [75/200], Step [60/312], Loss: 0.3505\n",
            "Epoch [75/200], Step [80/312], Loss: 0.1298\n",
            "Epoch [75/200], Step [100/312], Loss: 0.1409\n",
            "Epoch [75/200], Step [120/312], Loss: 0.0819\n",
            "Epoch [75/200], Step [140/312], Loss: 0.0859\n",
            "Epoch [75/200], Step [160/312], Loss: 0.0684\n",
            "Epoch [75/200], Step [180/312], Loss: 0.2278\n",
            "Epoch [75/200], Step [200/312], Loss: 0.1768\n",
            "Epoch [75/200], Step [220/312], Loss: 0.1147\n",
            "Epoch [75/200], Step [240/312], Loss: 0.0841\n",
            "Epoch [75/200], Step [260/312], Loss: 0.1698\n",
            "Epoch [75/200], Step [280/312], Loss: 0.1104\n",
            "Epoch [75/200], Step [300/312], Loss: 0.1643\n",
            "\n",
            "train-loss: 1.5959, train-acc: 99.5788\n",
            "validation loss: 3.0452, validation acc: 39.5909\n",
            "\n",
            "Epoch 76\n",
            "\n",
            "Epoch [76/200], Step [0/312], Loss: 0.0714\n",
            "Epoch [76/200], Step [20/312], Loss: 0.1323\n",
            "Epoch [76/200], Step [40/312], Loss: 0.0926\n",
            "Epoch [76/200], Step [60/312], Loss: 0.1105\n",
            "Epoch [76/200], Step [80/312], Loss: 0.0758\n",
            "Epoch [76/200], Step [100/312], Loss: 0.1401\n",
            "Epoch [76/200], Step [120/312], Loss: 0.0956\n",
            "Epoch [76/200], Step [140/312], Loss: 0.0786\n",
            "Epoch [76/200], Step [160/312], Loss: 0.1438\n",
            "Epoch [76/200], Step [180/312], Loss: 0.1073\n",
            "Epoch [76/200], Step [200/312], Loss: 0.0999\n",
            "Epoch [76/200], Step [220/312], Loss: 0.0875\n",
            "Epoch [76/200], Step [240/312], Loss: 0.0621\n",
            "Epoch [76/200], Step [260/312], Loss: 0.0604\n",
            "Epoch [76/200], Step [280/312], Loss: 0.1168\n",
            "Epoch [76/200], Step [300/312], Loss: 0.1487\n",
            "\n",
            "train-loss: 1.5765, train-acc: 99.6390\n",
            "validation loss: 3.0452, validation acc: 40.1123\n",
            "\n",
            "Epoch 77\n",
            "\n",
            "Epoch [77/200], Step [0/312], Loss: 0.0936\n",
            "Epoch [77/200], Step [20/312], Loss: 0.1121\n",
            "Epoch [77/200], Step [40/312], Loss: 0.0692\n",
            "Epoch [77/200], Step [60/312], Loss: 0.0806\n",
            "Epoch [77/200], Step [80/312], Loss: 0.0548\n",
            "Epoch [77/200], Step [100/312], Loss: 0.1219\n",
            "Epoch [77/200], Step [120/312], Loss: 0.0714\n",
            "Epoch [77/200], Step [140/312], Loss: 0.0988\n",
            "Epoch [77/200], Step [160/312], Loss: 0.1133\n",
            "Epoch [77/200], Step [180/312], Loss: 0.1736\n",
            "Epoch [77/200], Step [200/312], Loss: 0.0733\n",
            "Epoch [77/200], Step [220/312], Loss: 0.1319\n",
            "Epoch [77/200], Step [240/312], Loss: 0.1414\n",
            "Epoch [77/200], Step [260/312], Loss: 0.0815\n",
            "Epoch [77/200], Step [280/312], Loss: 0.1807\n",
            "Epoch [77/200], Step [300/312], Loss: 0.0804\n",
            "\n",
            "train-loss: 1.5576, train-acc: 99.5587\n",
            "validation loss: 3.0455, validation acc: 39.7513\n",
            "\n",
            "Epoch 78\n",
            "\n",
            "Epoch [78/200], Step [0/312], Loss: 0.0920\n",
            "Epoch [78/200], Step [20/312], Loss: 0.0823\n",
            "Epoch [78/200], Step [40/312], Loss: 0.0982\n",
            "Epoch [78/200], Step [60/312], Loss: 0.1977\n",
            "Epoch [78/200], Step [80/312], Loss: 0.0959\n",
            "Epoch [78/200], Step [100/312], Loss: 0.1194\n",
            "Epoch [78/200], Step [120/312], Loss: 0.1655\n",
            "Epoch [78/200], Step [140/312], Loss: 0.0605\n",
            "Epoch [78/200], Step [160/312], Loss: 0.1279\n",
            "Epoch [78/200], Step [180/312], Loss: 0.0607\n",
            "Epoch [78/200], Step [200/312], Loss: 0.0809\n",
            "Epoch [78/200], Step [220/312], Loss: 0.1288\n",
            "Epoch [78/200], Step [240/312], Loss: 0.0543\n",
            "Epoch [78/200], Step [260/312], Loss: 0.2148\n",
            "Epoch [78/200], Step [280/312], Loss: 0.0930\n",
            "Epoch [78/200], Step [300/312], Loss: 0.0625\n",
            "\n",
            "train-loss: 1.5391, train-acc: 99.6189\n",
            "validation loss: 3.0456, validation acc: 39.2298\n",
            "\n",
            "Epoch 79\n",
            "\n",
            "Epoch [79/200], Step [0/312], Loss: 0.0638\n",
            "Epoch [79/200], Step [20/312], Loss: 0.1557\n",
            "Epoch [79/200], Step [40/312], Loss: 0.0884\n",
            "Epoch [79/200], Step [60/312], Loss: 0.1199\n",
            "Epoch [79/200], Step [80/312], Loss: 0.1034\n",
            "Epoch [79/200], Step [100/312], Loss: 0.0923\n",
            "Epoch [79/200], Step [120/312], Loss: 0.0504\n",
            "Epoch [79/200], Step [140/312], Loss: 0.0691\n",
            "Epoch [79/200], Step [160/312], Loss: 0.1637\n",
            "Epoch [79/200], Step [180/312], Loss: 0.0702\n",
            "Epoch [79/200], Step [200/312], Loss: 0.0779\n",
            "Epoch [79/200], Step [220/312], Loss: 0.0970\n",
            "Epoch [79/200], Step [240/312], Loss: 0.2393\n",
            "Epoch [79/200], Step [260/312], Loss: 0.2171\n",
            "Epoch [79/200], Step [280/312], Loss: 0.0964\n",
            "Epoch [79/200], Step [300/312], Loss: 0.1479\n",
            "\n",
            "train-loss: 1.5211, train-acc: 99.7092\n",
            "validation loss: 3.0457, validation acc: 39.7112\n",
            "\n",
            "Epoch 80\n",
            "\n",
            "Epoch [80/200], Step [0/312], Loss: 0.1820\n",
            "Epoch [80/200], Step [20/312], Loss: 0.0712\n",
            "Epoch [80/200], Step [40/312], Loss: 0.1436\n",
            "Epoch [80/200], Step [60/312], Loss: 0.0869\n",
            "Epoch [80/200], Step [80/312], Loss: 0.1697\n",
            "Epoch [80/200], Step [100/312], Loss: 0.0957\n",
            "Epoch [80/200], Step [120/312], Loss: 0.1133\n",
            "Epoch [80/200], Step [140/312], Loss: 0.1328\n",
            "Epoch [80/200], Step [160/312], Loss: 0.1163\n",
            "Epoch [80/200], Step [180/312], Loss: 0.1113\n",
            "Epoch [80/200], Step [200/312], Loss: 0.1097\n",
            "Epoch [80/200], Step [220/312], Loss: 0.2146\n",
            "Epoch [80/200], Step [240/312], Loss: 0.0708\n",
            "Epoch [80/200], Step [260/312], Loss: 0.1184\n",
            "Epoch [80/200], Step [280/312], Loss: 0.1161\n",
            "Epoch [80/200], Step [300/312], Loss: 0.0835\n",
            "\n",
            "train-loss: 1.5035, train-acc: 99.6089\n",
            "validation loss: 3.0458, validation acc: 39.3903\n",
            "\n",
            "Epoch 81\n",
            "\n",
            "Epoch [81/200], Step [0/312], Loss: 0.0477\n",
            "Epoch [81/200], Step [20/312], Loss: 0.1012\n",
            "Epoch [81/200], Step [40/312], Loss: 0.1286\n",
            "Epoch [81/200], Step [60/312], Loss: 0.0578\n",
            "Epoch [81/200], Step [80/312], Loss: 0.0608\n",
            "Epoch [81/200], Step [100/312], Loss: 0.1282\n",
            "Epoch [81/200], Step [120/312], Loss: 0.1052\n",
            "Epoch [81/200], Step [140/312], Loss: 0.0661\n",
            "Epoch [81/200], Step [160/312], Loss: 0.0924\n",
            "Epoch [81/200], Step [180/312], Loss: 0.1271\n",
            "Epoch [81/200], Step [200/312], Loss: 0.0699\n",
            "Epoch [81/200], Step [220/312], Loss: 0.0800\n",
            "Epoch [81/200], Step [240/312], Loss: 0.1229\n",
            "Epoch [81/200], Step [260/312], Loss: 0.0428\n",
            "Epoch [81/200], Step [280/312], Loss: 0.1281\n",
            "Epoch [81/200], Step [300/312], Loss: 0.0374\n",
            "\n",
            "train-loss: 1.4862, train-acc: 99.7292\n",
            "validation loss: 3.0459, validation acc: 40.0321\n",
            "\n",
            "Epoch 82\n",
            "\n",
            "Epoch [82/200], Step [0/312], Loss: 0.1078\n",
            "Epoch [82/200], Step [20/312], Loss: 0.0964\n",
            "Epoch [82/200], Step [40/312], Loss: 0.1480\n",
            "Epoch [82/200], Step [60/312], Loss: 0.1304\n",
            "Epoch [82/200], Step [80/312], Loss: 0.0795\n",
            "Epoch [82/200], Step [100/312], Loss: 0.0947\n",
            "Epoch [82/200], Step [120/312], Loss: 0.1787\n",
            "Epoch [82/200], Step [140/312], Loss: 0.0981\n",
            "Epoch [82/200], Step [160/312], Loss: 0.0998\n",
            "Epoch [82/200], Step [180/312], Loss: 0.0978\n",
            "Epoch [82/200], Step [200/312], Loss: 0.0593\n",
            "Epoch [82/200], Step [220/312], Loss: 0.0845\n",
            "Epoch [82/200], Step [240/312], Loss: 0.1160\n",
            "Epoch [82/200], Step [260/312], Loss: 0.0815\n",
            "Epoch [82/200], Step [280/312], Loss: 0.0806\n",
            "Epoch [82/200], Step [300/312], Loss: 0.0939\n",
            "\n",
            "train-loss: 1.4693, train-acc: 99.6891\n",
            "validation loss: 3.0464, validation acc: 39.3101\n",
            "\n",
            "Epoch 83\n",
            "\n",
            "Epoch [83/200], Step [0/312], Loss: 0.0382\n",
            "Epoch [83/200], Step [20/312], Loss: 0.0562\n",
            "Epoch [83/200], Step [40/312], Loss: 0.1107\n",
            "Epoch [83/200], Step [60/312], Loss: 0.0782\n",
            "Epoch [83/200], Step [80/312], Loss: 0.3672\n",
            "Epoch [83/200], Step [100/312], Loss: 0.0731\n",
            "Epoch [83/200], Step [120/312], Loss: 0.0966\n",
            "Epoch [83/200], Step [140/312], Loss: 0.1294\n",
            "Epoch [83/200], Step [160/312], Loss: 0.1854\n",
            "Epoch [83/200], Step [180/312], Loss: 0.1220\n",
            "Epoch [83/200], Step [200/312], Loss: 0.0636\n",
            "Epoch [83/200], Step [220/312], Loss: 0.0477\n",
            "Epoch [83/200], Step [240/312], Loss: 0.1066\n",
            "Epoch [83/200], Step [260/312], Loss: 0.0418\n",
            "Epoch [83/200], Step [280/312], Loss: 0.0788\n",
            "Epoch [83/200], Step [300/312], Loss: 0.0811\n",
            "\n",
            "train-loss: 1.4528, train-acc: 99.7392\n",
            "validation loss: 3.0466, validation acc: 39.7914\n",
            "\n",
            "Epoch 84\n",
            "\n",
            "Epoch [84/200], Step [0/312], Loss: 0.1315\n",
            "Epoch [84/200], Step [20/312], Loss: 0.0838\n",
            "Epoch [84/200], Step [40/312], Loss: 0.0432\n",
            "Epoch [84/200], Step [60/312], Loss: 0.0790\n",
            "Epoch [84/200], Step [80/312], Loss: 0.0609\n",
            "Epoch [84/200], Step [100/312], Loss: 0.1783\n",
            "Epoch [84/200], Step [120/312], Loss: 0.0655\n",
            "Epoch [84/200], Step [140/312], Loss: 0.1175\n",
            "Epoch [84/200], Step [160/312], Loss: 0.0971\n",
            "Epoch [84/200], Step [180/312], Loss: 0.1641\n",
            "Epoch [84/200], Step [200/312], Loss: 0.0819\n",
            "Epoch [84/200], Step [220/312], Loss: 0.0942\n",
            "Epoch [84/200], Step [240/312], Loss: 0.0971\n",
            "Epoch [84/200], Step [260/312], Loss: 0.0934\n",
            "Epoch [84/200], Step [280/312], Loss: 0.1796\n",
            "Epoch [84/200], Step [300/312], Loss: 0.0981\n",
            "\n",
            "train-loss: 1.4366, train-acc: 99.6690\n",
            "validation loss: 3.0465, validation acc: 39.5909\n",
            "\n",
            "Epoch 85\n",
            "\n",
            "Epoch [85/200], Step [0/312], Loss: 0.0926\n",
            "Epoch [85/200], Step [20/312], Loss: 0.0992\n",
            "Epoch [85/200], Step [40/312], Loss: 0.0552\n",
            "Epoch [85/200], Step [60/312], Loss: 0.0612\n",
            "Epoch [85/200], Step [80/312], Loss: 0.1037\n",
            "Epoch [85/200], Step [100/312], Loss: 0.0893\n",
            "Epoch [85/200], Step [120/312], Loss: 0.0653\n",
            "Epoch [85/200], Step [140/312], Loss: 0.0918\n",
            "Epoch [85/200], Step [160/312], Loss: 0.0643\n",
            "Epoch [85/200], Step [180/312], Loss: 0.1618\n",
            "Epoch [85/200], Step [200/312], Loss: 0.0431\n",
            "Epoch [85/200], Step [220/312], Loss: 0.0550\n",
            "Epoch [85/200], Step [240/312], Loss: 0.2087\n",
            "Epoch [85/200], Step [260/312], Loss: 0.1756\n",
            "Epoch [85/200], Step [280/312], Loss: 0.0763\n",
            "Epoch [85/200], Step [300/312], Loss: 0.0619\n",
            "\n",
            "train-loss: 1.4208, train-acc: 99.7493\n",
            "validation loss: 3.0467, validation acc: 39.9920\n",
            "\n",
            "Epoch 86\n",
            "\n",
            "Epoch [86/200], Step [0/312], Loss: 0.1234\n",
            "Epoch [86/200], Step [20/312], Loss: 0.0862\n",
            "Epoch [86/200], Step [40/312], Loss: 0.0572\n",
            "Epoch [86/200], Step [60/312], Loss: 0.0987\n",
            "Epoch [86/200], Step [80/312], Loss: 0.0757\n",
            "Epoch [86/200], Step [100/312], Loss: 0.0522\n",
            "Epoch [86/200], Step [120/312], Loss: 0.1351\n",
            "Epoch [86/200], Step [140/312], Loss: 0.0778\n",
            "Epoch [86/200], Step [160/312], Loss: 0.0834\n",
            "Epoch [86/200], Step [180/312], Loss: 0.0809\n",
            "Epoch [86/200], Step [200/312], Loss: 0.1008\n",
            "Epoch [86/200], Step [220/312], Loss: 0.0518\n",
            "Epoch [86/200], Step [240/312], Loss: 0.0832\n",
            "Epoch [86/200], Step [260/312], Loss: 0.0541\n",
            "Epoch [86/200], Step [280/312], Loss: 0.0432\n",
            "Epoch [86/200], Step [300/312], Loss: 0.1312\n",
            "\n",
            "train-loss: 1.4054, train-acc: 99.7392\n",
            "validation loss: 3.0469, validation acc: 39.6711\n",
            "\n",
            "Epoch 87\n",
            "\n",
            "Epoch [87/200], Step [0/312], Loss: 0.0602\n",
            "Epoch [87/200], Step [20/312], Loss: 0.1269\n",
            "Epoch [87/200], Step [40/312], Loss: 0.0424\n",
            "Epoch [87/200], Step [60/312], Loss: 0.0496\n",
            "Epoch [87/200], Step [80/312], Loss: 0.0499\n",
            "Epoch [87/200], Step [100/312], Loss: 0.0506\n",
            "Epoch [87/200], Step [120/312], Loss: 0.0624\n",
            "Epoch [87/200], Step [140/312], Loss: 0.0452\n",
            "Epoch [87/200], Step [160/312], Loss: 0.0713\n",
            "Epoch [87/200], Step [180/312], Loss: 0.0808\n",
            "Epoch [87/200], Step [200/312], Loss: 0.0984\n",
            "Epoch [87/200], Step [220/312], Loss: 0.1083\n",
            "Epoch [87/200], Step [240/312], Loss: 0.0555\n",
            "Epoch [87/200], Step [260/312], Loss: 0.0674\n",
            "Epoch [87/200], Step [280/312], Loss: 0.0957\n",
            "Epoch [87/200], Step [300/312], Loss: 0.0806\n",
            "\n",
            "train-loss: 1.3902, train-acc: 99.8295\n",
            "validation loss: 3.0471, validation acc: 39.6711\n",
            "\n",
            "Epoch 88\n",
            "\n",
            "Epoch [88/200], Step [0/312], Loss: 0.1500\n",
            "Epoch [88/200], Step [20/312], Loss: 0.1533\n",
            "Epoch [88/200], Step [40/312], Loss: 0.1380\n",
            "Epoch [88/200], Step [60/312], Loss: 0.1548\n",
            "Epoch [88/200], Step [80/312], Loss: 0.0467\n",
            "Epoch [88/200], Step [100/312], Loss: 0.0413\n",
            "Epoch [88/200], Step [120/312], Loss: 0.1279\n",
            "Epoch [88/200], Step [140/312], Loss: 0.0612\n",
            "Epoch [88/200], Step [160/312], Loss: 0.0560\n",
            "Epoch [88/200], Step [180/312], Loss: 0.1327\n",
            "Epoch [88/200], Step [200/312], Loss: 0.0600\n",
            "Epoch [88/200], Step [220/312], Loss: 0.0579\n",
            "Epoch [88/200], Step [240/312], Loss: 0.0968\n",
            "Epoch [88/200], Step [260/312], Loss: 0.0802\n",
            "Epoch [88/200], Step [280/312], Loss: 0.0721\n",
            "Epoch [88/200], Step [300/312], Loss: 0.0416\n",
            "\n",
            "train-loss: 1.3754, train-acc: 99.8395\n",
            "validation loss: 3.0475, validation acc: 39.7112\n",
            "\n",
            "Epoch 89\n",
            "\n",
            "Epoch [89/200], Step [0/312], Loss: 0.1050\n",
            "Epoch [89/200], Step [20/312], Loss: 0.0534\n",
            "Epoch [89/200], Step [40/312], Loss: 0.1547\n",
            "Epoch [89/200], Step [60/312], Loss: 0.0984\n",
            "Epoch [89/200], Step [80/312], Loss: 0.0710\n",
            "Epoch [89/200], Step [100/312], Loss: 0.2056\n",
            "Epoch [89/200], Step [120/312], Loss: 0.1625\n",
            "Epoch [89/200], Step [140/312], Loss: 0.1646\n",
            "Epoch [89/200], Step [160/312], Loss: 0.1259\n",
            "Epoch [89/200], Step [180/312], Loss: 0.1153\n",
            "Epoch [89/200], Step [200/312], Loss: 0.0548\n",
            "Epoch [89/200], Step [220/312], Loss: 0.0455\n",
            "Epoch [89/200], Step [240/312], Loss: 0.1982\n",
            "Epoch [89/200], Step [260/312], Loss: 0.1455\n",
            "Epoch [89/200], Step [280/312], Loss: 0.0700\n",
            "Epoch [89/200], Step [300/312], Loss: 0.0379\n",
            "\n",
            "train-loss: 1.3609, train-acc: 99.7493\n",
            "validation loss: 3.0479, validation acc: 40.1524\n",
            "\n",
            "Epoch 90\n",
            "\n",
            "Epoch [90/200], Step [0/312], Loss: 0.1416\n",
            "Epoch [90/200], Step [20/312], Loss: 0.0819\n",
            "Epoch [90/200], Step [40/312], Loss: 0.0512\n",
            "Epoch [90/200], Step [60/312], Loss: 0.1062\n",
            "Epoch [90/200], Step [80/312], Loss: 0.0798\n",
            "Epoch [90/200], Step [100/312], Loss: 0.1132\n",
            "Epoch [90/200], Step [120/312], Loss: 0.1767\n",
            "Epoch [90/200], Step [140/312], Loss: 0.0653\n",
            "Epoch [90/200], Step [160/312], Loss: 0.0636\n",
            "Epoch [90/200], Step [180/312], Loss: 0.0873\n",
            "Epoch [90/200], Step [200/312], Loss: 0.0336\n",
            "Epoch [90/200], Step [220/312], Loss: 0.0479\n",
            "Epoch [90/200], Step [240/312], Loss: 0.0549\n",
            "Epoch [90/200], Step [260/312], Loss: 0.1384\n",
            "Epoch [90/200], Step [280/312], Loss: 0.0351\n",
            "Epoch [90/200], Step [300/312], Loss: 0.0508\n",
            "\n",
            "train-loss: 1.3467, train-acc: 99.7493\n",
            "validation loss: 3.0484, validation acc: 40.0321\n",
            "\n",
            "Epoch 91\n",
            "\n",
            "Epoch [91/200], Step [0/312], Loss: 0.1092\n",
            "Epoch [91/200], Step [20/312], Loss: 0.0456\n",
            "Epoch [91/200], Step [40/312], Loss: 0.0555\n",
            "Epoch [91/200], Step [60/312], Loss: 0.1090\n",
            "Epoch [91/200], Step [80/312], Loss: 0.1005\n",
            "Epoch [91/200], Step [100/312], Loss: 0.0726\n",
            "Epoch [91/200], Step [120/312], Loss: 0.1678\n",
            "Epoch [91/200], Step [140/312], Loss: 0.0819\n",
            "Epoch [91/200], Step [160/312], Loss: 0.0491\n",
            "Epoch [91/200], Step [180/312], Loss: 0.0553\n",
            "Epoch [91/200], Step [200/312], Loss: 0.0543\n",
            "Epoch [91/200], Step [220/312], Loss: 0.0685\n",
            "Epoch [91/200], Step [240/312], Loss: 0.0952\n",
            "Epoch [91/200], Step [260/312], Loss: 0.0438\n",
            "Epoch [91/200], Step [280/312], Loss: 0.0460\n",
            "Epoch [91/200], Step [300/312], Loss: 0.0815\n",
            "\n",
            "train-loss: 1.3328, train-acc: 99.7593\n",
            "validation loss: 3.0486, validation acc: 39.7112\n",
            "\n",
            "Epoch 92\n",
            "\n",
            "Epoch [92/200], Step [0/312], Loss: 0.0459\n",
            "Epoch [92/200], Step [20/312], Loss: 0.1753\n",
            "Epoch [92/200], Step [40/312], Loss: 0.0879\n",
            "Epoch [92/200], Step [60/312], Loss: 0.0524\n",
            "Epoch [92/200], Step [80/312], Loss: 0.0545\n",
            "Epoch [92/200], Step [100/312], Loss: 0.0860\n",
            "Epoch [92/200], Step [120/312], Loss: 0.0528\n",
            "Epoch [92/200], Step [140/312], Loss: 0.0807\n",
            "Epoch [92/200], Step [160/312], Loss: 0.1112\n",
            "Epoch [92/200], Step [180/312], Loss: 0.1460\n",
            "Epoch [92/200], Step [200/312], Loss: 0.0793\n",
            "Epoch [92/200], Step [220/312], Loss: 0.0516\n",
            "Epoch [92/200], Step [240/312], Loss: 0.0819\n",
            "Epoch [92/200], Step [260/312], Loss: 0.0380\n",
            "Epoch [92/200], Step [280/312], Loss: 0.0594\n",
            "Epoch [92/200], Step [300/312], Loss: 0.0702\n",
            "\n",
            "train-loss: 1.3192, train-acc: 99.7994\n",
            "validation loss: 3.0488, validation acc: 40.1524\n",
            "\n",
            "Epoch 93\n",
            "\n",
            "Epoch [93/200], Step [0/312], Loss: 0.1968\n",
            "Epoch [93/200], Step [20/312], Loss: 0.0525\n",
            "Epoch [93/200], Step [40/312], Loss: 0.0894\n",
            "Epoch [93/200], Step [60/312], Loss: 0.0769\n",
            "Epoch [93/200], Step [80/312], Loss: 0.0420\n",
            "Epoch [93/200], Step [100/312], Loss: 0.1460\n",
            "Epoch [93/200], Step [120/312], Loss: 0.0773\n",
            "Epoch [93/200], Step [140/312], Loss: 0.0671\n",
            "Epoch [93/200], Step [160/312], Loss: 0.0570\n",
            "Epoch [93/200], Step [180/312], Loss: 0.0539\n",
            "Epoch [93/200], Step [200/312], Loss: 0.0673\n",
            "Epoch [93/200], Step [220/312], Loss: 0.1052\n",
            "Epoch [93/200], Step [240/312], Loss: 0.0789\n",
            "Epoch [93/200], Step [260/312], Loss: 0.0775\n",
            "Epoch [93/200], Step [280/312], Loss: 0.0935\n",
            "Epoch [93/200], Step [300/312], Loss: 0.1181\n",
            "\n",
            "train-loss: 1.3059, train-acc: 99.8395\n",
            "validation loss: 3.0493, validation acc: 39.4304\n",
            "\n",
            "Epoch 94\n",
            "\n",
            "Epoch [94/200], Step [0/312], Loss: 0.0583\n",
            "Epoch [94/200], Step [20/312], Loss: 0.1356\n",
            "Epoch [94/200], Step [40/312], Loss: 0.0820\n",
            "Epoch [94/200], Step [60/312], Loss: 0.1186\n",
            "Epoch [94/200], Step [80/312], Loss: 0.0360\n",
            "Epoch [94/200], Step [100/312], Loss: 0.0515\n",
            "Epoch [94/200], Step [120/312], Loss: 0.1335\n",
            "Epoch [94/200], Step [140/312], Loss: 0.1041\n",
            "Epoch [94/200], Step [160/312], Loss: 0.0465\n",
            "Epoch [94/200], Step [180/312], Loss: 0.0592\n",
            "Epoch [94/200], Step [200/312], Loss: 0.0939\n",
            "Epoch [94/200], Step [220/312], Loss: 0.0620\n",
            "Epoch [94/200], Step [240/312], Loss: 0.0489\n",
            "Epoch [94/200], Step [260/312], Loss: 0.0701\n",
            "Epoch [94/200], Step [280/312], Loss: 0.1913\n",
            "Epoch [94/200], Step [300/312], Loss: 0.0641\n",
            "\n",
            "train-loss: 1.2928, train-acc: 99.7794\n",
            "validation loss: 3.0499, validation acc: 39.1897\n",
            "\n",
            "Epoch 95\n",
            "\n",
            "Epoch [95/200], Step [0/312], Loss: 0.0389\n",
            "Epoch [95/200], Step [20/312], Loss: 0.1072\n",
            "Epoch [95/200], Step [40/312], Loss: 0.0577\n",
            "Epoch [95/200], Step [60/312], Loss: 0.0637\n",
            "Epoch [95/200], Step [80/312], Loss: 0.0460\n",
            "Epoch [95/200], Step [100/312], Loss: 0.0879\n",
            "Epoch [95/200], Step [120/312], Loss: 0.1659\n",
            "Epoch [95/200], Step [140/312], Loss: 0.0726\n",
            "Epoch [95/200], Step [160/312], Loss: 0.0655\n",
            "Epoch [95/200], Step [180/312], Loss: 0.0970\n",
            "Epoch [95/200], Step [200/312], Loss: 0.0370\n",
            "Epoch [95/200], Step [220/312], Loss: 0.0706\n",
            "Epoch [95/200], Step [240/312], Loss: 0.0268\n",
            "Epoch [95/200], Step [260/312], Loss: 0.1521\n",
            "Epoch [95/200], Step [280/312], Loss: 0.1251\n",
            "Epoch [95/200], Step [300/312], Loss: 0.0652\n",
            "\n",
            "train-loss: 1.2800, train-acc: 99.8596\n",
            "validation loss: 3.0502, validation acc: 39.3903\n",
            "\n",
            "Epoch 96\n",
            "\n",
            "Epoch [96/200], Step [0/312], Loss: 0.0420\n",
            "Epoch [96/200], Step [20/312], Loss: 0.0446\n",
            "Epoch [96/200], Step [40/312], Loss: 0.0417\n",
            "Epoch [96/200], Step [60/312], Loss: 0.0751\n",
            "Epoch [96/200], Step [80/312], Loss: 0.1467\n",
            "Epoch [96/200], Step [100/312], Loss: 0.0421\n",
            "Epoch [96/200], Step [120/312], Loss: 0.0425\n",
            "Epoch [96/200], Step [140/312], Loss: 0.1113\n",
            "Epoch [96/200], Step [160/312], Loss: 0.0948\n",
            "Epoch [96/200], Step [180/312], Loss: 0.0796\n",
            "Epoch [96/200], Step [200/312], Loss: 0.0877\n",
            "Epoch [96/200], Step [220/312], Loss: 0.3643\n",
            "Epoch [96/200], Step [240/312], Loss: 0.0738\n",
            "Epoch [96/200], Step [260/312], Loss: 0.0391\n",
            "Epoch [96/200], Step [280/312], Loss: 0.0581\n",
            "Epoch [96/200], Step [300/312], Loss: 0.0582\n",
            "\n",
            "train-loss: 1.2674, train-acc: 99.7392\n",
            "validation loss: 3.0505, validation acc: 39.5106\n",
            "\n",
            "Epoch 97\n",
            "\n",
            "Epoch [97/200], Step [0/312], Loss: 0.0463\n",
            "Epoch [97/200], Step [20/312], Loss: 0.0752\n",
            "Epoch [97/200], Step [40/312], Loss: 0.0422\n",
            "Epoch [97/200], Step [60/312], Loss: 0.0366\n",
            "Epoch [97/200], Step [80/312], Loss: 0.0383\n",
            "Epoch [97/200], Step [100/312], Loss: 0.0615\n",
            "Epoch [97/200], Step [120/312], Loss: 0.0454\n",
            "Epoch [97/200], Step [140/312], Loss: 0.0699\n",
            "Epoch [97/200], Step [160/312], Loss: 0.0548\n",
            "Epoch [97/200], Step [180/312], Loss: 0.0414\n",
            "Epoch [97/200], Step [200/312], Loss: 0.0706\n",
            "Epoch [97/200], Step [220/312], Loss: 0.1007\n",
            "Epoch [97/200], Step [240/312], Loss: 0.0468\n",
            "Epoch [97/200], Step [260/312], Loss: 0.0306\n",
            "Epoch [97/200], Step [280/312], Loss: 0.0435\n",
            "Epoch [97/200], Step [300/312], Loss: 0.0414\n",
            "\n",
            "train-loss: 1.2551, train-acc: 99.7794\n",
            "validation loss: 3.0510, validation acc: 38.9491\n",
            "\n",
            "Epoch 98\n",
            "\n",
            "Epoch [98/200], Step [0/312], Loss: 0.0691\n",
            "Epoch [98/200], Step [20/312], Loss: 0.1464\n",
            "Epoch [98/200], Step [40/312], Loss: 0.0511\n",
            "Epoch [98/200], Step [60/312], Loss: 0.0407\n",
            "Epoch [98/200], Step [80/312], Loss: 0.1201\n",
            "Epoch [98/200], Step [100/312], Loss: 0.1489\n",
            "Epoch [98/200], Step [120/312], Loss: 0.1019\n",
            "Epoch [98/200], Step [140/312], Loss: 0.0832\n",
            "Epoch [98/200], Step [160/312], Loss: 0.0595\n",
            "Epoch [98/200], Step [180/312], Loss: 0.0636\n",
            "Epoch [98/200], Step [200/312], Loss: 0.0547\n",
            "Epoch [98/200], Step [220/312], Loss: 0.0398\n",
            "Epoch [98/200], Step [240/312], Loss: 0.0565\n",
            "Epoch [98/200], Step [260/312], Loss: 0.0422\n",
            "Epoch [98/200], Step [280/312], Loss: 0.0458\n",
            "Epoch [98/200], Step [300/312], Loss: 0.0616\n",
            "\n",
            "train-loss: 1.2430, train-acc: 99.7894\n",
            "validation loss: 3.0515, validation acc: 39.7513\n",
            "\n",
            "Epoch 99\n",
            "\n",
            "Epoch [99/200], Step [0/312], Loss: 0.0668\n",
            "Epoch [99/200], Step [20/312], Loss: 0.0961\n",
            "Epoch [99/200], Step [40/312], Loss: 0.0923\n",
            "Epoch [99/200], Step [60/312], Loss: 0.0575\n",
            "Epoch [99/200], Step [80/312], Loss: 0.0946\n",
            "Epoch [99/200], Step [100/312], Loss: 0.1730\n",
            "Epoch [99/200], Step [120/312], Loss: 0.0468\n",
            "Epoch [99/200], Step [140/312], Loss: 0.1485\n",
            "Epoch [99/200], Step [160/312], Loss: 0.1316\n",
            "Epoch [99/200], Step [180/312], Loss: 0.0936\n",
            "Epoch [99/200], Step [200/312], Loss: 0.0564\n",
            "Epoch [99/200], Step [220/312], Loss: 0.0591\n",
            "Epoch [99/200], Step [240/312], Loss: 0.0921\n",
            "Epoch [99/200], Step [260/312], Loss: 0.0238\n",
            "Epoch [99/200], Step [280/312], Loss: 0.0827\n",
            "Epoch [99/200], Step [300/312], Loss: 0.0743\n",
            "\n",
            "train-loss: 1.2312, train-acc: 99.8195\n",
            "validation loss: 3.0520, validation acc: 39.7914\n",
            "\n",
            "Epoch 100\n",
            "\n",
            "Epoch [100/200], Step [0/312], Loss: 0.0795\n",
            "Epoch [100/200], Step [20/312], Loss: 0.1441\n",
            "Epoch [100/200], Step [40/312], Loss: 0.1325\n",
            "Epoch [100/200], Step [60/312], Loss: 0.2159\n",
            "Epoch [100/200], Step [80/312], Loss: 0.0282\n",
            "Epoch [100/200], Step [100/312], Loss: 0.0670\n",
            "Epoch [100/200], Step [120/312], Loss: 0.0410\n",
            "Epoch [100/200], Step [140/312], Loss: 0.1067\n",
            "Epoch [100/200], Step [160/312], Loss: 0.1232\n",
            "Epoch [100/200], Step [180/312], Loss: 0.0839\n",
            "Epoch [100/200], Step [200/312], Loss: 0.0568\n",
            "Epoch [100/200], Step [220/312], Loss: 0.1233\n",
            "Epoch [100/200], Step [240/312], Loss: 0.1138\n",
            "Epoch [100/200], Step [260/312], Loss: 0.0370\n",
            "Epoch [100/200], Step [280/312], Loss: 0.0748\n",
            "Epoch [100/200], Step [300/312], Loss: 0.0333\n",
            "\n",
            "train-loss: 1.2196, train-acc: 99.8797\n",
            "validation loss: 3.0525, validation acc: 39.5507\n",
            "\n",
            "Epoch 101\n",
            "\n",
            "Epoch [101/200], Step [0/312], Loss: 0.0267\n",
            "Epoch [101/200], Step [20/312], Loss: 0.0735\n",
            "Epoch [101/200], Step [40/312], Loss: 0.0456\n",
            "Epoch [101/200], Step [60/312], Loss: 0.0782\n",
            "Epoch [101/200], Step [80/312], Loss: 0.0399\n",
            "Epoch [101/200], Step [100/312], Loss: 0.0355\n",
            "Epoch [101/200], Step [120/312], Loss: 0.0325\n",
            "Epoch [101/200], Step [140/312], Loss: 0.0327\n",
            "Epoch [101/200], Step [160/312], Loss: 0.1029\n",
            "Epoch [101/200], Step [180/312], Loss: 0.0384\n",
            "Epoch [101/200], Step [200/312], Loss: 0.1773\n",
            "Epoch [101/200], Step [220/312], Loss: 0.0299\n",
            "Epoch [101/200], Step [240/312], Loss: 0.0589\n",
            "Epoch [101/200], Step [260/312], Loss: 0.0497\n",
            "Epoch [101/200], Step [280/312], Loss: 0.1043\n",
            "Epoch [101/200], Step [300/312], Loss: 0.0345\n",
            "\n",
            "train-loss: 1.2081, train-acc: 99.8496\n",
            "validation loss: 3.0530, validation acc: 39.8315\n",
            "\n",
            "Epoch 102\n",
            "\n",
            "Epoch [102/200], Step [0/312], Loss: 0.0743\n",
            "Epoch [102/200], Step [20/312], Loss: 0.0810\n",
            "Epoch [102/200], Step [40/312], Loss: 0.0705\n",
            "Epoch [102/200], Step [60/312], Loss: 0.0281\n",
            "Epoch [102/200], Step [80/312], Loss: 0.1303\n",
            "Epoch [102/200], Step [100/312], Loss: 0.2255\n",
            "Epoch [102/200], Step [120/312], Loss: 0.0376\n",
            "Epoch [102/200], Step [140/312], Loss: 0.1089\n",
            "Epoch [102/200], Step [160/312], Loss: 0.0590\n",
            "Epoch [102/200], Step [180/312], Loss: 0.0391\n",
            "Epoch [102/200], Step [200/312], Loss: 0.0270\n",
            "Epoch [102/200], Step [220/312], Loss: 0.0481\n",
            "Epoch [102/200], Step [240/312], Loss: 0.0684\n",
            "Epoch [102/200], Step [260/312], Loss: 0.0507\n",
            "Epoch [102/200], Step [280/312], Loss: 0.0397\n",
            "Epoch [102/200], Step [300/312], Loss: 0.0344\n",
            "\n",
            "train-loss: 1.1969, train-acc: 99.8195\n",
            "validation loss: 3.0536, validation acc: 39.4705\n",
            "\n",
            "Epoch 103\n",
            "\n",
            "Epoch [103/200], Step [0/312], Loss: 0.0959\n",
            "Epoch [103/200], Step [20/312], Loss: 0.0293\n",
            "Epoch [103/200], Step [40/312], Loss: 0.0895\n",
            "Epoch [103/200], Step [60/312], Loss: 0.0355\n",
            "Epoch [103/200], Step [80/312], Loss: 0.0402\n",
            "Epoch [103/200], Step [100/312], Loss: 0.0398\n",
            "Epoch [103/200], Step [120/312], Loss: 0.1973\n",
            "Epoch [103/200], Step [140/312], Loss: 0.1633\n",
            "Epoch [103/200], Step [160/312], Loss: 0.3608\n",
            "Epoch [103/200], Step [180/312], Loss: 0.0771\n",
            "Epoch [103/200], Step [200/312], Loss: 0.0361\n",
            "Epoch [103/200], Step [220/312], Loss: 0.0831\n",
            "Epoch [103/200], Step [240/312], Loss: 0.0345\n",
            "Epoch [103/200], Step [260/312], Loss: 0.0968\n",
            "Epoch [103/200], Step [280/312], Loss: 0.0395\n",
            "Epoch [103/200], Step [300/312], Loss: 0.0442\n",
            "\n",
            "train-loss: 1.1859, train-acc: 99.8395\n",
            "validation loss: 3.0540, validation acc: 40.2327\n",
            "\n",
            "Epoch 104\n",
            "\n",
            "Epoch [104/200], Step [0/312], Loss: 0.0588\n",
            "Epoch [104/200], Step [20/312], Loss: 0.1113\n",
            "Epoch [104/200], Step [40/312], Loss: 0.0766\n",
            "Epoch [104/200], Step [60/312], Loss: 0.0267\n",
            "Epoch [104/200], Step [80/312], Loss: 0.1021\n",
            "Epoch [104/200], Step [100/312], Loss: 0.0750\n",
            "Epoch [104/200], Step [120/312], Loss: 0.0465\n",
            "Epoch [104/200], Step [140/312], Loss: 0.0499\n",
            "Epoch [104/200], Step [160/312], Loss: 0.0497\n",
            "Epoch [104/200], Step [180/312], Loss: 0.0361\n",
            "Epoch [104/200], Step [200/312], Loss: 0.0301\n",
            "Epoch [104/200], Step [220/312], Loss: 0.0433\n",
            "Epoch [104/200], Step [240/312], Loss: 0.0405\n",
            "Epoch [104/200], Step [260/312], Loss: 0.0271\n",
            "Epoch [104/200], Step [280/312], Loss: 0.0559\n",
            "Epoch [104/200], Step [300/312], Loss: 0.0746\n",
            "\n",
            "train-loss: 1.1751, train-acc: 99.8797\n",
            "validation loss: 3.0545, validation acc: 39.9118\n",
            "\n",
            "Epoch 105\n",
            "\n",
            "Epoch [105/200], Step [0/312], Loss: 0.0284\n",
            "Epoch [105/200], Step [20/312], Loss: 0.0278\n",
            "Epoch [105/200], Step [40/312], Loss: 0.1020\n",
            "Epoch [105/200], Step [60/312], Loss: 0.0382\n",
            "Epoch [105/200], Step [80/312], Loss: 0.0484\n",
            "Epoch [105/200], Step [100/312], Loss: 0.0362\n",
            "Epoch [105/200], Step [120/312], Loss: 0.1018\n",
            "Epoch [105/200], Step [140/312], Loss: 0.0401\n",
            "Epoch [105/200], Step [160/312], Loss: 0.0450\n",
            "Epoch [105/200], Step [180/312], Loss: 0.0337\n",
            "Epoch [105/200], Step [200/312], Loss: 0.0886\n",
            "Epoch [105/200], Step [220/312], Loss: 0.0318\n",
            "Epoch [105/200], Step [240/312], Loss: 0.0676\n",
            "Epoch [105/200], Step [260/312], Loss: 0.0493\n",
            "Epoch [105/200], Step [280/312], Loss: 0.0224\n",
            "Epoch [105/200], Step [300/312], Loss: 0.0344\n",
            "\n",
            "train-loss: 1.1644, train-acc: 99.8696\n",
            "validation loss: 3.0549, validation acc: 39.7513\n",
            "\n",
            "Epoch 106\n",
            "\n",
            "Epoch [106/200], Step [0/312], Loss: 0.0341\n",
            "Epoch [106/200], Step [20/312], Loss: 0.0572\n",
            "Epoch [106/200], Step [40/312], Loss: 0.0702\n",
            "Epoch [106/200], Step [60/312], Loss: 0.0513\n",
            "Epoch [106/200], Step [80/312], Loss: 0.0856\n",
            "Epoch [106/200], Step [100/312], Loss: 0.0756\n",
            "Epoch [106/200], Step [120/312], Loss: 0.0335\n",
            "Epoch [106/200], Step [140/312], Loss: 0.0498\n",
            "Epoch [106/200], Step [160/312], Loss: 0.1668\n",
            "Epoch [106/200], Step [180/312], Loss: 0.0829\n",
            "Epoch [106/200], Step [200/312], Loss: 0.0257\n",
            "Epoch [106/200], Step [220/312], Loss: 0.0359\n",
            "Epoch [106/200], Step [240/312], Loss: 0.1062\n",
            "Epoch [106/200], Step [260/312], Loss: 0.0384\n",
            "Epoch [106/200], Step [280/312], Loss: 0.0408\n",
            "Epoch [106/200], Step [300/312], Loss: 0.0476\n",
            "\n",
            "train-loss: 1.1540, train-acc: 99.8496\n",
            "validation loss: 3.0556, validation acc: 39.5106\n",
            "\n",
            "Epoch 107\n",
            "\n",
            "Epoch [107/200], Step [0/312], Loss: 0.0660\n",
            "Epoch [107/200], Step [20/312], Loss: 0.0354\n",
            "Epoch [107/200], Step [40/312], Loss: 0.0426\n",
            "Epoch [107/200], Step [60/312], Loss: 0.0421\n",
            "Epoch [107/200], Step [80/312], Loss: 0.0642\n",
            "Epoch [107/200], Step [100/312], Loss: 0.0594\n",
            "Epoch [107/200], Step [120/312], Loss: 0.1185\n",
            "Epoch [107/200], Step [140/312], Loss: 0.0280\n",
            "Epoch [107/200], Step [160/312], Loss: 0.0396\n",
            "Epoch [107/200], Step [180/312], Loss: 0.0512\n",
            "Epoch [107/200], Step [200/312], Loss: 0.0844\n",
            "Epoch [107/200], Step [220/312], Loss: 0.0444\n",
            "Epoch [107/200], Step [240/312], Loss: 0.0235\n",
            "Epoch [107/200], Step [260/312], Loss: 0.0429\n",
            "Epoch [107/200], Step [280/312], Loss: 0.1119\n",
            "Epoch [107/200], Step [300/312], Loss: 0.0304\n",
            "\n",
            "train-loss: 1.1438, train-acc: 99.8496\n",
            "validation loss: 3.0560, validation acc: 38.8688\n",
            "\n",
            "Epoch 108\n",
            "\n",
            "Epoch [108/200], Step [0/312], Loss: 0.0297\n",
            "Epoch [108/200], Step [20/312], Loss: 0.0734\n",
            "Epoch [108/200], Step [40/312], Loss: 0.0250\n",
            "Epoch [108/200], Step [60/312], Loss: 0.0677\n",
            "Epoch [108/200], Step [80/312], Loss: 0.0211\n",
            "Epoch [108/200], Step [100/312], Loss: 0.0325\n",
            "Epoch [108/200], Step [120/312], Loss: 0.0443\n",
            "Epoch [108/200], Step [140/312], Loss: 0.1327\n",
            "Epoch [108/200], Step [160/312], Loss: 0.0721\n",
            "Epoch [108/200], Step [180/312], Loss: 0.1047\n",
            "Epoch [108/200], Step [200/312], Loss: 0.0275\n",
            "Epoch [108/200], Step [220/312], Loss: 0.0414\n",
            "Epoch [108/200], Step [240/312], Loss: 0.0426\n",
            "Epoch [108/200], Step [260/312], Loss: 0.1729\n",
            "Epoch [108/200], Step [280/312], Loss: 0.0378\n",
            "Epoch [108/200], Step [300/312], Loss: 0.3406\n",
            "\n",
            "train-loss: 1.1338, train-acc: 99.7693\n",
            "validation loss: 3.0567, validation acc: 39.5507\n",
            "\n",
            "Epoch 109\n",
            "\n",
            "Epoch [109/200], Step [0/312], Loss: 0.0454\n",
            "Epoch [109/200], Step [20/312], Loss: 0.0548\n",
            "Epoch [109/200], Step [40/312], Loss: 0.0299\n",
            "Epoch [109/200], Step [60/312], Loss: 0.0250\n",
            "Epoch [109/200], Step [80/312], Loss: 0.0401\n",
            "Epoch [109/200], Step [100/312], Loss: 0.0370\n",
            "Epoch [109/200], Step [120/312], Loss: 0.2480\n",
            "Epoch [109/200], Step [140/312], Loss: 0.0562\n",
            "Epoch [109/200], Step [160/312], Loss: 0.0331\n",
            "Epoch [109/200], Step [180/312], Loss: 0.1493\n",
            "Epoch [109/200], Step [200/312], Loss: 0.0866\n",
            "Epoch [109/200], Step [220/312], Loss: 0.0328\n",
            "Epoch [109/200], Step [240/312], Loss: 0.0451\n",
            "Epoch [109/200], Step [260/312], Loss: 0.0571\n",
            "Epoch [109/200], Step [280/312], Loss: 0.0267\n",
            "Epoch [109/200], Step [300/312], Loss: 0.0725\n",
            "\n",
            "train-loss: 1.1239, train-acc: 99.9198\n",
            "validation loss: 3.0571, validation acc: 39.7914\n",
            "\n",
            "Epoch 110\n",
            "\n",
            "Epoch [110/200], Step [0/312], Loss: 0.0450\n",
            "Epoch [110/200], Step [20/312], Loss: 0.0306\n",
            "Epoch [110/200], Step [40/312], Loss: 0.0537\n",
            "Epoch [110/200], Step [60/312], Loss: 0.0615\n",
            "Epoch [110/200], Step [80/312], Loss: 0.0262\n",
            "Epoch [110/200], Step [100/312], Loss: 0.0838\n",
            "Epoch [110/200], Step [120/312], Loss: 0.0215\n",
            "Epoch [110/200], Step [140/312], Loss: 0.0247\n",
            "Epoch [110/200], Step [160/312], Loss: 0.0260\n",
            "Epoch [110/200], Step [180/312], Loss: 0.0465\n",
            "Epoch [110/200], Step [200/312], Loss: 0.0538\n",
            "Epoch [110/200], Step [220/312], Loss: 0.0426\n",
            "Epoch [110/200], Step [240/312], Loss: 0.0794\n",
            "Epoch [110/200], Step [260/312], Loss: 0.1875\n",
            "Epoch [110/200], Step [280/312], Loss: 0.0458\n",
            "Epoch [110/200], Step [300/312], Loss: 0.0826\n",
            "\n",
            "train-loss: 1.1142, train-acc: 99.8997\n",
            "validation loss: 3.0573, validation acc: 39.9920\n",
            "\n",
            "Epoch 111\n",
            "\n",
            "Epoch [111/200], Step [0/312], Loss: 0.0688\n",
            "Epoch [111/200], Step [20/312], Loss: 0.1054\n",
            "Epoch [111/200], Step [40/312], Loss: 0.0692\n",
            "Epoch [111/200], Step [60/312], Loss: 0.0309\n",
            "Epoch [111/200], Step [80/312], Loss: 0.0819\n",
            "Epoch [111/200], Step [100/312], Loss: 0.0987\n",
            "Epoch [111/200], Step [120/312], Loss: 0.0435\n",
            "Epoch [111/200], Step [140/312], Loss: 0.0550\n",
            "Epoch [111/200], Step [160/312], Loss: 0.0208\n",
            "Epoch [111/200], Step [180/312], Loss: 0.0622\n",
            "Epoch [111/200], Step [200/312], Loss: 0.0469\n",
            "Epoch [111/200], Step [220/312], Loss: 0.1799\n",
            "Epoch [111/200], Step [240/312], Loss: 0.1213\n",
            "Epoch [111/200], Step [260/312], Loss: 0.0324\n",
            "Epoch [111/200], Step [280/312], Loss: 0.0624\n",
            "Epoch [111/200], Step [300/312], Loss: 0.0397\n",
            "\n",
            "train-loss: 1.1047, train-acc: 99.8596\n",
            "validation loss: 3.0579, validation acc: 39.5909\n",
            "\n",
            "Epoch 112\n",
            "\n",
            "Epoch [112/200], Step [0/312], Loss: 0.0388\n",
            "Epoch [112/200], Step [20/312], Loss: 0.0314\n",
            "Epoch [112/200], Step [40/312], Loss: 0.0595\n",
            "Epoch [112/200], Step [60/312], Loss: 0.0318\n",
            "Epoch [112/200], Step [80/312], Loss: 0.0468\n",
            "Epoch [112/200], Step [100/312], Loss: 0.0219\n",
            "Epoch [112/200], Step [120/312], Loss: 0.0230\n",
            "Epoch [112/200], Step [140/312], Loss: 0.1482\n",
            "Epoch [112/200], Step [160/312], Loss: 0.0342\n",
            "Epoch [112/200], Step [180/312], Loss: 0.0345\n",
            "Epoch [112/200], Step [200/312], Loss: 0.0225\n",
            "Epoch [112/200], Step [220/312], Loss: 0.0489\n",
            "Epoch [112/200], Step [240/312], Loss: 0.0449\n",
            "Epoch [112/200], Step [260/312], Loss: 0.0544\n",
            "Epoch [112/200], Step [280/312], Loss: 0.0419\n",
            "Epoch [112/200], Step [300/312], Loss: 0.0691\n",
            "\n",
            "train-loss: 1.0953, train-acc: 99.8797\n",
            "validation loss: 3.0584, validation acc: 39.5507\n",
            "\n",
            "Epoch 113\n",
            "\n",
            "Epoch [113/200], Step [0/312], Loss: 0.0629\n",
            "Epoch [113/200], Step [20/312], Loss: 0.1306\n",
            "Epoch [113/200], Step [40/312], Loss: 0.0548\n",
            "Epoch [113/200], Step [60/312], Loss: 0.0222\n",
            "Epoch [113/200], Step [80/312], Loss: 0.0241\n",
            "Epoch [113/200], Step [100/312], Loss: 0.0363\n",
            "Epoch [113/200], Step [120/312], Loss: 0.0600\n",
            "Epoch [113/200], Step [140/312], Loss: 0.0475\n",
            "Epoch [113/200], Step [160/312], Loss: 0.0408\n",
            "Epoch [113/200], Step [180/312], Loss: 0.0586\n",
            "Epoch [113/200], Step [200/312], Loss: 0.2007\n",
            "Epoch [113/200], Step [220/312], Loss: 0.0389\n",
            "Epoch [113/200], Step [240/312], Loss: 0.0374\n",
            "Epoch [113/200], Step [260/312], Loss: 0.0666\n",
            "Epoch [113/200], Step [280/312], Loss: 0.0264\n",
            "Epoch [113/200], Step [300/312], Loss: 0.0528\n",
            "\n",
            "train-loss: 1.0861, train-acc: 99.8596\n",
            "validation loss: 3.0589, validation acc: 39.7112\n",
            "\n",
            "Epoch 114\n",
            "\n",
            "Epoch [114/200], Step [0/312], Loss: 0.0469\n",
            "Epoch [114/200], Step [20/312], Loss: 0.0867\n",
            "Epoch [114/200], Step [40/312], Loss: 0.0544\n",
            "Epoch [114/200], Step [60/312], Loss: 0.0369\n",
            "Epoch [114/200], Step [80/312], Loss: 0.0354\n",
            "Epoch [114/200], Step [100/312], Loss: 0.0794\n",
            "Epoch [114/200], Step [120/312], Loss: 0.0349\n",
            "Epoch [114/200], Step [140/312], Loss: 0.0248\n",
            "Epoch [114/200], Step [160/312], Loss: 0.0267\n",
            "Epoch [114/200], Step [180/312], Loss: 0.0223\n",
            "Epoch [114/200], Step [200/312], Loss: 0.0630\n",
            "Epoch [114/200], Step [220/312], Loss: 0.0500\n",
            "Epoch [114/200], Step [240/312], Loss: 0.0326\n",
            "Epoch [114/200], Step [260/312], Loss: 0.0373\n",
            "Epoch [114/200], Step [280/312], Loss: 0.0366\n",
            "Epoch [114/200], Step [300/312], Loss: 0.0290\n",
            "\n",
            "train-loss: 1.0770, train-acc: 99.8897\n",
            "validation loss: 3.0597, validation acc: 39.3101\n",
            "\n",
            "Epoch 115\n",
            "\n",
            "Epoch [115/200], Step [0/312], Loss: 0.0427\n",
            "Epoch [115/200], Step [20/312], Loss: 0.0885\n",
            "Epoch [115/200], Step [40/312], Loss: 0.1363\n",
            "Epoch [115/200], Step [60/312], Loss: 0.1018\n",
            "Epoch [115/200], Step [80/312], Loss: 0.0285\n",
            "Epoch [115/200], Step [100/312], Loss: 0.0226\n",
            "Epoch [115/200], Step [120/312], Loss: 0.0510\n",
            "Epoch [115/200], Step [140/312], Loss: 0.0847\n",
            "Epoch [115/200], Step [160/312], Loss: 0.0169\n",
            "Epoch [115/200], Step [180/312], Loss: 0.0349\n",
            "Epoch [115/200], Step [200/312], Loss: 0.0316\n",
            "Epoch [115/200], Step [220/312], Loss: 0.0434\n",
            "Epoch [115/200], Step [240/312], Loss: 0.1285\n",
            "Epoch [115/200], Step [260/312], Loss: 0.0474\n",
            "Epoch [115/200], Step [280/312], Loss: 0.0652\n",
            "Epoch [115/200], Step [300/312], Loss: 0.0789\n",
            "\n",
            "train-loss: 1.0681, train-acc: 99.8496\n",
            "validation loss: 3.0602, validation acc: 39.3502\n",
            "\n",
            "Epoch 116\n",
            "\n",
            "Epoch [116/200], Step [0/312], Loss: 0.0302\n",
            "Epoch [116/200], Step [20/312], Loss: 0.0369\n",
            "Epoch [116/200], Step [40/312], Loss: 0.0811\n",
            "Epoch [116/200], Step [60/312], Loss: 0.0746\n",
            "Epoch [116/200], Step [80/312], Loss: 0.1057\n",
            "Epoch [116/200], Step [100/312], Loss: 0.0187\n",
            "Epoch [116/200], Step [120/312], Loss: 0.0255\n",
            "Epoch [116/200], Step [140/312], Loss: 0.0400\n",
            "Epoch [116/200], Step [160/312], Loss: 0.0542\n",
            "Epoch [116/200], Step [180/312], Loss: 0.0558\n",
            "Epoch [116/200], Step [200/312], Loss: 0.0331\n",
            "Epoch [116/200], Step [220/312], Loss: 0.0411\n",
            "Epoch [116/200], Step [240/312], Loss: 0.0772\n",
            "Epoch [116/200], Step [260/312], Loss: 0.0372\n",
            "Epoch [116/200], Step [280/312], Loss: 0.0549\n",
            "Epoch [116/200], Step [300/312], Loss: 0.0632\n",
            "\n",
            "train-loss: 1.0594, train-acc: 99.8596\n",
            "validation loss: 3.0608, validation acc: 39.4304\n",
            "\n",
            "Epoch 117\n",
            "\n",
            "Epoch [117/200], Step [0/312], Loss: 0.0244\n",
            "Epoch [117/200], Step [20/312], Loss: 0.0469\n",
            "Epoch [117/200], Step [40/312], Loss: 0.0567\n",
            "Epoch [117/200], Step [60/312], Loss: 0.0561\n",
            "Epoch [117/200], Step [80/312], Loss: 0.0213\n",
            "Epoch [117/200], Step [100/312], Loss: 0.0339\n",
            "Epoch [117/200], Step [120/312], Loss: 0.0441\n",
            "Epoch [117/200], Step [140/312], Loss: 0.0815\n",
            "Epoch [117/200], Step [160/312], Loss: 0.0274\n",
            "Epoch [117/200], Step [180/312], Loss: 0.0683\n",
            "Epoch [117/200], Step [200/312], Loss: 0.0273\n",
            "Epoch [117/200], Step [220/312], Loss: 0.0642\n",
            "Epoch [117/200], Step [240/312], Loss: 0.1370\n",
            "Epoch [117/200], Step [260/312], Loss: 0.0279\n",
            "Epoch [117/200], Step [280/312], Loss: 0.0277\n",
            "Epoch [117/200], Step [300/312], Loss: 0.0299\n",
            "\n",
            "train-loss: 1.0507, train-acc: 99.8496\n",
            "validation loss: 3.0614, validation acc: 39.1095\n",
            "\n",
            "Epoch 118\n",
            "\n",
            "Epoch [118/200], Step [0/312], Loss: 0.0385\n",
            "Epoch [118/200], Step [20/312], Loss: 0.0341\n",
            "Epoch [118/200], Step [40/312], Loss: 0.0296\n",
            "Epoch [118/200], Step [60/312], Loss: 0.0219\n",
            "Epoch [118/200], Step [80/312], Loss: 0.0793\n",
            "Epoch [118/200], Step [100/312], Loss: 0.0870\n",
            "Epoch [118/200], Step [120/312], Loss: 0.0405\n",
            "Epoch [118/200], Step [140/312], Loss: 0.0720\n",
            "Epoch [118/200], Step [160/312], Loss: 0.0191\n",
            "Epoch [118/200], Step [180/312], Loss: 0.0278\n",
            "Epoch [118/200], Step [200/312], Loss: 0.0239\n",
            "Epoch [118/200], Step [220/312], Loss: 0.0699\n",
            "Epoch [118/200], Step [240/312], Loss: 0.0445\n",
            "Epoch [118/200], Step [260/312], Loss: 0.0954\n",
            "Epoch [118/200], Step [280/312], Loss: 0.0548\n",
            "Epoch [118/200], Step [300/312], Loss: 0.0379\n",
            "\n",
            "train-loss: 1.0423, train-acc: 99.8395\n",
            "validation loss: 3.0619, validation acc: 39.7513\n",
            "\n",
            "Epoch 119\n",
            "\n",
            "Epoch [119/200], Step [0/312], Loss: 0.0590\n",
            "Epoch [119/200], Step [20/312], Loss: 0.0529\n",
            "Epoch [119/200], Step [40/312], Loss: 0.0452\n",
            "Epoch [119/200], Step [60/312], Loss: 0.0207\n",
            "Epoch [119/200], Step [80/312], Loss: 0.1102\n",
            "Epoch [119/200], Step [100/312], Loss: 0.0339\n",
            "Epoch [119/200], Step [120/312], Loss: 0.0379\n",
            "Epoch [119/200], Step [140/312], Loss: 0.0285\n",
            "Epoch [119/200], Step [160/312], Loss: 0.1262\n",
            "Epoch [119/200], Step [180/312], Loss: 0.0420\n",
            "Epoch [119/200], Step [200/312], Loss: 0.0243\n",
            "Epoch [119/200], Step [220/312], Loss: 0.0490\n",
            "Epoch [119/200], Step [240/312], Loss: 0.0221\n",
            "Epoch [119/200], Step [260/312], Loss: 0.0564\n",
            "Epoch [119/200], Step [280/312], Loss: 0.3945\n",
            "Epoch [119/200], Step [300/312], Loss: 0.0350\n",
            "\n",
            "train-loss: 1.0340, train-acc: 99.8496\n",
            "validation loss: 3.0623, validation acc: 39.6310\n",
            "\n",
            "Epoch 120\n",
            "\n",
            "Epoch [120/200], Step [0/312], Loss: 0.0589\n",
            "Epoch [120/200], Step [20/312], Loss: 0.0209\n",
            "Epoch [120/200], Step [40/312], Loss: 0.0230\n",
            "Epoch [120/200], Step [60/312], Loss: 0.0509\n",
            "Epoch [120/200], Step [80/312], Loss: 0.0651\n",
            "Epoch [120/200], Step [100/312], Loss: 0.0517\n",
            "Epoch [120/200], Step [120/312], Loss: 0.0567\n",
            "Epoch [120/200], Step [140/312], Loss: 0.0229\n",
            "Epoch [120/200], Step [160/312], Loss: 0.0188\n",
            "Epoch [120/200], Step [180/312], Loss: 0.0628\n",
            "Epoch [120/200], Step [200/312], Loss: 0.0446\n",
            "Epoch [120/200], Step [220/312], Loss: 0.0868\n",
            "Epoch [120/200], Step [240/312], Loss: 0.0250\n",
            "Epoch [120/200], Step [260/312], Loss: 0.1143\n",
            "Epoch [120/200], Step [280/312], Loss: 0.0342\n",
            "Epoch [120/200], Step [300/312], Loss: 0.0223\n",
            "\n",
            "train-loss: 1.0258, train-acc: 99.8395\n",
            "validation loss: 3.0627, validation acc: 39.4304\n",
            "\n",
            "Epoch 121\n",
            "\n",
            "Epoch [121/200], Step [0/312], Loss: 0.0240\n",
            "Epoch [121/200], Step [20/312], Loss: 0.0546\n",
            "Epoch [121/200], Step [40/312], Loss: 0.0444\n",
            "Epoch [121/200], Step [60/312], Loss: 0.0284\n",
            "Epoch [121/200], Step [80/312], Loss: 0.0209\n",
            "Epoch [121/200], Step [100/312], Loss: 0.0922\n",
            "Epoch [121/200], Step [120/312], Loss: 0.0237\n",
            "Epoch [121/200], Step [140/312], Loss: 0.0308\n",
            "Epoch [121/200], Step [160/312], Loss: 0.0267\n",
            "Epoch [121/200], Step [180/312], Loss: 0.0307\n",
            "Epoch [121/200], Step [200/312], Loss: 0.0446\n",
            "Epoch [121/200], Step [220/312], Loss: 0.0207\n",
            "Epoch [121/200], Step [240/312], Loss: 0.0355\n",
            "Epoch [121/200], Step [260/312], Loss: 0.0834\n",
            "Epoch [121/200], Step [280/312], Loss: 0.0337\n",
            "Epoch [121/200], Step [300/312], Loss: 0.0262\n",
            "\n",
            "train-loss: 1.0177, train-acc: 99.9398\n",
            "validation loss: 3.0631, validation acc: 39.5909\n",
            "\n",
            "Epoch 122\n",
            "\n",
            "Epoch [122/200], Step [0/312], Loss: 0.0435\n",
            "Epoch [122/200], Step [20/312], Loss: 0.0433\n",
            "Epoch [122/200], Step [40/312], Loss: 0.0258\n",
            "Epoch [122/200], Step [60/312], Loss: 0.0333\n",
            "Epoch [122/200], Step [80/312], Loss: 0.0192\n",
            "Epoch [122/200], Step [100/312], Loss: 0.0224\n",
            "Epoch [122/200], Step [120/312], Loss: 0.0262\n",
            "Epoch [122/200], Step [140/312], Loss: 0.0548\n",
            "Epoch [122/200], Step [160/312], Loss: 0.0755\n",
            "Epoch [122/200], Step [180/312], Loss: 0.0403\n",
            "Epoch [122/200], Step [200/312], Loss: 0.0373\n",
            "Epoch [122/200], Step [220/312], Loss: 0.0411\n",
            "Epoch [122/200], Step [240/312], Loss: 0.0655\n",
            "Epoch [122/200], Step [260/312], Loss: 0.0289\n",
            "Epoch [122/200], Step [280/312], Loss: 0.0391\n",
            "Epoch [122/200], Step [300/312], Loss: 0.0211\n",
            "\n",
            "train-loss: 1.0097, train-acc: 99.8897\n",
            "validation loss: 3.0635, validation acc: 39.7112\n",
            "\n",
            "Epoch 123\n",
            "\n",
            "Epoch [123/200], Step [0/312], Loss: 0.0344\n",
            "Epoch [123/200], Step [20/312], Loss: 0.0324\n",
            "Epoch [123/200], Step [40/312], Loss: 0.0249\n",
            "Epoch [123/200], Step [60/312], Loss: 0.0691\n",
            "Epoch [123/200], Step [80/312], Loss: 0.1305\n",
            "Epoch [123/200], Step [100/312], Loss: 0.0571\n",
            "Epoch [123/200], Step [120/312], Loss: 0.0466\n",
            "Epoch [123/200], Step [140/312], Loss: 0.0224\n",
            "Epoch [123/200], Step [160/312], Loss: 0.0198\n",
            "Epoch [123/200], Step [180/312], Loss: 0.0226\n",
            "Epoch [123/200], Step [200/312], Loss: 0.0249\n",
            "Epoch [123/200], Step [220/312], Loss: 0.0321\n",
            "Epoch [123/200], Step [240/312], Loss: 0.0695\n",
            "Epoch [123/200], Step [260/312], Loss: 0.1141\n",
            "Epoch [123/200], Step [280/312], Loss: 0.0428\n",
            "Epoch [123/200], Step [300/312], Loss: 0.1217\n",
            "\n",
            "train-loss: 1.0019, train-acc: 99.8997\n",
            "validation loss: 3.0641, validation acc: 38.7886\n",
            "\n",
            "Epoch 124\n",
            "\n",
            "Epoch [124/200], Step [0/312], Loss: 0.0243\n",
            "Epoch [124/200], Step [20/312], Loss: 0.0231\n",
            "Epoch [124/200], Step [40/312], Loss: 0.0412\n",
            "Epoch [124/200], Step [60/312], Loss: 0.0261\n",
            "Epoch [124/200], Step [80/312], Loss: 0.0523\n",
            "Epoch [124/200], Step [100/312], Loss: 0.0228\n",
            "Epoch [124/200], Step [120/312], Loss: 0.0213\n",
            "Epoch [124/200], Step [140/312], Loss: 0.0283\n",
            "Epoch [124/200], Step [160/312], Loss: 0.0622\n",
            "Epoch [124/200], Step [180/312], Loss: 0.0332\n",
            "Epoch [124/200], Step [200/312], Loss: 0.0407\n",
            "Epoch [124/200], Step [220/312], Loss: 0.0779\n",
            "Epoch [124/200], Step [240/312], Loss: 0.0290\n",
            "Epoch [124/200], Step [260/312], Loss: 0.1378\n",
            "Epoch [124/200], Step [280/312], Loss: 0.0377\n",
            "Epoch [124/200], Step [300/312], Loss: 0.0791\n",
            "\n",
            "train-loss: 0.9942, train-acc: 99.8596\n",
            "validation loss: 3.0646, validation acc: 39.1897\n",
            "\n",
            "Epoch 125\n",
            "\n",
            "Epoch [125/200], Step [0/312], Loss: 0.0804\n",
            "Epoch [125/200], Step [20/312], Loss: 0.0387\n",
            "Epoch [125/200], Step [40/312], Loss: 0.0210\n",
            "Epoch [125/200], Step [60/312], Loss: 0.0186\n",
            "Epoch [125/200], Step [80/312], Loss: 0.0365\n",
            "Epoch [125/200], Step [100/312], Loss: 0.0463\n",
            "Epoch [125/200], Step [120/312], Loss: 0.0340\n",
            "Epoch [125/200], Step [140/312], Loss: 0.0432\n",
            "Epoch [125/200], Step [160/312], Loss: 0.0325\n",
            "Epoch [125/200], Step [180/312], Loss: 0.0212\n",
            "Epoch [125/200], Step [200/312], Loss: 0.0397\n",
            "Epoch [125/200], Step [220/312], Loss: 0.0409\n",
            "Epoch [125/200], Step [240/312], Loss: 0.0285\n",
            "Epoch [125/200], Step [260/312], Loss: 0.1934\n",
            "Epoch [125/200], Step [280/312], Loss: 0.0179\n",
            "Epoch [125/200], Step [300/312], Loss: 0.0171\n",
            "\n",
            "train-loss: 0.9866, train-acc: 99.8596\n",
            "validation loss: 3.0652, validation acc: 39.5507\n",
            "\n",
            "Epoch 126\n",
            "\n",
            "Epoch [126/200], Step [0/312], Loss: 0.0510\n",
            "Epoch [126/200], Step [20/312], Loss: 0.0456\n",
            "Epoch [126/200], Step [40/312], Loss: 0.0296\n",
            "Epoch [126/200], Step [60/312], Loss: 0.0399\n",
            "Epoch [126/200], Step [80/312], Loss: 0.0207\n",
            "Epoch [126/200], Step [100/312], Loss: 0.0151\n",
            "Epoch [126/200], Step [120/312], Loss: 0.0329\n",
            "Epoch [126/200], Step [140/312], Loss: 0.0435\n",
            "Epoch [126/200], Step [160/312], Loss: 0.0692\n",
            "Epoch [126/200], Step [180/312], Loss: 0.0436\n",
            "Epoch [126/200], Step [200/312], Loss: 0.0531\n",
            "Epoch [126/200], Step [220/312], Loss: 0.0598\n",
            "Epoch [126/200], Step [240/312], Loss: 0.0493\n",
            "Epoch [126/200], Step [260/312], Loss: 0.1064\n",
            "Epoch [126/200], Step [280/312], Loss: 0.0191\n",
            "Epoch [126/200], Step [300/312], Loss: 0.0217\n",
            "\n",
            "train-loss: 0.9792, train-acc: 99.8997\n",
            "validation loss: 3.0658, validation acc: 38.8688\n",
            "\n",
            "Epoch 127\n",
            "\n",
            "Epoch [127/200], Step [0/312], Loss: 0.0553\n",
            "Epoch [127/200], Step [20/312], Loss: 0.0295\n",
            "Epoch [127/200], Step [40/312], Loss: 0.0283\n",
            "Epoch [127/200], Step [60/312], Loss: 0.0310\n",
            "Epoch [127/200], Step [80/312], Loss: 0.0692\n",
            "Epoch [127/200], Step [100/312], Loss: 0.0213\n",
            "Epoch [127/200], Step [120/312], Loss: 0.0374\n",
            "Epoch [127/200], Step [140/312], Loss: 0.0261\n",
            "Epoch [127/200], Step [160/312], Loss: 0.0267\n",
            "Epoch [127/200], Step [180/312], Loss: 0.0370\n",
            "Epoch [127/200], Step [200/312], Loss: 0.0522\n",
            "Epoch [127/200], Step [220/312], Loss: 0.0203\n",
            "Epoch [127/200], Step [240/312], Loss: 0.0357\n",
            "Epoch [127/200], Step [260/312], Loss: 0.0864\n",
            "Epoch [127/200], Step [280/312], Loss: 0.0295\n",
            "Epoch [127/200], Step [300/312], Loss: 0.0259\n",
            "\n",
            "train-loss: 0.9718, train-acc: 99.9198\n",
            "validation loss: 3.0663, validation acc: 39.7914\n",
            "\n",
            "Epoch 128\n",
            "\n",
            "Epoch [128/200], Step [0/312], Loss: 0.0408\n",
            "Epoch [128/200], Step [20/312], Loss: 0.0329\n",
            "Epoch [128/200], Step [40/312], Loss: 0.0562\n",
            "Epoch [128/200], Step [60/312], Loss: 0.0382\n",
            "Epoch [128/200], Step [80/312], Loss: 0.0420\n",
            "Epoch [128/200], Step [100/312], Loss: 0.0185\n",
            "Epoch [128/200], Step [120/312], Loss: 0.0206\n",
            "Epoch [128/200], Step [140/312], Loss: 0.0828\n",
            "Epoch [128/200], Step [160/312], Loss: 0.0313\n",
            "Epoch [128/200], Step [180/312], Loss: 0.1076\n",
            "Epoch [128/200], Step [200/312], Loss: 0.0349\n",
            "Epoch [128/200], Step [220/312], Loss: 0.0442\n",
            "Epoch [128/200], Step [240/312], Loss: 0.0411\n",
            "Epoch [128/200], Step [260/312], Loss: 0.0708\n",
            "Epoch [128/200], Step [280/312], Loss: 0.0199\n",
            "Epoch [128/200], Step [300/312], Loss: 0.0625\n",
            "\n",
            "train-loss: 0.9645, train-acc: 99.9097\n",
            "validation loss: 3.0667, validation acc: 39.1496\n",
            "\n",
            "Epoch 129\n",
            "\n",
            "Epoch [129/200], Step [0/312], Loss: 0.0644\n",
            "Epoch [129/200], Step [20/312], Loss: 0.0519\n",
            "Epoch [129/200], Step [40/312], Loss: 0.0559\n",
            "Epoch [129/200], Step [60/312], Loss: 0.0403\n",
            "Epoch [129/200], Step [80/312], Loss: 0.1118\n",
            "Epoch [129/200], Step [100/312], Loss: 0.1324\n",
            "Epoch [129/200], Step [120/312], Loss: 0.1142\n",
            "Epoch [129/200], Step [140/312], Loss: 0.0284\n",
            "Epoch [129/200], Step [160/312], Loss: 0.0323\n",
            "Epoch [129/200], Step [180/312], Loss: 0.0358\n",
            "Epoch [129/200], Step [200/312], Loss: 0.0382\n",
            "Epoch [129/200], Step [220/312], Loss: 0.0680\n",
            "Epoch [129/200], Step [240/312], Loss: 0.1061\n",
            "Epoch [129/200], Step [260/312], Loss: 0.0421\n",
            "Epoch [129/200], Step [280/312], Loss: 0.0266\n",
            "Epoch [129/200], Step [300/312], Loss: 0.0515\n",
            "\n",
            "train-loss: 0.9574, train-acc: 99.8997\n",
            "validation loss: 3.0673, validation acc: 38.6683\n",
            "\n",
            "Epoch 130\n",
            "\n",
            "Epoch [130/200], Step [0/312], Loss: 0.0327\n",
            "Epoch [130/200], Step [20/312], Loss: 0.0204\n",
            "Epoch [130/200], Step [40/312], Loss: 0.0362\n",
            "Epoch [130/200], Step [60/312], Loss: 0.0258\n",
            "Epoch [130/200], Step [80/312], Loss: 0.0279\n",
            "Epoch [130/200], Step [100/312], Loss: 0.0224\n",
            "Epoch [130/200], Step [120/312], Loss: 0.0229\n",
            "Epoch [130/200], Step [140/312], Loss: 0.0241\n",
            "Epoch [130/200], Step [160/312], Loss: 0.0453\n",
            "Epoch [130/200], Step [180/312], Loss: 0.0137\n",
            "Epoch [130/200], Step [200/312], Loss: 0.0270\n",
            "Epoch [130/200], Step [220/312], Loss: 0.0346\n",
            "Epoch [130/200], Step [240/312], Loss: 0.0390\n",
            "Epoch [130/200], Step [260/312], Loss: 0.0184\n",
            "Epoch [130/200], Step [280/312], Loss: 0.0408\n",
            "Epoch [130/200], Step [300/312], Loss: 0.1077\n",
            "\n",
            "train-loss: 0.9504, train-acc: 99.8997\n",
            "validation loss: 3.0678, validation acc: 39.1897\n",
            "\n",
            "Epoch 131\n",
            "\n",
            "Epoch [131/200], Step [0/312], Loss: 0.0340\n",
            "Epoch [131/200], Step [20/312], Loss: 0.0344\n",
            "Epoch [131/200], Step [40/312], Loss: 0.0228\n",
            "Epoch [131/200], Step [60/312], Loss: 0.0745\n",
            "Epoch [131/200], Step [80/312], Loss: 0.0223\n",
            "Epoch [131/200], Step [100/312], Loss: 0.0348\n",
            "Epoch [131/200], Step [120/312], Loss: 0.0462\n",
            "Epoch [131/200], Step [140/312], Loss: 0.0233\n",
            "Epoch [131/200], Step [160/312], Loss: 0.0230\n",
            "Epoch [131/200], Step [180/312], Loss: 0.1415\n",
            "Epoch [131/200], Step [200/312], Loss: 0.0329\n",
            "Epoch [131/200], Step [220/312], Loss: 0.0394\n",
            "Epoch [131/200], Step [240/312], Loss: 0.0215\n",
            "Epoch [131/200], Step [260/312], Loss: 0.0385\n",
            "Epoch [131/200], Step [280/312], Loss: 0.0722\n",
            "Epoch [131/200], Step [300/312], Loss: 0.0220\n",
            "\n",
            "train-loss: 0.9435, train-acc: 99.9097\n",
            "validation loss: 3.0684, validation acc: 40.3129\n",
            "\n",
            "Epoch 132\n",
            "\n",
            "Epoch [132/200], Step [0/312], Loss: 0.0490\n",
            "Epoch [132/200], Step [20/312], Loss: 0.0207\n",
            "Epoch [132/200], Step [40/312], Loss: 0.0235\n",
            "Epoch [132/200], Step [60/312], Loss: 0.0137\n",
            "Epoch [132/200], Step [80/312], Loss: 0.0342\n",
            "Epoch [132/200], Step [100/312], Loss: 0.0577\n",
            "Epoch [132/200], Step [120/312], Loss: 0.0271\n",
            "Epoch [132/200], Step [140/312], Loss: 0.0410\n",
            "Epoch [132/200], Step [160/312], Loss: 0.0262\n",
            "Epoch [132/200], Step [180/312], Loss: 0.0178\n",
            "Epoch [132/200], Step [200/312], Loss: 0.0250\n",
            "Epoch [132/200], Step [220/312], Loss: 0.0225\n",
            "Epoch [132/200], Step [240/312], Loss: 0.0308\n",
            "Epoch [132/200], Step [260/312], Loss: 0.0500\n",
            "Epoch [132/200], Step [280/312], Loss: 0.0356\n",
            "Epoch [132/200], Step [300/312], Loss: 0.0191\n",
            "\n",
            "train-loss: 0.9366, train-acc: 99.8797\n",
            "validation loss: 3.0689, validation acc: 39.3502\n",
            "\n",
            "Epoch 133\n",
            "\n",
            "Epoch [133/200], Step [0/312], Loss: 0.0121\n",
            "Epoch [133/200], Step [20/312], Loss: 0.0443\n",
            "Epoch [133/200], Step [40/312], Loss: 0.0361\n",
            "Epoch [133/200], Step [60/312], Loss: 0.0240\n",
            "Epoch [133/200], Step [80/312], Loss: 0.0374\n",
            "Epoch [133/200], Step [100/312], Loss: 0.0782\n",
            "Epoch [133/200], Step [120/312], Loss: 0.0548\n",
            "Epoch [133/200], Step [140/312], Loss: 0.0331\n",
            "Epoch [133/200], Step [160/312], Loss: 0.0300\n",
            "Epoch [133/200], Step [180/312], Loss: 0.0663\n",
            "Epoch [133/200], Step [200/312], Loss: 0.0360\n",
            "Epoch [133/200], Step [220/312], Loss: 0.0175\n",
            "Epoch [133/200], Step [240/312], Loss: 0.0585\n",
            "Epoch [133/200], Step [260/312], Loss: 0.0927\n",
            "Epoch [133/200], Step [280/312], Loss: 0.0635\n",
            "Epoch [133/200], Step [300/312], Loss: 0.0917\n",
            "\n",
            "train-loss: 0.9299, train-acc: 99.8496\n",
            "validation loss: 3.0694, validation acc: 39.3903\n",
            "\n",
            "Epoch 134\n",
            "\n",
            "Epoch [134/200], Step [0/312], Loss: 0.0523\n",
            "Epoch [134/200], Step [20/312], Loss: 0.0456\n",
            "Epoch [134/200], Step [40/312], Loss: 0.0490\n",
            "Epoch [134/200], Step [60/312], Loss: 0.0300\n",
            "Epoch [134/200], Step [80/312], Loss: 0.1209\n",
            "Epoch [134/200], Step [100/312], Loss: 0.0441\n",
            "Epoch [134/200], Step [120/312], Loss: 0.0228\n",
            "Epoch [134/200], Step [140/312], Loss: 0.0751\n",
            "Epoch [134/200], Step [160/312], Loss: 0.0394\n",
            "Epoch [134/200], Step [180/312], Loss: 0.0291\n",
            "Epoch [134/200], Step [200/312], Loss: 0.0192\n",
            "Epoch [134/200], Step [220/312], Loss: 0.0299\n",
            "Epoch [134/200], Step [240/312], Loss: 0.0291\n",
            "Epoch [134/200], Step [260/312], Loss: 0.0477\n",
            "Epoch [134/200], Step [280/312], Loss: 0.0453\n",
            "Epoch [134/200], Step [300/312], Loss: 0.0492\n",
            "\n",
            "train-loss: 0.9233, train-acc: 99.9398\n",
            "validation loss: 3.0698, validation acc: 40.0321\n",
            "\n",
            "Epoch 135\n",
            "\n",
            "Epoch [135/200], Step [0/312], Loss: 0.0225\n",
            "Epoch [135/200], Step [20/312], Loss: 0.0171\n",
            "Epoch [135/200], Step [40/312], Loss: 0.0325\n",
            "Epoch [135/200], Step [60/312], Loss: 0.0208\n",
            "Epoch [135/200], Step [80/312], Loss: 0.0468\n",
            "Epoch [135/200], Step [100/312], Loss: 0.0151\n",
            "Epoch [135/200], Step [120/312], Loss: 0.0476\n",
            "Epoch [135/200], Step [140/312], Loss: 0.0293\n",
            "Epoch [135/200], Step [160/312], Loss: 0.0686\n",
            "Epoch [135/200], Step [180/312], Loss: 0.0607\n",
            "Epoch [135/200], Step [200/312], Loss: 0.0220\n",
            "Epoch [135/200], Step [220/312], Loss: 0.0975\n",
            "Epoch [135/200], Step [240/312], Loss: 0.0237\n",
            "Epoch [135/200], Step [260/312], Loss: 0.0575\n",
            "Epoch [135/200], Step [280/312], Loss: 0.0254\n",
            "Epoch [135/200], Step [300/312], Loss: 0.0197\n",
            "\n",
            "train-loss: 0.9167, train-acc: 99.9097\n",
            "validation loss: 3.0701, validation acc: 40.1524\n",
            "\n",
            "Epoch 136\n",
            "\n",
            "Epoch [136/200], Step [0/312], Loss: 0.0254\n",
            "Epoch [136/200], Step [20/312], Loss: 0.0131\n",
            "Epoch [136/200], Step [40/312], Loss: 0.0278\n",
            "Epoch [136/200], Step [60/312], Loss: 0.0523\n",
            "Epoch [136/200], Step [80/312], Loss: 0.0328\n",
            "Epoch [136/200], Step [100/312], Loss: 0.0291\n",
            "Epoch [136/200], Step [120/312], Loss: 0.0264\n",
            "Epoch [136/200], Step [140/312], Loss: 0.0505\n",
            "Epoch [136/200], Step [160/312], Loss: 0.0317\n",
            "Epoch [136/200], Step [180/312], Loss: 0.0248\n",
            "Epoch [136/200], Step [200/312], Loss: 0.0276\n",
            "Epoch [136/200], Step [220/312], Loss: 0.0172\n",
            "Epoch [136/200], Step [240/312], Loss: 0.0280\n",
            "Epoch [136/200], Step [260/312], Loss: 0.0201\n",
            "Epoch [136/200], Step [280/312], Loss: 0.0519\n",
            "Epoch [136/200], Step [300/312], Loss: 0.0526\n",
            "\n",
            "train-loss: 0.9103, train-acc: 99.9298\n",
            "validation loss: 3.0703, validation acc: 39.7914\n",
            "\n",
            "Epoch 137\n",
            "\n",
            "Epoch [137/200], Step [0/312], Loss: 0.0649\n",
            "Epoch [137/200], Step [20/312], Loss: 0.0639\n",
            "Epoch [137/200], Step [40/312], Loss: 0.0499\n",
            "Epoch [137/200], Step [60/312], Loss: 0.0164\n",
            "Epoch [137/200], Step [80/312], Loss: 0.0187\n",
            "Epoch [137/200], Step [100/312], Loss: 0.1152\n",
            "Epoch [137/200], Step [120/312], Loss: 0.0234\n",
            "Epoch [137/200], Step [140/312], Loss: 0.0461\n",
            "Epoch [137/200], Step [160/312], Loss: 0.0510\n",
            "Epoch [137/200], Step [180/312], Loss: 0.0298\n",
            "Epoch [137/200], Step [200/312], Loss: 0.0342\n",
            "Epoch [137/200], Step [220/312], Loss: 0.0748\n",
            "Epoch [137/200], Step [240/312], Loss: 0.0352\n",
            "Epoch [137/200], Step [260/312], Loss: 0.0275\n",
            "Epoch [137/200], Step [280/312], Loss: 0.0480\n",
            "Epoch [137/200], Step [300/312], Loss: 0.0236\n",
            "\n",
            "train-loss: 0.9040, train-acc: 99.8696\n",
            "validation loss: 3.0707, validation acc: 39.7914\n",
            "\n",
            "Epoch 138\n",
            "\n",
            "Epoch [138/200], Step [0/312], Loss: 0.0248\n",
            "Epoch [138/200], Step [20/312], Loss: 0.0446\n",
            "Epoch [138/200], Step [40/312], Loss: 0.0318\n",
            "Epoch [138/200], Step [60/312], Loss: 0.0200\n",
            "Epoch [138/200], Step [80/312], Loss: 0.0272\n",
            "Epoch [138/200], Step [100/312], Loss: 0.0137\n",
            "Epoch [138/200], Step [120/312], Loss: 0.0333\n",
            "Epoch [138/200], Step [140/312], Loss: 0.0213\n",
            "Epoch [138/200], Step [160/312], Loss: 0.0665\n",
            "Epoch [138/200], Step [180/312], Loss: 0.0251\n",
            "Epoch [138/200], Step [200/312], Loss: 0.0266\n",
            "Epoch [138/200], Step [220/312], Loss: 0.0204\n",
            "Epoch [138/200], Step [240/312], Loss: 0.0228\n",
            "Epoch [138/200], Step [260/312], Loss: 0.0180\n",
            "Epoch [138/200], Step [280/312], Loss: 0.0334\n",
            "Epoch [138/200], Step [300/312], Loss: 0.0301\n",
            "\n",
            "train-loss: 0.8977, train-acc: 99.9398\n",
            "validation loss: 3.0709, validation acc: 39.2298\n",
            "\n",
            "Epoch 139\n",
            "\n",
            "Epoch [139/200], Step [0/312], Loss: 0.0443\n",
            "Epoch [139/200], Step [20/312], Loss: 0.1108\n",
            "Epoch [139/200], Step [40/312], Loss: 0.0157\n",
            "Epoch [139/200], Step [60/312], Loss: 0.0863\n",
            "Epoch [139/200], Step [80/312], Loss: 0.0358\n",
            "Epoch [139/200], Step [100/312], Loss: 0.0419\n",
            "Epoch [139/200], Step [120/312], Loss: 0.0250\n",
            "Epoch [139/200], Step [140/312], Loss: 0.0211\n",
            "Epoch [139/200], Step [160/312], Loss: 0.0311\n",
            "Epoch [139/200], Step [180/312], Loss: 0.0773\n",
            "Epoch [139/200], Step [200/312], Loss: 0.0751\n",
            "Epoch [139/200], Step [220/312], Loss: 0.0625\n",
            "Epoch [139/200], Step [240/312], Loss: 0.1127\n",
            "Epoch [139/200], Step [260/312], Loss: 0.0667\n",
            "Epoch [139/200], Step [280/312], Loss: 0.0231\n",
            "Epoch [139/200], Step [300/312], Loss: 0.0228\n",
            "\n",
            "train-loss: 0.8915, train-acc: 99.8496\n",
            "validation loss: 3.0712, validation acc: 39.7513\n",
            "\n",
            "Epoch 140\n",
            "\n",
            "Epoch [140/200], Step [0/312], Loss: 0.0195\n",
            "Epoch [140/200], Step [20/312], Loss: 0.0169\n",
            "Epoch [140/200], Step [40/312], Loss: 0.0267\n",
            "Epoch [140/200], Step [60/312], Loss: 0.0803\n",
            "Epoch [140/200], Step [80/312], Loss: 0.0685\n",
            "Epoch [140/200], Step [100/312], Loss: 0.0248\n",
            "Epoch [140/200], Step [120/312], Loss: 0.0652\n",
            "Epoch [140/200], Step [140/312], Loss: 0.0279\n",
            "Epoch [140/200], Step [160/312], Loss: 0.0190\n",
            "Epoch [140/200], Step [180/312], Loss: 0.1839\n",
            "Epoch [140/200], Step [200/312], Loss: 0.0313\n",
            "Epoch [140/200], Step [220/312], Loss: 0.0505\n",
            "Epoch [140/200], Step [240/312], Loss: 0.0498\n",
            "Epoch [140/200], Step [260/312], Loss: 0.0241\n",
            "Epoch [140/200], Step [280/312], Loss: 0.0285\n",
            "Epoch [140/200], Step [300/312], Loss: 0.0194\n",
            "\n",
            "train-loss: 0.8855, train-acc: 99.8997\n",
            "validation loss: 3.0716, validation acc: 39.4705\n",
            "\n",
            "Epoch 141\n",
            "\n",
            "Epoch [141/200], Step [0/312], Loss: 0.0732\n",
            "Epoch [141/200], Step [20/312], Loss: 0.0503\n",
            "Epoch [141/200], Step [40/312], Loss: 0.0213\n",
            "Epoch [141/200], Step [60/312], Loss: 0.0279\n",
            "Epoch [141/200], Step [80/312], Loss: 0.0376\n",
            "Epoch [141/200], Step [100/312], Loss: 0.0316\n",
            "Epoch [141/200], Step [120/312], Loss: 0.1015\n",
            "Epoch [141/200], Step [140/312], Loss: 0.0314\n",
            "Epoch [141/200], Step [160/312], Loss: 0.0341\n",
            "Epoch [141/200], Step [180/312], Loss: 0.0121\n",
            "Epoch [141/200], Step [200/312], Loss: 0.0318\n",
            "Epoch [141/200], Step [220/312], Loss: 0.0391\n",
            "Epoch [141/200], Step [240/312], Loss: 0.0199\n",
            "Epoch [141/200], Step [260/312], Loss: 0.0491\n",
            "Epoch [141/200], Step [280/312], Loss: 0.0230\n",
            "Epoch [141/200], Step [300/312], Loss: 0.0138\n",
            "\n",
            "train-loss: 0.8794, train-acc: 99.8997\n",
            "validation loss: 3.0721, validation acc: 38.9491\n",
            "\n",
            "Epoch 142\n",
            "\n",
            "Epoch [142/200], Step [0/312], Loss: 0.0173\n",
            "Epoch [142/200], Step [20/312], Loss: 0.0759\n",
            "Epoch [142/200], Step [40/312], Loss: 0.0259\n",
            "Epoch [142/200], Step [60/312], Loss: 0.0432\n",
            "Epoch [142/200], Step [80/312], Loss: 0.0224\n",
            "Epoch [142/200], Step [100/312], Loss: 0.0332\n",
            "Epoch [142/200], Step [120/312], Loss: 0.0321\n",
            "Epoch [142/200], Step [140/312], Loss: 0.0112\n",
            "Epoch [142/200], Step [160/312], Loss: 0.0187\n",
            "Epoch [142/200], Step [180/312], Loss: 0.0478\n",
            "Epoch [142/200], Step [200/312], Loss: 0.0303\n",
            "Epoch [142/200], Step [220/312], Loss: 0.0216\n",
            "Epoch [142/200], Step [240/312], Loss: 0.0307\n",
            "Epoch [142/200], Step [260/312], Loss: 0.0217\n",
            "Epoch [142/200], Step [280/312], Loss: 0.0379\n",
            "Epoch [142/200], Step [300/312], Loss: 0.0364\n",
            "\n",
            "train-loss: 0.8735, train-acc: 99.9298\n",
            "validation loss: 3.0726, validation acc: 39.3502\n",
            "\n",
            "Epoch 143\n",
            "\n",
            "Epoch [143/200], Step [0/312], Loss: 0.0205\n",
            "Epoch [143/200], Step [20/312], Loss: 0.0371\n",
            "Epoch [143/200], Step [40/312], Loss: 0.0634\n",
            "Epoch [143/200], Step [60/312], Loss: 0.0252\n",
            "Epoch [143/200], Step [80/312], Loss: 0.1576\n",
            "Epoch [143/200], Step [100/312], Loss: 0.0439\n",
            "Epoch [143/200], Step [120/312], Loss: 0.0149\n",
            "Epoch [143/200], Step [140/312], Loss: 0.0647\n",
            "Epoch [143/200], Step [160/312], Loss: 0.0427\n",
            "Epoch [143/200], Step [180/312], Loss: 0.0538\n",
            "Epoch [143/200], Step [200/312], Loss: 0.0583\n",
            "Epoch [143/200], Step [220/312], Loss: 0.0132\n",
            "Epoch [143/200], Step [240/312], Loss: 0.0258\n",
            "Epoch [143/200], Step [260/312], Loss: 0.0385\n",
            "Epoch [143/200], Step [280/312], Loss: 0.0476\n",
            "Epoch [143/200], Step [300/312], Loss: 0.0709\n",
            "\n",
            "train-loss: 0.8677, train-acc: 99.9599\n",
            "validation loss: 3.0730, validation acc: 39.0293\n",
            "\n",
            "Epoch 144\n",
            "\n",
            "Epoch [144/200], Step [0/312], Loss: 0.0317\n",
            "Epoch [144/200], Step [20/312], Loss: 0.0259\n",
            "Epoch [144/200], Step [40/312], Loss: 0.0267\n",
            "Epoch [144/200], Step [60/312], Loss: 0.0305\n",
            "Epoch [144/200], Step [80/312], Loss: 0.0199\n",
            "Epoch [144/200], Step [100/312], Loss: 0.0168\n",
            "Epoch [144/200], Step [120/312], Loss: 0.0469\n",
            "Epoch [144/200], Step [140/312], Loss: 0.0417\n",
            "Epoch [144/200], Step [160/312], Loss: 0.0934\n",
            "Epoch [144/200], Step [180/312], Loss: 0.0162\n",
            "Epoch [144/200], Step [200/312], Loss: 0.0128\n",
            "Epoch [144/200], Step [220/312], Loss: 0.0294\n",
            "Epoch [144/200], Step [240/312], Loss: 0.0334\n",
            "Epoch [144/200], Step [260/312], Loss: 0.0345\n",
            "Epoch [144/200], Step [280/312], Loss: 0.0221\n",
            "Epoch [144/200], Step [300/312], Loss: 0.0335\n",
            "\n",
            "train-loss: 0.8619, train-acc: 99.8295\n",
            "validation loss: 3.0735, validation acc: 39.3101\n",
            "\n",
            "Epoch 145\n",
            "\n",
            "Epoch [145/200], Step [0/312], Loss: 0.0325\n",
            "Epoch [145/200], Step [20/312], Loss: 0.0235\n",
            "Epoch [145/200], Step [40/312], Loss: 0.0191\n",
            "Epoch [145/200], Step [60/312], Loss: 0.0391\n",
            "Epoch [145/200], Step [80/312], Loss: 0.0265\n",
            "Epoch [145/200], Step [100/312], Loss: 0.0360\n",
            "Epoch [145/200], Step [120/312], Loss: 0.0444\n",
            "Epoch [145/200], Step [140/312], Loss: 0.0141\n",
            "Epoch [145/200], Step [160/312], Loss: 0.0143\n",
            "Epoch [145/200], Step [180/312], Loss: 0.0232\n",
            "Epoch [145/200], Step [200/312], Loss: 0.0243\n",
            "Epoch [145/200], Step [220/312], Loss: 0.0196\n",
            "Epoch [145/200], Step [240/312], Loss: 0.0136\n",
            "Epoch [145/200], Step [260/312], Loss: 0.0608\n",
            "Epoch [145/200], Step [280/312], Loss: 0.0181\n",
            "Epoch [145/200], Step [300/312], Loss: 0.0346\n",
            "\n",
            "train-loss: 0.8562, train-acc: 99.8997\n",
            "validation loss: 3.0739, validation acc: 39.2298\n",
            "\n",
            "Epoch 146\n",
            "\n",
            "Epoch [146/200], Step [0/312], Loss: 0.0233\n",
            "Epoch [146/200], Step [20/312], Loss: 0.0302\n",
            "Epoch [146/200], Step [40/312], Loss: 0.0581\n",
            "Epoch [146/200], Step [60/312], Loss: 0.0149\n",
            "Epoch [146/200], Step [80/312], Loss: 0.0357\n",
            "Epoch [146/200], Step [100/312], Loss: 0.0195\n",
            "Epoch [146/200], Step [120/312], Loss: 0.0179\n",
            "Epoch [146/200], Step [140/312], Loss: 0.0151\n",
            "Epoch [146/200], Step [160/312], Loss: 0.0251\n",
            "Epoch [146/200], Step [180/312], Loss: 0.0201\n",
            "Epoch [146/200], Step [200/312], Loss: 0.0464\n",
            "Epoch [146/200], Step [220/312], Loss: 0.0298\n",
            "Epoch [146/200], Step [240/312], Loss: 0.0231\n",
            "Epoch [146/200], Step [260/312], Loss: 0.0575\n",
            "Epoch [146/200], Step [280/312], Loss: 0.0375\n",
            "Epoch [146/200], Step [300/312], Loss: 0.0350\n",
            "\n",
            "train-loss: 0.8506, train-acc: 99.9398\n",
            "validation loss: 3.0743, validation acc: 39.3903\n",
            "\n",
            "Epoch 147\n",
            "\n",
            "Epoch [147/200], Step [0/312], Loss: 0.0168\n",
            "Epoch [147/200], Step [20/312], Loss: 0.0211\n",
            "Epoch [147/200], Step [40/312], Loss: 0.1235\n",
            "Epoch [147/200], Step [60/312], Loss: 0.0376\n",
            "Epoch [147/200], Step [80/312], Loss: 0.0578\n",
            "Epoch [147/200], Step [100/312], Loss: 0.0273\n",
            "Epoch [147/200], Step [120/312], Loss: 0.0420\n",
            "Epoch [147/200], Step [140/312], Loss: 0.0148\n",
            "Epoch [147/200], Step [160/312], Loss: 0.0367\n",
            "Epoch [147/200], Step [180/312], Loss: 0.0875\n",
            "Epoch [147/200], Step [200/312], Loss: 0.0130\n",
            "Epoch [147/200], Step [220/312], Loss: 0.0350\n",
            "Epoch [147/200], Step [240/312], Loss: 0.0474\n",
            "Epoch [147/200], Step [260/312], Loss: 0.0253\n",
            "Epoch [147/200], Step [280/312], Loss: 0.0148\n",
            "Epoch [147/200], Step [300/312], Loss: 0.0362\n",
            "\n",
            "train-loss: 0.8451, train-acc: 99.8596\n",
            "validation loss: 3.0748, validation acc: 40.0722\n",
            "\n",
            "Epoch 148\n",
            "\n",
            "Epoch [148/200], Step [0/312], Loss: 0.0214\n",
            "Epoch [148/200], Step [20/312], Loss: 0.0821\n",
            "Epoch [148/200], Step [40/312], Loss: 0.0253\n",
            "Epoch [148/200], Step [60/312], Loss: 0.0507\n",
            "Epoch [148/200], Step [80/312], Loss: 0.0278\n",
            "Epoch [148/200], Step [100/312], Loss: 0.0798\n",
            "Epoch [148/200], Step [120/312], Loss: 0.0513\n",
            "Epoch [148/200], Step [140/312], Loss: 0.0243\n",
            "Epoch [148/200], Step [160/312], Loss: 0.0131\n",
            "Epoch [148/200], Step [180/312], Loss: 0.0512\n",
            "Epoch [148/200], Step [200/312], Loss: 0.0093\n",
            "Epoch [148/200], Step [220/312], Loss: 0.0500\n",
            "Epoch [148/200], Step [240/312], Loss: 0.0323\n",
            "Epoch [148/200], Step [260/312], Loss: 0.0763\n",
            "Epoch [148/200], Step [280/312], Loss: 0.0676\n",
            "Epoch [148/200], Step [300/312], Loss: 0.0256\n",
            "\n",
            "train-loss: 0.8396, train-acc: 99.9198\n",
            "validation loss: 3.0752, validation acc: 39.7513\n",
            "\n",
            "Epoch 149\n",
            "\n",
            "Epoch [149/200], Step [0/312], Loss: 0.0215\n",
            "Epoch [149/200], Step [20/312], Loss: 0.0287\n",
            "Epoch [149/200], Step [40/312], Loss: 0.0419\n",
            "Epoch [149/200], Step [60/312], Loss: 0.0461\n",
            "Epoch [149/200], Step [80/312], Loss: 0.0214\n",
            "Epoch [149/200], Step [100/312], Loss: 0.0325\n",
            "Epoch [149/200], Step [120/312], Loss: 0.1056\n",
            "Epoch [149/200], Step [140/312], Loss: 0.0219\n",
            "Epoch [149/200], Step [160/312], Loss: 0.0364\n",
            "Epoch [149/200], Step [180/312], Loss: 0.0382\n",
            "Epoch [149/200], Step [200/312], Loss: 0.0220\n",
            "Epoch [149/200], Step [220/312], Loss: 0.0407\n",
            "Epoch [149/200], Step [240/312], Loss: 0.0232\n",
            "Epoch [149/200], Step [260/312], Loss: 0.0333\n",
            "Epoch [149/200], Step [280/312], Loss: 0.0353\n",
            "Epoch [149/200], Step [300/312], Loss: 0.0162\n",
            "\n",
            "train-loss: 0.8342, train-acc: 99.8797\n",
            "validation loss: 3.0758, validation acc: 38.8287\n",
            "\n",
            "Epoch 150\n",
            "\n",
            "Epoch [150/200], Step [0/312], Loss: 0.0210\n",
            "Epoch [150/200], Step [20/312], Loss: 0.0147\n",
            "Epoch [150/200], Step [40/312], Loss: 0.0810\n",
            "Epoch [150/200], Step [60/312], Loss: 0.0305\n",
            "Epoch [150/200], Step [80/312], Loss: 0.0290\n",
            "Epoch [150/200], Step [100/312], Loss: 0.0555\n",
            "Epoch [150/200], Step [120/312], Loss: 0.0186\n",
            "Epoch [150/200], Step [140/312], Loss: 0.0511\n",
            "Epoch [150/200], Step [160/312], Loss: 0.0252\n",
            "Epoch [150/200], Step [180/312], Loss: 0.0288\n",
            "Epoch [150/200], Step [200/312], Loss: 0.0303\n",
            "Epoch [150/200], Step [220/312], Loss: 0.0354\n",
            "Epoch [150/200], Step [240/312], Loss: 0.0465\n",
            "Epoch [150/200], Step [260/312], Loss: 0.0429\n",
            "Epoch [150/200], Step [280/312], Loss: 0.0279\n",
            "Epoch [150/200], Step [300/312], Loss: 0.0464\n",
            "\n",
            "train-loss: 0.8289, train-acc: 99.8797\n",
            "validation loss: 3.0764, validation acc: 38.9089\n",
            "\n",
            "Epoch 151\n",
            "\n",
            "Epoch [151/200], Step [0/312], Loss: 0.0107\n",
            "Epoch [151/200], Step [20/312], Loss: 0.0178\n",
            "Epoch [151/200], Step [40/312], Loss: 0.0168\n",
            "Epoch [151/200], Step [60/312], Loss: 0.0320\n",
            "Epoch [151/200], Step [80/312], Loss: 0.0324\n",
            "Epoch [151/200], Step [100/312], Loss: 0.0628\n",
            "Epoch [151/200], Step [120/312], Loss: 0.0825\n",
            "Epoch [151/200], Step [140/312], Loss: 0.0193\n",
            "Epoch [151/200], Step [160/312], Loss: 0.0222\n",
            "Epoch [151/200], Step [180/312], Loss: 0.0225\n",
            "Epoch [151/200], Step [200/312], Loss: 0.0210\n",
            "Epoch [151/200], Step [220/312], Loss: 0.0108\n",
            "Epoch [151/200], Step [240/312], Loss: 0.0213\n",
            "Epoch [151/200], Step [260/312], Loss: 0.0293\n",
            "Epoch [151/200], Step [280/312], Loss: 0.0281\n",
            "Epoch [151/200], Step [300/312], Loss: 0.0306\n",
            "\n",
            "train-loss: 0.8237, train-acc: 99.9398\n",
            "validation loss: 3.0769, validation acc: 39.3903\n",
            "\n",
            "Epoch 152\n",
            "\n",
            "Epoch [152/200], Step [0/312], Loss: 0.0169\n",
            "Epoch [152/200], Step [20/312], Loss: 0.0406\n",
            "Epoch [152/200], Step [40/312], Loss: 0.0122\n",
            "Epoch [152/200], Step [60/312], Loss: 0.0701\n",
            "Epoch [152/200], Step [80/312], Loss: 0.0122\n",
            "Epoch [152/200], Step [100/312], Loss: 0.0121\n",
            "Epoch [152/200], Step [120/312], Loss: 0.0339\n",
            "Epoch [152/200], Step [140/312], Loss: 0.0345\n",
            "Epoch [152/200], Step [160/312], Loss: 0.0250\n",
            "Epoch [152/200], Step [180/312], Loss: 0.0199\n",
            "Epoch [152/200], Step [200/312], Loss: 0.0138\n",
            "Epoch [152/200], Step [220/312], Loss: 0.0248\n",
            "Epoch [152/200], Step [240/312], Loss: 0.0381\n",
            "Epoch [152/200], Step [260/312], Loss: 0.2479\n",
            "Epoch [152/200], Step [280/312], Loss: 0.0587\n",
            "Epoch [152/200], Step [300/312], Loss: 0.0295\n",
            "\n",
            "train-loss: 0.8185, train-acc: 99.9198\n",
            "validation loss: 3.0774, validation acc: 39.7513\n",
            "\n",
            "Epoch 153\n",
            "\n",
            "Epoch [153/200], Step [0/312], Loss: 0.0434\n",
            "Epoch [153/200], Step [20/312], Loss: 0.0169\n",
            "Epoch [153/200], Step [40/312], Loss: 0.0394\n",
            "Epoch [153/200], Step [60/312], Loss: 0.0467\n",
            "Epoch [153/200], Step [80/312], Loss: 0.0239\n",
            "Epoch [153/200], Step [100/312], Loss: 0.0148\n",
            "Epoch [153/200], Step [120/312], Loss: 0.0216\n",
            "Epoch [153/200], Step [140/312], Loss: 0.0225\n",
            "Epoch [153/200], Step [160/312], Loss: 0.0295\n",
            "Epoch [153/200], Step [180/312], Loss: 0.0215\n",
            "Epoch [153/200], Step [200/312], Loss: 0.0145\n",
            "Epoch [153/200], Step [220/312], Loss: 0.0450\n",
            "Epoch [153/200], Step [240/312], Loss: 0.0415\n",
            "Epoch [153/200], Step [260/312], Loss: 0.0212\n",
            "Epoch [153/200], Step [280/312], Loss: 0.0214\n",
            "Epoch [153/200], Step [300/312], Loss: 0.0310\n",
            "\n",
            "train-loss: 0.8134, train-acc: 99.8897\n",
            "validation loss: 3.0779, validation acc: 39.3101\n",
            "\n",
            "Epoch 154\n",
            "\n",
            "Epoch [154/200], Step [0/312], Loss: 0.0274\n",
            "Epoch [154/200], Step [20/312], Loss: 0.0225\n",
            "Epoch [154/200], Step [40/312], Loss: 0.0299\n",
            "Epoch [154/200], Step [60/312], Loss: 0.0449\n",
            "Epoch [154/200], Step [80/312], Loss: 0.0174\n",
            "Epoch [154/200], Step [100/312], Loss: 0.0281\n",
            "Epoch [154/200], Step [120/312], Loss: 0.0457\n",
            "Epoch [154/200], Step [140/312], Loss: 0.0259\n",
            "Epoch [154/200], Step [160/312], Loss: 0.0202\n",
            "Epoch [154/200], Step [180/312], Loss: 0.0178\n",
            "Epoch [154/200], Step [200/312], Loss: 0.0138\n",
            "Epoch [154/200], Step [220/312], Loss: 0.0164\n",
            "Epoch [154/200], Step [240/312], Loss: 0.0782\n",
            "Epoch [154/200], Step [260/312], Loss: 0.0134\n",
            "Epoch [154/200], Step [280/312], Loss: 0.0218\n",
            "Epoch [154/200], Step [300/312], Loss: 0.0166\n",
            "\n",
            "train-loss: 0.8083, train-acc: 99.9198\n",
            "validation loss: 3.0782, validation acc: 39.2700\n",
            "\n",
            "Epoch 155\n",
            "\n",
            "Epoch [155/200], Step [0/312], Loss: 0.0352\n",
            "Epoch [155/200], Step [20/312], Loss: 0.0270\n",
            "Epoch [155/200], Step [40/312], Loss: 0.0266\n",
            "Epoch [155/200], Step [60/312], Loss: 0.0127\n",
            "Epoch [155/200], Step [80/312], Loss: 0.0220\n",
            "Epoch [155/200], Step [100/312], Loss: 0.0330\n",
            "Epoch [155/200], Step [120/312], Loss: 0.0202\n",
            "Epoch [155/200], Step [140/312], Loss: 0.0174\n",
            "Epoch [155/200], Step [160/312], Loss: 0.0115\n",
            "Epoch [155/200], Step [180/312], Loss: 0.0447\n",
            "Epoch [155/200], Step [200/312], Loss: 0.0194\n",
            "Epoch [155/200], Step [220/312], Loss: 0.0443\n",
            "Epoch [155/200], Step [240/312], Loss: 0.0135\n",
            "Epoch [155/200], Step [260/312], Loss: 0.0569\n",
            "Epoch [155/200], Step [280/312], Loss: 0.0217\n",
            "Epoch [155/200], Step [300/312], Loss: 0.0716\n",
            "\n",
            "train-loss: 0.8033, train-acc: 99.8797\n",
            "validation loss: 3.0788, validation acc: 39.1897\n",
            "\n",
            "Epoch 156\n",
            "\n",
            "Epoch [156/200], Step [0/312], Loss: 0.0329\n",
            "Epoch [156/200], Step [20/312], Loss: 0.0169\n",
            "Epoch [156/200], Step [40/312], Loss: 0.0206\n",
            "Epoch [156/200], Step [60/312], Loss: 0.0260\n",
            "Epoch [156/200], Step [80/312], Loss: 0.0291\n",
            "Epoch [156/200], Step [100/312], Loss: 0.0133\n",
            "Epoch [156/200], Step [120/312], Loss: 0.0243\n",
            "Epoch [156/200], Step [140/312], Loss: 0.0180\n",
            "Epoch [156/200], Step [160/312], Loss: 0.0224\n",
            "Epoch [156/200], Step [180/312], Loss: 0.0237\n",
            "Epoch [156/200], Step [200/312], Loss: 0.0770\n",
            "Epoch [156/200], Step [220/312], Loss: 0.0124\n",
            "Epoch [156/200], Step [240/312], Loss: 0.0145\n",
            "Epoch [156/200], Step [260/312], Loss: 0.0489\n",
            "Epoch [156/200], Step [280/312], Loss: 0.0285\n",
            "Epoch [156/200], Step [300/312], Loss: 0.0272\n",
            "\n",
            "train-loss: 0.7984, train-acc: 99.9398\n",
            "validation loss: 3.0791, validation acc: 39.8315\n",
            "\n",
            "Epoch 157\n",
            "\n",
            "Epoch [157/200], Step [0/312], Loss: 0.0218\n",
            "Epoch [157/200], Step [20/312], Loss: 0.0311\n",
            "Epoch [157/200], Step [40/312], Loss: 0.0285\n",
            "Epoch [157/200], Step [60/312], Loss: 0.0344\n",
            "Epoch [157/200], Step [80/312], Loss: 0.0583\n",
            "Epoch [157/200], Step [100/312], Loss: 0.0178\n",
            "Epoch [157/200], Step [120/312], Loss: 0.0274\n",
            "Epoch [157/200], Step [140/312], Loss: 0.0567\n",
            "Epoch [157/200], Step [160/312], Loss: 0.0281\n",
            "Epoch [157/200], Step [180/312], Loss: 0.0167\n",
            "Epoch [157/200], Step [200/312], Loss: 0.0591\n",
            "Epoch [157/200], Step [220/312], Loss: 0.0182\n",
            "Epoch [157/200], Step [240/312], Loss: 0.0267\n",
            "Epoch [157/200], Step [260/312], Loss: 0.0194\n",
            "Epoch [157/200], Step [280/312], Loss: 0.0327\n",
            "Epoch [157/200], Step [300/312], Loss: 0.0243\n",
            "\n",
            "train-loss: 0.7935, train-acc: 99.8797\n",
            "validation loss: 3.0796, validation acc: 39.5106\n",
            "\n",
            "Epoch 158\n",
            "\n",
            "Epoch [158/200], Step [0/312], Loss: 0.0169\n",
            "Epoch [158/200], Step [20/312], Loss: 0.0185\n",
            "Epoch [158/200], Step [40/312], Loss: 0.0626\n",
            "Epoch [158/200], Step [60/312], Loss: 0.0216\n",
            "Epoch [158/200], Step [80/312], Loss: 0.0176\n",
            "Epoch [158/200], Step [100/312], Loss: 0.0144\n",
            "Epoch [158/200], Step [120/312], Loss: 0.0255\n",
            "Epoch [158/200], Step [140/312], Loss: 0.0229\n",
            "Epoch [158/200], Step [160/312], Loss: 0.0295\n",
            "Epoch [158/200], Step [180/312], Loss: 0.0240\n",
            "Epoch [158/200], Step [200/312], Loss: 0.0283\n",
            "Epoch [158/200], Step [220/312], Loss: 0.0163\n",
            "Epoch [158/200], Step [240/312], Loss: 0.0171\n",
            "Epoch [158/200], Step [260/312], Loss: 0.0458\n",
            "Epoch [158/200], Step [280/312], Loss: 0.0231\n",
            "Epoch [158/200], Step [300/312], Loss: 0.0225\n",
            "\n",
            "train-loss: 0.7887, train-acc: 99.9398\n",
            "validation loss: 3.0800, validation acc: 39.3101\n",
            "\n",
            "Epoch 159\n",
            "\n",
            "Epoch [159/200], Step [0/312], Loss: 0.0209\n",
            "Epoch [159/200], Step [20/312], Loss: 0.0176\n",
            "Epoch [159/200], Step [40/312], Loss: 0.0412\n",
            "Epoch [159/200], Step [60/312], Loss: 0.0174\n",
            "Epoch [159/200], Step [80/312], Loss: 0.0317\n",
            "Epoch [159/200], Step [100/312], Loss: 0.0235\n",
            "Epoch [159/200], Step [120/312], Loss: 0.0501\n",
            "Epoch [159/200], Step [140/312], Loss: 0.0414\n",
            "Epoch [159/200], Step [160/312], Loss: 0.0451\n",
            "Epoch [159/200], Step [180/312], Loss: 0.0141\n",
            "Epoch [159/200], Step [200/312], Loss: 0.0120\n",
            "Epoch [159/200], Step [220/312], Loss: 0.0222\n",
            "Epoch [159/200], Step [240/312], Loss: 0.0151\n",
            "Epoch [159/200], Step [260/312], Loss: 0.0449\n",
            "Epoch [159/200], Step [280/312], Loss: 0.0169\n",
            "Epoch [159/200], Step [300/312], Loss: 0.0283\n",
            "\n",
            "train-loss: 0.7840, train-acc: 99.8797\n",
            "validation loss: 3.0806, validation acc: 39.4304\n",
            "\n",
            "Epoch 160\n",
            "\n",
            "Epoch [160/200], Step [0/312], Loss: 0.0120\n",
            "Epoch [160/200], Step [20/312], Loss: 0.0429\n",
            "Epoch [160/200], Step [40/312], Loss: 0.0175\n",
            "Epoch [160/200], Step [60/312], Loss: 0.0164\n",
            "Epoch [160/200], Step [80/312], Loss: 0.0330\n",
            "Epoch [160/200], Step [100/312], Loss: 0.0997\n",
            "Epoch [160/200], Step [120/312], Loss: 0.0235\n",
            "Epoch [160/200], Step [140/312], Loss: 0.0202\n",
            "Epoch [160/200], Step [160/312], Loss: 0.0140\n",
            "Epoch [160/200], Step [180/312], Loss: 0.2916\n",
            "Epoch [160/200], Step [200/312], Loss: 0.0115\n",
            "Epoch [160/200], Step [220/312], Loss: 0.0201\n",
            "Epoch [160/200], Step [240/312], Loss: 0.0176\n",
            "Epoch [160/200], Step [260/312], Loss: 0.0354\n",
            "Epoch [160/200], Step [280/312], Loss: 0.1189\n",
            "Epoch [160/200], Step [300/312], Loss: 0.0316\n",
            "\n",
            "train-loss: 0.7793, train-acc: 99.9198\n",
            "validation loss: 3.0810, validation acc: 39.6310\n",
            "\n",
            "Epoch 161\n",
            "\n",
            "Epoch [161/200], Step [0/312], Loss: 0.0164\n",
            "Epoch [161/200], Step [20/312], Loss: 0.0214\n",
            "Epoch [161/200], Step [40/312], Loss: 0.0141\n",
            "Epoch [161/200], Step [60/312], Loss: 0.0144\n",
            "Epoch [161/200], Step [80/312], Loss: 0.0843\n",
            "Epoch [161/200], Step [100/312], Loss: 0.0911\n",
            "Epoch [161/200], Step [120/312], Loss: 0.0241\n",
            "Epoch [161/200], Step [140/312], Loss: 0.0158\n",
            "Epoch [161/200], Step [160/312], Loss: 0.0668\n",
            "Epoch [161/200], Step [180/312], Loss: 0.0163\n",
            "Epoch [161/200], Step [200/312], Loss: 0.0130\n",
            "Epoch [161/200], Step [220/312], Loss: 0.0157\n",
            "Epoch [161/200], Step [240/312], Loss: 0.1288\n",
            "Epoch [161/200], Step [260/312], Loss: 0.0301\n",
            "Epoch [161/200], Step [280/312], Loss: 0.0468\n",
            "Epoch [161/200], Step [300/312], Loss: 0.0138\n",
            "\n",
            "train-loss: 0.7746, train-acc: 99.8897\n",
            "validation loss: 3.0815, validation acc: 39.4304\n",
            "\n",
            "Epoch 162\n",
            "\n",
            "Epoch [162/200], Step [0/312], Loss: 0.0280\n",
            "Epoch [162/200], Step [20/312], Loss: 0.0169\n",
            "Epoch [162/200], Step [40/312], Loss: 0.0360\n",
            "Epoch [162/200], Step [60/312], Loss: 0.0184\n",
            "Epoch [162/200], Step [80/312], Loss: 0.0386\n",
            "Epoch [162/200], Step [100/312], Loss: 0.0172\n",
            "Epoch [162/200], Step [120/312], Loss: 0.0440\n",
            "Epoch [162/200], Step [140/312], Loss: 0.1768\n",
            "Epoch [162/200], Step [160/312], Loss: 0.0320\n",
            "Epoch [162/200], Step [180/312], Loss: 0.0192\n",
            "Epoch [162/200], Step [200/312], Loss: 0.0178\n",
            "Epoch [162/200], Step [220/312], Loss: 0.0532\n",
            "Epoch [162/200], Step [240/312], Loss: 0.0299\n",
            "Epoch [162/200], Step [260/312], Loss: 0.0251\n",
            "Epoch [162/200], Step [280/312], Loss: 0.0458\n",
            "Epoch [162/200], Step [300/312], Loss: 0.0236\n",
            "\n",
            "train-loss: 0.7701, train-acc: 99.9097\n",
            "validation loss: 3.0820, validation acc: 39.0694\n",
            "\n",
            "Epoch 163\n",
            "\n",
            "Epoch [163/200], Step [0/312], Loss: 0.0316\n",
            "Epoch [163/200], Step [20/312], Loss: 0.0142\n",
            "Epoch [163/200], Step [40/312], Loss: 0.0187\n",
            "Epoch [163/200], Step [60/312], Loss: 0.0369\n",
            "Epoch [163/200], Step [80/312], Loss: 0.0674\n",
            "Epoch [163/200], Step [100/312], Loss: 0.0468\n",
            "Epoch [163/200], Step [120/312], Loss: 0.0231\n",
            "Epoch [163/200], Step [140/312], Loss: 0.0134\n",
            "Epoch [163/200], Step [160/312], Loss: 0.0260\n",
            "Epoch [163/200], Step [180/312], Loss: 0.0148\n",
            "Epoch [163/200], Step [200/312], Loss: 0.0152\n",
            "Epoch [163/200], Step [220/312], Loss: 0.0404\n",
            "Epoch [163/200], Step [240/312], Loss: 0.0537\n",
            "Epoch [163/200], Step [260/312], Loss: 0.0641\n",
            "Epoch [163/200], Step [280/312], Loss: 0.0127\n",
            "Epoch [163/200], Step [300/312], Loss: 0.0289\n",
            "\n",
            "train-loss: 0.7656, train-acc: 99.9499\n",
            "validation loss: 3.0824, validation acc: 38.9089\n",
            "\n",
            "Epoch 164\n",
            "\n",
            "Epoch [164/200], Step [0/312], Loss: 0.0228\n",
            "Epoch [164/200], Step [20/312], Loss: 0.0153\n",
            "Epoch [164/200], Step [40/312], Loss: 0.0174\n",
            "Epoch [164/200], Step [60/312], Loss: 0.0140\n",
            "Epoch [164/200], Step [80/312], Loss: 0.0339\n",
            "Epoch [164/200], Step [100/312], Loss: 0.0245\n",
            "Epoch [164/200], Step [120/312], Loss: 0.0165\n",
            "Epoch [164/200], Step [140/312], Loss: 0.0137\n",
            "Epoch [164/200], Step [160/312], Loss: 0.1405\n",
            "Epoch [164/200], Step [180/312], Loss: 0.0435\n",
            "Epoch [164/200], Step [200/312], Loss: 0.0182\n",
            "Epoch [164/200], Step [220/312], Loss: 0.0101\n",
            "Epoch [164/200], Step [240/312], Loss: 0.0529\n",
            "Epoch [164/200], Step [260/312], Loss: 0.0135\n",
            "Epoch [164/200], Step [280/312], Loss: 0.0334\n",
            "Epoch [164/200], Step [300/312], Loss: 0.0140\n",
            "\n",
            "train-loss: 0.7611, train-acc: 99.9198\n",
            "validation loss: 3.0830, validation acc: 39.1095\n",
            "\n",
            "Epoch 165\n",
            "\n",
            "Epoch [165/200], Step [0/312], Loss: 0.0626\n",
            "Epoch [165/200], Step [20/312], Loss: 0.0286\n",
            "Epoch [165/200], Step [40/312], Loss: 0.0260\n",
            "Epoch [165/200], Step [60/312], Loss: 0.0235\n",
            "Epoch [165/200], Step [80/312], Loss: 0.0156\n",
            "Epoch [165/200], Step [100/312], Loss: 0.0191\n",
            "Epoch [165/200], Step [120/312], Loss: 0.0631\n",
            "Epoch [165/200], Step [140/312], Loss: 0.0147\n",
            "Epoch [165/200], Step [160/312], Loss: 0.0368\n",
            "Epoch [165/200], Step [180/312], Loss: 0.0287\n",
            "Epoch [165/200], Step [200/312], Loss: 0.0255\n",
            "Epoch [165/200], Step [220/312], Loss: 0.0205\n",
            "Epoch [165/200], Step [240/312], Loss: 0.0281\n",
            "Epoch [165/200], Step [260/312], Loss: 0.0342\n",
            "Epoch [165/200], Step [280/312], Loss: 0.0126\n",
            "Epoch [165/200], Step [300/312], Loss: 0.0441\n",
            "\n",
            "train-loss: 0.7567, train-acc: 99.8997\n",
            "validation loss: 3.0834, validation acc: 38.6683\n",
            "\n",
            "Epoch 166\n",
            "\n",
            "Epoch [166/200], Step [0/312], Loss: 0.0393\n",
            "Epoch [166/200], Step [20/312], Loss: 0.0216\n",
            "Epoch [166/200], Step [40/312], Loss: 0.0304\n",
            "Epoch [166/200], Step [60/312], Loss: 0.0378\n",
            "Epoch [166/200], Step [80/312], Loss: 0.0217\n",
            "Epoch [166/200], Step [100/312], Loss: 0.0440\n",
            "Epoch [166/200], Step [120/312], Loss: 0.0315\n",
            "Epoch [166/200], Step [140/312], Loss: 0.0287\n",
            "Epoch [166/200], Step [160/312], Loss: 0.0671\n",
            "Epoch [166/200], Step [180/312], Loss: 0.0228\n",
            "Epoch [166/200], Step [200/312], Loss: 0.0152\n",
            "Epoch [166/200], Step [220/312], Loss: 0.0092\n",
            "Epoch [166/200], Step [240/312], Loss: 0.0571\n",
            "Epoch [166/200], Step [260/312], Loss: 0.0749\n",
            "Epoch [166/200], Step [280/312], Loss: 0.0594\n",
            "Epoch [166/200], Step [300/312], Loss: 0.0114\n",
            "\n",
            "train-loss: 0.7523, train-acc: 99.9699\n",
            "validation loss: 3.0838, validation acc: 39.2700\n",
            "\n",
            "Epoch 167\n",
            "\n",
            "Epoch [167/200], Step [0/312], Loss: 0.0395\n",
            "Epoch [167/200], Step [20/312], Loss: 0.0181\n",
            "Epoch [167/200], Step [40/312], Loss: 0.0123\n",
            "Epoch [167/200], Step [60/312], Loss: 0.0218\n",
            "Epoch [167/200], Step [80/312], Loss: 0.0216\n",
            "Epoch [167/200], Step [100/312], Loss: 0.0306\n",
            "Epoch [167/200], Step [120/312], Loss: 0.0422\n",
            "Epoch [167/200], Step [140/312], Loss: 0.0393\n",
            "Epoch [167/200], Step [160/312], Loss: 0.0188\n",
            "Epoch [167/200], Step [180/312], Loss: 0.0604\n",
            "Epoch [167/200], Step [200/312], Loss: 0.0495\n",
            "Epoch [167/200], Step [220/312], Loss: 0.0449\n",
            "Epoch [167/200], Step [240/312], Loss: 0.0341\n",
            "Epoch [167/200], Step [260/312], Loss: 0.0182\n",
            "Epoch [167/200], Step [280/312], Loss: 0.0164\n",
            "Epoch [167/200], Step [300/312], Loss: 0.0257\n",
            "\n",
            "train-loss: 0.7480, train-acc: 99.9198\n",
            "validation loss: 3.0842, validation acc: 39.2700\n",
            "\n",
            "Epoch 168\n",
            "\n",
            "Epoch [168/200], Step [0/312], Loss: 0.0098\n",
            "Epoch [168/200], Step [20/312], Loss: 0.0254\n",
            "Epoch [168/200], Step [40/312], Loss: 0.0324\n",
            "Epoch [168/200], Step [60/312], Loss: 0.0165\n",
            "Epoch [168/200], Step [80/312], Loss: 0.0128\n",
            "Epoch [168/200], Step [100/312], Loss: 0.0229\n",
            "Epoch [168/200], Step [120/312], Loss: 0.0553\n",
            "Epoch [168/200], Step [140/312], Loss: 0.0261\n",
            "Epoch [168/200], Step [160/312], Loss: 0.0198\n",
            "Epoch [168/200], Step [180/312], Loss: 0.0096\n",
            "Epoch [168/200], Step [200/312], Loss: 0.0287\n",
            "Epoch [168/200], Step [220/312], Loss: 0.0228\n",
            "Epoch [168/200], Step [240/312], Loss: 0.0700\n",
            "Epoch [168/200], Step [260/312], Loss: 0.0254\n",
            "Epoch [168/200], Step [280/312], Loss: 0.0329\n",
            "Epoch [168/200], Step [300/312], Loss: 0.0139\n",
            "\n",
            "train-loss: 0.7437, train-acc: 99.9398\n",
            "validation loss: 3.0845, validation acc: 38.9491\n",
            "\n",
            "Epoch 169\n",
            "\n",
            "Epoch [169/200], Step [0/312], Loss: 0.1015\n",
            "Epoch [169/200], Step [20/312], Loss: 0.0159\n",
            "Epoch [169/200], Step [40/312], Loss: 0.0128\n",
            "Epoch [169/200], Step [60/312], Loss: 0.0190\n",
            "Epoch [169/200], Step [80/312], Loss: 0.0432\n",
            "Epoch [169/200], Step [100/312], Loss: 0.0161\n",
            "Epoch [169/200], Step [120/312], Loss: 0.0494\n",
            "Epoch [169/200], Step [140/312], Loss: 0.0518\n",
            "Epoch [169/200], Step [160/312], Loss: 0.0277\n",
            "Epoch [169/200], Step [180/312], Loss: 0.0158\n",
            "Epoch [169/200], Step [200/312], Loss: 0.0260\n",
            "Epoch [169/200], Step [220/312], Loss: 0.0269\n",
            "Epoch [169/200], Step [240/312], Loss: 0.0160\n",
            "Epoch [169/200], Step [260/312], Loss: 0.0357\n",
            "Epoch [169/200], Step [280/312], Loss: 0.0711\n",
            "Epoch [169/200], Step [300/312], Loss: 0.0186\n",
            "\n",
            "train-loss: 0.7395, train-acc: 99.9298\n",
            "validation loss: 3.0850, validation acc: 38.7886\n",
            "\n",
            "Epoch 170\n",
            "\n",
            "Epoch [170/200], Step [0/312], Loss: 0.0283\n",
            "Epoch [170/200], Step [20/312], Loss: 0.1108\n",
            "Epoch [170/200], Step [40/312], Loss: 0.0181\n",
            "Epoch [170/200], Step [60/312], Loss: 0.0375\n",
            "Epoch [170/200], Step [80/312], Loss: 0.0146\n",
            "Epoch [170/200], Step [100/312], Loss: 0.0357\n",
            "Epoch [170/200], Step [120/312], Loss: 0.0197\n",
            "Epoch [170/200], Step [140/312], Loss: 0.0135\n",
            "Epoch [170/200], Step [160/312], Loss: 0.0347\n",
            "Epoch [170/200], Step [180/312], Loss: 0.0252\n",
            "Epoch [170/200], Step [200/312], Loss: 0.0601\n",
            "Epoch [170/200], Step [220/312], Loss: 0.0205\n",
            "Epoch [170/200], Step [240/312], Loss: 0.0584\n",
            "Epoch [170/200], Step [260/312], Loss: 0.0160\n",
            "Epoch [170/200], Step [280/312], Loss: 0.0344\n",
            "Epoch [170/200], Step [300/312], Loss: 0.0192\n",
            "\n",
            "train-loss: 0.7353, train-acc: 99.9499\n",
            "validation loss: 3.0855, validation acc: 39.1897\n",
            "\n",
            "Epoch 171\n",
            "\n",
            "Epoch [171/200], Step [0/312], Loss: 0.0099\n",
            "Epoch [171/200], Step [20/312], Loss: 0.0175\n",
            "Epoch [171/200], Step [40/312], Loss: 0.0264\n",
            "Epoch [171/200], Step [60/312], Loss: 0.0177\n",
            "Epoch [171/200], Step [80/312], Loss: 0.0266\n",
            "Epoch [171/200], Step [100/312], Loss: 0.0325\n",
            "Epoch [171/200], Step [120/312], Loss: 0.0251\n",
            "Epoch [171/200], Step [140/312], Loss: 0.0180\n",
            "Epoch [171/200], Step [160/312], Loss: 0.0703\n",
            "Epoch [171/200], Step [180/312], Loss: 0.0219\n",
            "Epoch [171/200], Step [200/312], Loss: 0.0168\n",
            "Epoch [171/200], Step [220/312], Loss: 0.0292\n",
            "Epoch [171/200], Step [240/312], Loss: 0.0378\n",
            "Epoch [171/200], Step [260/312], Loss: 0.0323\n",
            "Epoch [171/200], Step [280/312], Loss: 0.0258\n",
            "Epoch [171/200], Step [300/312], Loss: 0.0278\n",
            "\n",
            "train-loss: 0.7312, train-acc: 99.8596\n",
            "validation loss: 3.0859, validation acc: 39.2700\n",
            "\n",
            "Epoch 172\n",
            "\n",
            "Epoch [172/200], Step [0/312], Loss: 0.0363\n",
            "Epoch [172/200], Step [20/312], Loss: 0.0166\n",
            "Epoch [172/200], Step [40/312], Loss: 0.0395\n",
            "Epoch [172/200], Step [60/312], Loss: 0.0287\n",
            "Epoch [172/200], Step [80/312], Loss: 0.0186\n",
            "Epoch [172/200], Step [100/312], Loss: 0.0375\n",
            "Epoch [172/200], Step [120/312], Loss: 0.0166\n",
            "Epoch [172/200], Step [140/312], Loss: 0.0697\n",
            "Epoch [172/200], Step [160/312], Loss: 0.0177\n",
            "Epoch [172/200], Step [180/312], Loss: 0.0248\n",
            "Epoch [172/200], Step [200/312], Loss: 0.0243\n",
            "Epoch [172/200], Step [220/312], Loss: 0.0189\n",
            "Epoch [172/200], Step [240/312], Loss: 0.0108\n",
            "Epoch [172/200], Step [260/312], Loss: 0.0581\n",
            "Epoch [172/200], Step [280/312], Loss: 0.0180\n",
            "Epoch [172/200], Step [300/312], Loss: 0.0728\n",
            "\n",
            "train-loss: 0.7271, train-acc: 99.9298\n",
            "validation loss: 3.0864, validation acc: 38.7084\n",
            "\n",
            "Epoch 173\n",
            "\n",
            "Epoch [173/200], Step [0/312], Loss: 0.0093\n",
            "Epoch [173/200], Step [20/312], Loss: 0.0409\n",
            "Epoch [173/200], Step [40/312], Loss: 0.0332\n",
            "Epoch [173/200], Step [60/312], Loss: 0.0217\n",
            "Epoch [173/200], Step [80/312], Loss: 0.0217\n",
            "Epoch [173/200], Step [100/312], Loss: 0.0185\n",
            "Epoch [173/200], Step [120/312], Loss: 0.0214\n",
            "Epoch [173/200], Step [140/312], Loss: 0.0226\n",
            "Epoch [173/200], Step [160/312], Loss: 0.0691\n",
            "Epoch [173/200], Step [180/312], Loss: 0.0201\n",
            "Epoch [173/200], Step [200/312], Loss: 0.0364\n",
            "Epoch [173/200], Step [220/312], Loss: 0.0265\n",
            "Epoch [173/200], Step [240/312], Loss: 0.0262\n",
            "Epoch [173/200], Step [260/312], Loss: 0.0200\n",
            "Epoch [173/200], Step [280/312], Loss: 0.0228\n",
            "Epoch [173/200], Step [300/312], Loss: 0.0207\n",
            "\n",
            "train-loss: 0.7231, train-acc: 99.9398\n",
            "validation loss: 3.0868, validation acc: 39.2700\n",
            "\n",
            "Epoch 174\n",
            "\n",
            "Epoch [174/200], Step [0/312], Loss: 0.0195\n",
            "Epoch [174/200], Step [20/312], Loss: 0.0247\n",
            "Epoch [174/200], Step [40/312], Loss: 0.0263\n",
            "Epoch [174/200], Step [60/312], Loss: 0.0202\n",
            "Epoch [174/200], Step [80/312], Loss: 0.0210\n",
            "Epoch [174/200], Step [100/312], Loss: 0.0127\n",
            "Epoch [174/200], Step [120/312], Loss: 0.0128\n",
            "Epoch [174/200], Step [140/312], Loss: 0.0161\n",
            "Epoch [174/200], Step [160/312], Loss: 0.0111\n",
            "Epoch [174/200], Step [180/312], Loss: 0.0206\n",
            "Epoch [174/200], Step [200/312], Loss: 0.1165\n",
            "Epoch [174/200], Step [220/312], Loss: 0.0112\n",
            "Epoch [174/200], Step [240/312], Loss: 0.0174\n",
            "Epoch [174/200], Step [260/312], Loss: 0.0230\n",
            "Epoch [174/200], Step [280/312], Loss: 0.0139\n",
            "Epoch [174/200], Step [300/312], Loss: 0.0210\n",
            "\n",
            "train-loss: 0.7191, train-acc: 99.9398\n",
            "validation loss: 3.0872, validation acc: 39.4705\n",
            "\n",
            "Epoch 175\n",
            "\n",
            "Epoch [175/200], Step [0/312], Loss: 0.0169\n",
            "Epoch [175/200], Step [20/312], Loss: 0.0358\n",
            "Epoch [175/200], Step [40/312], Loss: 0.0177\n",
            "Epoch [175/200], Step [60/312], Loss: 0.0134\n",
            "Epoch [175/200], Step [80/312], Loss: 0.0184\n",
            "Epoch [175/200], Step [100/312], Loss: 0.0244\n",
            "Epoch [175/200], Step [120/312], Loss: 0.0279\n",
            "Epoch [175/200], Step [140/312], Loss: 0.0117\n",
            "Epoch [175/200], Step [160/312], Loss: 0.0173\n",
            "Epoch [175/200], Step [180/312], Loss: 0.0184\n",
            "Epoch [175/200], Step [200/312], Loss: 0.0565\n",
            "Epoch [175/200], Step [220/312], Loss: 0.0327\n",
            "Epoch [175/200], Step [240/312], Loss: 0.0133\n",
            "Epoch [175/200], Step [260/312], Loss: 0.0196\n",
            "Epoch [175/200], Step [280/312], Loss: 0.0239\n",
            "Epoch [175/200], Step [300/312], Loss: 0.0176\n",
            "\n",
            "train-loss: 0.7152, train-acc: 99.9398\n",
            "validation loss: 3.0876, validation acc: 38.8287\n",
            "\n",
            "Epoch 176\n",
            "\n",
            "Epoch [176/200], Step [0/312], Loss: 0.0220\n",
            "Epoch [176/200], Step [20/312], Loss: 0.0286\n",
            "Epoch [176/200], Step [40/312], Loss: 0.0127\n",
            "Epoch [176/200], Step [60/312], Loss: 0.0124\n",
            "Epoch [176/200], Step [80/312], Loss: 0.0186\n",
            "Epoch [176/200], Step [100/312], Loss: 0.0201\n",
            "Epoch [176/200], Step [120/312], Loss: 0.0407\n",
            "Epoch [176/200], Step [140/312], Loss: 0.0211\n",
            "Epoch [176/200], Step [160/312], Loss: 0.0146\n",
            "Epoch [176/200], Step [180/312], Loss: 0.0440\n",
            "Epoch [176/200], Step [200/312], Loss: 0.0118\n",
            "Epoch [176/200], Step [220/312], Loss: 0.0149\n",
            "Epoch [176/200], Step [240/312], Loss: 0.0548\n",
            "Epoch [176/200], Step [260/312], Loss: 0.0209\n",
            "Epoch [176/200], Step [280/312], Loss: 0.0494\n",
            "Epoch [176/200], Step [300/312], Loss: 0.0179\n",
            "\n",
            "train-loss: 0.7113, train-acc: 99.9097\n",
            "validation loss: 3.0879, validation acc: 39.1897\n",
            "\n",
            "Epoch 177\n",
            "\n",
            "Epoch [177/200], Step [0/312], Loss: 0.0480\n",
            "Epoch [177/200], Step [20/312], Loss: 0.0340\n",
            "Epoch [177/200], Step [40/312], Loss: 0.0356\n",
            "Epoch [177/200], Step [60/312], Loss: 0.0165\n",
            "Epoch [177/200], Step [80/312], Loss: 0.0551\n",
            "Epoch [177/200], Step [100/312], Loss: 0.0135\n",
            "Epoch [177/200], Step [120/312], Loss: 0.0289\n",
            "Epoch [177/200], Step [140/312], Loss: 0.0175\n",
            "Epoch [177/200], Step [160/312], Loss: 0.0185\n",
            "Epoch [177/200], Step [180/312], Loss: 0.0227\n",
            "Epoch [177/200], Step [200/312], Loss: 0.0155\n",
            "Epoch [177/200], Step [220/312], Loss: 0.0306\n",
            "Epoch [177/200], Step [240/312], Loss: 0.0191\n",
            "Epoch [177/200], Step [260/312], Loss: 0.0097\n",
            "Epoch [177/200], Step [280/312], Loss: 0.0233\n",
            "Epoch [177/200], Step [300/312], Loss: 0.0124\n",
            "\n",
            "train-loss: 0.7074, train-acc: 99.9097\n",
            "validation loss: 3.0885, validation acc: 39.3502\n",
            "\n",
            "Epoch 178\n",
            "\n",
            "Epoch [178/200], Step [0/312], Loss: 0.0189\n",
            "Epoch [178/200], Step [20/312], Loss: 0.0487\n",
            "Epoch [178/200], Step [40/312], Loss: 0.0674\n",
            "Epoch [178/200], Step [60/312], Loss: 0.0187\n",
            "Epoch [178/200], Step [80/312], Loss: 0.0389\n",
            "Epoch [178/200], Step [100/312], Loss: 0.0097\n",
            "Epoch [178/200], Step [120/312], Loss: 0.0114\n",
            "Epoch [178/200], Step [140/312], Loss: 0.0226\n",
            "Epoch [178/200], Step [160/312], Loss: 0.0726\n",
            "Epoch [178/200], Step [180/312], Loss: 0.0615\n",
            "Epoch [178/200], Step [200/312], Loss: 0.0266\n",
            "Epoch [178/200], Step [220/312], Loss: 0.0278\n",
            "Epoch [178/200], Step [240/312], Loss: 0.0112\n",
            "Epoch [178/200], Step [260/312], Loss: 0.0134\n",
            "Epoch [178/200], Step [280/312], Loss: 0.0374\n",
            "Epoch [178/200], Step [300/312], Loss: 0.0165\n",
            "\n",
            "train-loss: 0.7036, train-acc: 99.9699\n",
            "validation loss: 3.0889, validation acc: 39.7513\n",
            "\n",
            "Epoch 179\n",
            "\n",
            "Epoch [179/200], Step [0/312], Loss: 0.0893\n",
            "Epoch [179/200], Step [20/312], Loss: 0.0391\n",
            "Epoch [179/200], Step [40/312], Loss: 0.0240\n",
            "Epoch [179/200], Step [60/312], Loss: 0.0191\n",
            "Epoch [179/200], Step [80/312], Loss: 0.0268\n",
            "Epoch [179/200], Step [100/312], Loss: 0.0143\n",
            "Epoch [179/200], Step [120/312], Loss: 0.0184\n",
            "Epoch [179/200], Step [140/312], Loss: 0.0207\n",
            "Epoch [179/200], Step [160/312], Loss: 0.0686\n",
            "Epoch [179/200], Step [180/312], Loss: 0.0235\n",
            "Epoch [179/200], Step [200/312], Loss: 0.0121\n",
            "Epoch [179/200], Step [220/312], Loss: 0.0146\n",
            "Epoch [179/200], Step [240/312], Loss: 0.0727\n",
            "Epoch [179/200], Step [260/312], Loss: 0.0160\n",
            "Epoch [179/200], Step [280/312], Loss: 0.0439\n",
            "Epoch [179/200], Step [300/312], Loss: 0.0291\n",
            "\n",
            "train-loss: 0.6998, train-acc: 99.9699\n",
            "validation loss: 3.0893, validation acc: 38.9491\n",
            "\n",
            "Epoch 180\n",
            "\n",
            "Epoch [180/200], Step [0/312], Loss: 0.0199\n",
            "Epoch [180/200], Step [20/312], Loss: 0.0304\n",
            "Epoch [180/200], Step [40/312], Loss: 0.0197\n",
            "Epoch [180/200], Step [60/312], Loss: 0.1052\n",
            "Epoch [180/200], Step [80/312], Loss: 0.0643\n",
            "Epoch [180/200], Step [100/312], Loss: 0.0195\n",
            "Epoch [180/200], Step [120/312], Loss: 0.0273\n",
            "Epoch [180/200], Step [140/312], Loss: 0.0212\n",
            "Epoch [180/200], Step [160/312], Loss: 0.0197\n",
            "Epoch [180/200], Step [180/312], Loss: 0.0298\n",
            "Epoch [180/200], Step [200/312], Loss: 0.0313\n",
            "Epoch [180/200], Step [220/312], Loss: 0.0320\n",
            "Epoch [180/200], Step [240/312], Loss: 0.0169\n",
            "Epoch [180/200], Step [260/312], Loss: 0.0590\n",
            "Epoch [180/200], Step [280/312], Loss: 0.0297\n",
            "Epoch [180/200], Step [300/312], Loss: 0.0199\n",
            "\n",
            "train-loss: 0.6961, train-acc: 99.9398\n",
            "validation loss: 3.0896, validation acc: 39.5507\n",
            "\n",
            "Epoch 181\n",
            "\n",
            "Epoch [181/200], Step [0/312], Loss: 0.0486\n",
            "Epoch [181/200], Step [20/312], Loss: 0.0124\n",
            "Epoch [181/200], Step [40/312], Loss: 0.0210\n",
            "Epoch [181/200], Step [60/312], Loss: 0.0134\n",
            "Epoch [181/200], Step [80/312], Loss: 0.0318\n",
            "Epoch [181/200], Step [100/312], Loss: 0.0317\n",
            "Epoch [181/200], Step [120/312], Loss: 0.0088\n",
            "Epoch [181/200], Step [140/312], Loss: 0.0187\n",
            "Epoch [181/200], Step [160/312], Loss: 0.0330\n",
            "Epoch [181/200], Step [180/312], Loss: 0.0106\n",
            "Epoch [181/200], Step [200/312], Loss: 0.0122\n",
            "Epoch [181/200], Step [220/312], Loss: 0.0384\n",
            "Epoch [181/200], Step [240/312], Loss: 0.0204\n",
            "Epoch [181/200], Step [260/312], Loss: 0.0156\n",
            "Epoch [181/200], Step [280/312], Loss: 0.0134\n",
            "Epoch [181/200], Step [300/312], Loss: 0.0387\n",
            "\n",
            "train-loss: 0.6924, train-acc: 99.9298\n",
            "validation loss: 3.0898, validation acc: 39.3101\n",
            "\n",
            "Epoch 182\n",
            "\n",
            "Epoch [182/200], Step [0/312], Loss: 0.0282\n",
            "Epoch [182/200], Step [20/312], Loss: 0.0124\n",
            "Epoch [182/200], Step [40/312], Loss: 0.0135\n",
            "Epoch [182/200], Step [60/312], Loss: 0.0162\n",
            "Epoch [182/200], Step [80/312], Loss: 0.0562\n",
            "Epoch [182/200], Step [100/312], Loss: 0.0168\n",
            "Epoch [182/200], Step [120/312], Loss: 0.0184\n",
            "Epoch [182/200], Step [140/312], Loss: 0.0102\n",
            "Epoch [182/200], Step [160/312], Loss: 0.0240\n",
            "Epoch [182/200], Step [180/312], Loss: 0.2864\n",
            "Epoch [182/200], Step [200/312], Loss: 0.0182\n",
            "Epoch [182/200], Step [220/312], Loss: 0.0383\n",
            "Epoch [182/200], Step [240/312], Loss: 0.0322\n",
            "Epoch [182/200], Step [260/312], Loss: 0.0257\n",
            "Epoch [182/200], Step [280/312], Loss: 0.0198\n",
            "Epoch [182/200], Step [300/312], Loss: 0.0552\n",
            "\n",
            "train-loss: 0.6888, train-acc: 99.8295\n",
            "validation loss: 3.0903, validation acc: 39.2298\n",
            "\n",
            "Epoch 183\n",
            "\n",
            "Epoch [183/200], Step [0/312], Loss: 0.0191\n",
            "Epoch [183/200], Step [20/312], Loss: 0.1110\n",
            "Epoch [183/200], Step [40/312], Loss: 0.0099\n",
            "Epoch [183/200], Step [60/312], Loss: 0.0179\n",
            "Epoch [183/200], Step [80/312], Loss: 0.0126\n",
            "Epoch [183/200], Step [100/312], Loss: 0.0429\n",
            "Epoch [183/200], Step [120/312], Loss: 0.0128\n",
            "Epoch [183/200], Step [140/312], Loss: 0.0397\n",
            "Epoch [183/200], Step [160/312], Loss: 0.0172\n",
            "Epoch [183/200], Step [180/312], Loss: 0.0463\n",
            "Epoch [183/200], Step [200/312], Loss: 0.0330\n",
            "Epoch [183/200], Step [220/312], Loss: 0.0295\n",
            "Epoch [183/200], Step [240/312], Loss: 0.0257\n",
            "Epoch [183/200], Step [260/312], Loss: 0.0148\n",
            "Epoch [183/200], Step [280/312], Loss: 0.0383\n",
            "Epoch [183/200], Step [300/312], Loss: 0.0360\n",
            "\n",
            "train-loss: 0.6852, train-acc: 99.9499\n",
            "validation loss: 3.0907, validation acc: 39.8315\n",
            "\n",
            "Epoch 184\n",
            "\n",
            "Epoch [184/200], Step [0/312], Loss: 0.0094\n",
            "Epoch [184/200], Step [20/312], Loss: 0.0429\n",
            "Epoch [184/200], Step [40/312], Loss: 0.0162\n",
            "Epoch [184/200], Step [60/312], Loss: 0.0893\n",
            "Epoch [184/200], Step [80/312], Loss: 0.0327\n",
            "Epoch [184/200], Step [100/312], Loss: 0.0320\n",
            "Epoch [184/200], Step [120/312], Loss: 0.0127\n",
            "Epoch [184/200], Step [140/312], Loss: 0.0330\n",
            "Epoch [184/200], Step [160/312], Loss: 0.0170\n",
            "Epoch [184/200], Step [180/312], Loss: 0.0198\n",
            "Epoch [184/200], Step [200/312], Loss: 0.0468\n",
            "Epoch [184/200], Step [220/312], Loss: 0.0184\n",
            "Epoch [184/200], Step [240/312], Loss: 0.0149\n",
            "Epoch [184/200], Step [260/312], Loss: 0.1583\n",
            "Epoch [184/200], Step [280/312], Loss: 0.0180\n",
            "Epoch [184/200], Step [300/312], Loss: 0.0366\n",
            "\n",
            "train-loss: 0.6816, train-acc: 99.9398\n",
            "validation loss: 3.0909, validation acc: 39.1897\n",
            "\n",
            "Epoch 185\n",
            "\n",
            "Epoch [185/200], Step [0/312], Loss: 0.0137\n",
            "Epoch [185/200], Step [20/312], Loss: 0.0117\n",
            "Epoch [185/200], Step [40/312], Loss: 0.0269\n",
            "Epoch [185/200], Step [60/312], Loss: 0.0243\n",
            "Epoch [185/200], Step [80/312], Loss: 0.0826\n",
            "Epoch [185/200], Step [100/312], Loss: 0.0299\n",
            "Epoch [185/200], Step [120/312], Loss: 0.0210\n",
            "Epoch [185/200], Step [140/312], Loss: 0.0380\n",
            "Epoch [185/200], Step [160/312], Loss: 0.0091\n",
            "Epoch [185/200], Step [180/312], Loss: 0.0831\n",
            "Epoch [185/200], Step [200/312], Loss: 0.0223\n",
            "Epoch [185/200], Step [220/312], Loss: 0.0167\n",
            "Epoch [185/200], Step [240/312], Loss: 0.0155\n",
            "Epoch [185/200], Step [260/312], Loss: 0.0189\n",
            "Epoch [185/200], Step [280/312], Loss: 0.0676\n",
            "Epoch [185/200], Step [300/312], Loss: 0.0223\n",
            "\n",
            "train-loss: 0.6781, train-acc: 99.8997\n",
            "validation loss: 3.0914, validation acc: 39.2298\n",
            "\n",
            "Epoch 186\n",
            "\n",
            "Epoch [186/200], Step [0/312], Loss: 0.0132\n",
            "Epoch [186/200], Step [20/312], Loss: 0.0266\n",
            "Epoch [186/200], Step [40/312], Loss: 0.0138\n",
            "Epoch [186/200], Step [60/312], Loss: 0.0434\n",
            "Epoch [186/200], Step [80/312], Loss: 0.0090\n",
            "Epoch [186/200], Step [100/312], Loss: 0.0516\n",
            "Epoch [186/200], Step [120/312], Loss: 0.0214\n",
            "Epoch [186/200], Step [140/312], Loss: 0.0108\n",
            "Epoch [186/200], Step [160/312], Loss: 0.0225\n",
            "Epoch [186/200], Step [180/312], Loss: 0.0124\n",
            "Epoch [186/200], Step [200/312], Loss: 0.0372\n",
            "Epoch [186/200], Step [220/312], Loss: 0.0853\n",
            "Epoch [186/200], Step [240/312], Loss: 0.0254\n",
            "Epoch [186/200], Step [260/312], Loss: 0.0199\n",
            "Epoch [186/200], Step [280/312], Loss: 0.0355\n",
            "Epoch [186/200], Step [300/312], Loss: 0.0380\n",
            "\n",
            "train-loss: 0.6746, train-acc: 99.9298\n",
            "validation loss: 3.0918, validation acc: 39.3502\n",
            "\n",
            "Epoch 187\n",
            "\n",
            "Epoch [187/200], Step [0/312], Loss: 0.0125\n",
            "Epoch [187/200], Step [20/312], Loss: 0.0232\n",
            "Epoch [187/200], Step [40/312], Loss: 0.0551\n",
            "Epoch [187/200], Step [60/312], Loss: 0.0747\n",
            "Epoch [187/200], Step [80/312], Loss: 0.0216\n",
            "Epoch [187/200], Step [100/312], Loss: 0.0202\n",
            "Epoch [187/200], Step [120/312], Loss: 0.0200\n",
            "Epoch [187/200], Step [140/312], Loss: 0.0139\n",
            "Epoch [187/200], Step [160/312], Loss: 0.0126\n",
            "Epoch [187/200], Step [180/312], Loss: 0.0094\n",
            "Epoch [187/200], Step [200/312], Loss: 0.0191\n",
            "Epoch [187/200], Step [220/312], Loss: 0.0219\n",
            "Epoch [187/200], Step [240/312], Loss: 0.0267\n",
            "Epoch [187/200], Step [260/312], Loss: 0.0128\n",
            "Epoch [187/200], Step [280/312], Loss: 0.0161\n",
            "Epoch [187/200], Step [300/312], Loss: 0.0231\n",
            "\n",
            "train-loss: 0.6711, train-acc: 99.9499\n",
            "validation loss: 3.0921, validation acc: 39.1897\n",
            "\n",
            "Epoch 188\n",
            "\n",
            "Epoch [188/200], Step [0/312], Loss: 0.0101\n",
            "Epoch [188/200], Step [20/312], Loss: 0.0382\n",
            "Epoch [188/200], Step [40/312], Loss: 0.0666\n",
            "Epoch [188/200], Step [60/312], Loss: 0.0330\n",
            "Epoch [188/200], Step [80/312], Loss: 0.0189\n",
            "Epoch [188/200], Step [100/312], Loss: 0.0163\n",
            "Epoch [188/200], Step [120/312], Loss: 0.0134\n",
            "Epoch [188/200], Step [140/312], Loss: 0.0553\n",
            "Epoch [188/200], Step [160/312], Loss: 0.0174\n",
            "Epoch [188/200], Step [180/312], Loss: 0.0382\n",
            "Epoch [188/200], Step [200/312], Loss: 0.0227\n",
            "Epoch [188/200], Step [220/312], Loss: 0.0138\n",
            "Epoch [188/200], Step [240/312], Loss: 0.0204\n",
            "Epoch [188/200], Step [260/312], Loss: 0.0079\n",
            "Epoch [188/200], Step [280/312], Loss: 0.0144\n",
            "Epoch [188/200], Step [300/312], Loss: 0.0172\n",
            "\n",
            "train-loss: 0.6677, train-acc: 99.8997\n",
            "validation loss: 3.0926, validation acc: 39.3502\n",
            "\n",
            "Epoch 189\n",
            "\n",
            "Epoch [189/200], Step [0/312], Loss: 0.0137\n",
            "Epoch [189/200], Step [20/312], Loss: 0.0149\n",
            "Epoch [189/200], Step [40/312], Loss: 0.0181\n",
            "Epoch [189/200], Step [60/312], Loss: 0.0157\n",
            "Epoch [189/200], Step [80/312], Loss: 0.0136\n",
            "Epoch [189/200], Step [100/312], Loss: 0.0204\n",
            "Epoch [189/200], Step [120/312], Loss: 0.0244\n",
            "Epoch [189/200], Step [140/312], Loss: 0.0104\n",
            "Epoch [189/200], Step [160/312], Loss: 0.0491\n",
            "Epoch [189/200], Step [180/312], Loss: 0.0207\n",
            "Epoch [189/200], Step [200/312], Loss: 0.0136\n",
            "Epoch [189/200], Step [220/312], Loss: 0.0234\n",
            "Epoch [189/200], Step [240/312], Loss: 0.0219\n",
            "Epoch [189/200], Step [260/312], Loss: 0.0141\n",
            "Epoch [189/200], Step [280/312], Loss: 0.0161\n",
            "Epoch [189/200], Step [300/312], Loss: 0.0370\n",
            "\n",
            "train-loss: 0.6643, train-acc: 99.9298\n",
            "validation loss: 3.0931, validation acc: 39.3101\n",
            "\n",
            "Epoch 190\n",
            "\n",
            "Epoch [190/200], Step [0/312], Loss: 0.0144\n",
            "Epoch [190/200], Step [20/312], Loss: 0.0500\n",
            "Epoch [190/200], Step [40/312], Loss: 0.0167\n",
            "Epoch [190/200], Step [60/312], Loss: 0.0163\n",
            "Epoch [190/200], Step [80/312], Loss: 0.0151\n",
            "Epoch [190/200], Step [100/312], Loss: 0.0344\n",
            "Epoch [190/200], Step [120/312], Loss: 0.0462\n",
            "Epoch [190/200], Step [140/312], Loss: 0.0091\n",
            "Epoch [190/200], Step [160/312], Loss: 0.0093\n",
            "Epoch [190/200], Step [180/312], Loss: 0.0203\n",
            "Epoch [190/200], Step [200/312], Loss: 0.0166\n",
            "Epoch [190/200], Step [220/312], Loss: 0.1294\n",
            "Epoch [190/200], Step [240/312], Loss: 0.0132\n",
            "Epoch [190/200], Step [260/312], Loss: 0.0114\n",
            "Epoch [190/200], Step [280/312], Loss: 0.0288\n",
            "Epoch [190/200], Step [300/312], Loss: 0.0154\n",
            "\n",
            "train-loss: 0.6610, train-acc: 99.9198\n",
            "validation loss: 3.0934, validation acc: 39.5106\n",
            "\n",
            "Epoch 191\n",
            "\n",
            "Epoch [191/200], Step [0/312], Loss: 0.0195\n",
            "Epoch [191/200], Step [20/312], Loss: 0.0136\n",
            "Epoch [191/200], Step [40/312], Loss: 0.0115\n",
            "Epoch [191/200], Step [60/312], Loss: 0.0148\n",
            "Epoch [191/200], Step [80/312], Loss: 0.0261\n",
            "Epoch [191/200], Step [100/312], Loss: 0.0242\n",
            "Epoch [191/200], Step [120/312], Loss: 0.0351\n",
            "Epoch [191/200], Step [140/312], Loss: 0.0397\n",
            "Epoch [191/200], Step [160/312], Loss: 0.0256\n",
            "Epoch [191/200], Step [180/312], Loss: 0.0303\n",
            "Epoch [191/200], Step [200/312], Loss: 0.0142\n",
            "Epoch [191/200], Step [220/312], Loss: 0.0095\n",
            "Epoch [191/200], Step [240/312], Loss: 0.0115\n",
            "Epoch [191/200], Step [260/312], Loss: 0.0144\n",
            "Epoch [191/200], Step [280/312], Loss: 0.0360\n",
            "Epoch [191/200], Step [300/312], Loss: 0.0296\n",
            "\n",
            "train-loss: 0.6576, train-acc: 99.9398\n",
            "validation loss: 3.0938, validation acc: 39.2298\n",
            "\n",
            "Epoch 192\n",
            "\n",
            "Epoch [192/200], Step [0/312], Loss: 0.0403\n",
            "Epoch [192/200], Step [20/312], Loss: 0.0159\n",
            "Epoch [192/200], Step [40/312], Loss: 0.0121\n",
            "Epoch [192/200], Step [60/312], Loss: 0.0266\n",
            "Epoch [192/200], Step [80/312], Loss: 0.0184\n",
            "Epoch [192/200], Step [100/312], Loss: 0.0240\n",
            "Epoch [192/200], Step [120/312], Loss: 0.0167\n",
            "Epoch [192/200], Step [140/312], Loss: 0.0163\n",
            "Epoch [192/200], Step [160/312], Loss: 0.0373\n",
            "Epoch [192/200], Step [180/312], Loss: 0.0108\n",
            "Epoch [192/200], Step [200/312], Loss: 0.0284\n",
            "Epoch [192/200], Step [220/312], Loss: 0.0272\n",
            "Epoch [192/200], Step [240/312], Loss: 0.0269\n",
            "Epoch [192/200], Step [260/312], Loss: 0.0180\n",
            "Epoch [192/200], Step [280/312], Loss: 0.0130\n",
            "Epoch [192/200], Step [300/312], Loss: 0.0073\n",
            "\n",
            "train-loss: 0.6543, train-acc: 99.8997\n",
            "validation loss: 3.0940, validation acc: 39.4705\n",
            "\n",
            "Epoch 193\n",
            "\n",
            "Epoch [193/200], Step [0/312], Loss: 0.0104\n",
            "Epoch [193/200], Step [20/312], Loss: 0.0113\n",
            "Epoch [193/200], Step [40/312], Loss: 0.0288\n",
            "Epoch [193/200], Step [60/312], Loss: 0.0247\n",
            "Epoch [193/200], Step [80/312], Loss: 0.0197\n",
            "Epoch [193/200], Step [100/312], Loss: 0.0136\n",
            "Epoch [193/200], Step [120/312], Loss: 0.0143\n",
            "Epoch [193/200], Step [140/312], Loss: 0.0951\n",
            "Epoch [193/200], Step [160/312], Loss: 0.0139\n",
            "Epoch [193/200], Step [180/312], Loss: 0.0155\n",
            "Epoch [193/200], Step [200/312], Loss: 0.0266\n",
            "Epoch [193/200], Step [220/312], Loss: 0.0239\n",
            "Epoch [193/200], Step [240/312], Loss: 0.0107\n",
            "Epoch [193/200], Step [260/312], Loss: 0.0159\n",
            "Epoch [193/200], Step [280/312], Loss: 0.0246\n",
            "Epoch [193/200], Step [300/312], Loss: 0.0227\n",
            "\n",
            "train-loss: 0.6511, train-acc: 99.9398\n",
            "validation loss: 3.0944, validation acc: 39.6711\n",
            "\n",
            "Epoch 194\n",
            "\n",
            "Epoch [194/200], Step [0/312], Loss: 0.0173\n",
            "Epoch [194/200], Step [20/312], Loss: 0.0577\n",
            "Epoch [194/200], Step [40/312], Loss: 0.0101\n",
            "Epoch [194/200], Step [60/312], Loss: 0.0153\n",
            "Epoch [194/200], Step [80/312], Loss: 0.0086\n",
            "Epoch [194/200], Step [100/312], Loss: 0.0221\n",
            "Epoch [194/200], Step [120/312], Loss: 0.0193\n",
            "Epoch [194/200], Step [140/312], Loss: 0.0489\n",
            "Epoch [194/200], Step [160/312], Loss: 0.0139\n",
            "Epoch [194/200], Step [180/312], Loss: 0.0306\n",
            "Epoch [194/200], Step [200/312], Loss: 0.0210\n",
            "Epoch [194/200], Step [220/312], Loss: 0.0118\n",
            "Epoch [194/200], Step [240/312], Loss: 0.0183\n",
            "Epoch [194/200], Step [260/312], Loss: 0.0566\n",
            "Epoch [194/200], Step [280/312], Loss: 0.0164\n",
            "Epoch [194/200], Step [300/312], Loss: 0.0306\n",
            "\n",
            "train-loss: 0.6478, train-acc: 99.9599\n",
            "validation loss: 3.0948, validation acc: 38.3073\n",
            "\n",
            "Epoch 195\n",
            "\n",
            "Epoch [195/200], Step [0/312], Loss: 0.0261\n",
            "Epoch [195/200], Step [20/312], Loss: 0.0334\n",
            "Epoch [195/200], Step [40/312], Loss: 0.0139\n",
            "Epoch [195/200], Step [60/312], Loss: 0.0154\n",
            "Epoch [195/200], Step [80/312], Loss: 0.0232\n",
            "Epoch [195/200], Step [100/312], Loss: 0.1152\n",
            "Epoch [195/200], Step [120/312], Loss: 0.0316\n",
            "Epoch [195/200], Step [140/312], Loss: 0.0568\n",
            "Epoch [195/200], Step [160/312], Loss: 0.0450\n",
            "Epoch [195/200], Step [180/312], Loss: 0.0090\n",
            "Epoch [195/200], Step [200/312], Loss: 0.0209\n",
            "Epoch [195/200], Step [220/312], Loss: 0.0171\n",
            "Epoch [195/200], Step [240/312], Loss: 0.0102\n",
            "Epoch [195/200], Step [260/312], Loss: 0.0117\n",
            "Epoch [195/200], Step [280/312], Loss: 0.0102\n",
            "Epoch [195/200], Step [300/312], Loss: 0.0141\n",
            "\n",
            "train-loss: 0.6447, train-acc: 99.9298\n",
            "validation loss: 3.0951, validation acc: 39.2298\n",
            "\n",
            "Epoch 196\n",
            "\n",
            "Epoch [196/200], Step [0/312], Loss: 0.0150\n",
            "Epoch [196/200], Step [20/312], Loss: 0.0286\n",
            "Epoch [196/200], Step [40/312], Loss: 0.0157\n",
            "Epoch [196/200], Step [60/312], Loss: 0.0345\n",
            "Epoch [196/200], Step [80/312], Loss: 0.0111\n",
            "Epoch [196/200], Step [100/312], Loss: 0.0168\n",
            "Epoch [196/200], Step [120/312], Loss: 0.0239\n",
            "Epoch [196/200], Step [140/312], Loss: 0.0166\n",
            "Epoch [196/200], Step [160/312], Loss: 0.0194\n",
            "Epoch [196/200], Step [180/312], Loss: 0.0108\n",
            "Epoch [196/200], Step [200/312], Loss: 0.0100\n",
            "Epoch [196/200], Step [220/312], Loss: 0.0203\n",
            "Epoch [196/200], Step [240/312], Loss: 0.0143\n",
            "Epoch [196/200], Step [260/312], Loss: 0.0148\n",
            "Epoch [196/200], Step [280/312], Loss: 0.0175\n",
            "Epoch [196/200], Step [300/312], Loss: 0.0207\n",
            "\n",
            "train-loss: 0.6415, train-acc: 99.9499\n",
            "validation loss: 3.0954, validation acc: 40.0722\n",
            "\n",
            "Epoch 197\n",
            "\n",
            "Epoch [197/200], Step [0/312], Loss: 0.0289\n",
            "Epoch [197/200], Step [20/312], Loss: 0.0162\n",
            "Epoch [197/200], Step [40/312], Loss: 0.1062\n",
            "Epoch [197/200], Step [60/312], Loss: 0.0126\n",
            "Epoch [197/200], Step [80/312], Loss: 0.0228\n",
            "Epoch [197/200], Step [100/312], Loss: 0.0119\n",
            "Epoch [197/200], Step [120/312], Loss: 0.0310\n",
            "Epoch [197/200], Step [140/312], Loss: 0.0270\n",
            "Epoch [197/200], Step [160/312], Loss: 0.0296\n",
            "Epoch [197/200], Step [180/312], Loss: 0.0543\n",
            "Epoch [197/200], Step [200/312], Loss: 0.0162\n",
            "Epoch [197/200], Step [220/312], Loss: 0.0172\n",
            "Epoch [197/200], Step [240/312], Loss: 0.0259\n",
            "Epoch [197/200], Step [260/312], Loss: 0.0111\n",
            "Epoch [197/200], Step [280/312], Loss: 0.0243\n",
            "Epoch [197/200], Step [300/312], Loss: 0.0131\n",
            "\n",
            "train-loss: 0.6384, train-acc: 99.9198\n",
            "validation loss: 3.0957, validation acc: 39.7513\n",
            "\n",
            "Epoch 198\n",
            "\n",
            "Epoch [198/200], Step [0/312], Loss: 0.0104\n",
            "Epoch [198/200], Step [20/312], Loss: 0.0107\n",
            "Epoch [198/200], Step [40/312], Loss: 0.0155\n",
            "Epoch [198/200], Step [60/312], Loss: 0.0224\n",
            "Epoch [198/200], Step [80/312], Loss: 0.0308\n",
            "Epoch [198/200], Step [100/312], Loss: 0.0143\n",
            "Epoch [198/200], Step [120/312], Loss: 0.0130\n",
            "Epoch [198/200], Step [140/312], Loss: 0.0160\n",
            "Epoch [198/200], Step [160/312], Loss: 0.0426\n",
            "Epoch [198/200], Step [180/312], Loss: 0.0179\n",
            "Epoch [198/200], Step [200/312], Loss: 0.0419\n",
            "Epoch [198/200], Step [220/312], Loss: 0.0217\n",
            "Epoch [198/200], Step [240/312], Loss: 0.0399\n",
            "Epoch [198/200], Step [260/312], Loss: 0.0215\n",
            "Epoch [198/200], Step [280/312], Loss: 0.0274\n",
            "Epoch [198/200], Step [300/312], Loss: 0.0211\n",
            "\n",
            "train-loss: 0.6353, train-acc: 99.9499\n",
            "validation loss: 3.0959, validation acc: 38.9089\n",
            "\n",
            "Epoch 199\n",
            "\n",
            "Epoch [199/200], Step [0/312], Loss: 0.0128\n",
            "Epoch [199/200], Step [20/312], Loss: 0.0107\n",
            "Epoch [199/200], Step [40/312], Loss: 0.0199\n",
            "Epoch [199/200], Step [60/312], Loss: 0.0455\n",
            "Epoch [199/200], Step [80/312], Loss: 0.0501\n",
            "Epoch [199/200], Step [100/312], Loss: 0.0322\n",
            "Epoch [199/200], Step [120/312], Loss: 0.0138\n",
            "Epoch [199/200], Step [140/312], Loss: 0.0152\n",
            "Epoch [199/200], Step [160/312], Loss: 0.0155\n",
            "Epoch [199/200], Step [180/312], Loss: 0.0159\n",
            "Epoch [199/200], Step [200/312], Loss: 0.0216\n",
            "Epoch [199/200], Step [220/312], Loss: 0.0101\n",
            "Epoch [199/200], Step [240/312], Loss: 0.0128\n",
            "Epoch [199/200], Step [260/312], Loss: 0.0067\n",
            "Epoch [199/200], Step [280/312], Loss: 0.0179\n",
            "Epoch [199/200], Step [300/312], Loss: 0.0232\n",
            "\n",
            "train-loss: 0.6322, train-acc: 99.9198\n",
            "validation loss: 3.0963, validation acc: 39.3903\n",
            "\n",
            "Epoch 200\n",
            "\n",
            "Epoch [200/200], Step [0/312], Loss: 0.0291\n",
            "Epoch [200/200], Step [20/312], Loss: 0.0097\n",
            "Epoch [200/200], Step [40/312], Loss: 0.0200\n",
            "Epoch [200/200], Step [60/312], Loss: 0.0161\n",
            "Epoch [200/200], Step [80/312], Loss: 0.0325\n",
            "Epoch [200/200], Step [100/312], Loss: 0.0292\n",
            "Epoch [200/200], Step [120/312], Loss: 0.0186\n",
            "Epoch [200/200], Step [140/312], Loss: 0.0208\n",
            "Epoch [200/200], Step [160/312], Loss: 0.0188\n",
            "Epoch [200/200], Step [180/312], Loss: 0.0124\n",
            "Epoch [200/200], Step [200/312], Loss: 0.0203\n",
            "Epoch [200/200], Step [220/312], Loss: 0.0641\n",
            "Epoch [200/200], Step [240/312], Loss: 0.0316\n",
            "Epoch [200/200], Step [260/312], Loss: 0.0642\n",
            "Epoch [200/200], Step [280/312], Loss: 0.0123\n",
            "Epoch [200/200], Step [300/312], Loss: 0.0139\n",
            "\n",
            "train-loss: 0.6292, train-acc: 99.9499\n",
            "validation loss: 3.0965, validation acc: 39.9519\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title(\"Train-Validation Accuracy\")\n",
        "plt.plot(train_acc, label='train')\n",
        "plt.plot(val_acc, label='validation')\n",
        "plt.xlabel('num_epochs', fontsize=12)\n",
        "plt.ylabel('accuracy', fontsize=12)\n",
        "plt.legend(loc='best')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "ctv-jUjEJIkF",
        "outputId": "7951fb5b-1491-4292-d7dd-1f32da688159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1f6c903710>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "pred_list=[]\n",
        "true_list=[]\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for data in valiloader:\n",
        "    images, labels = data\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    pred_list.append(predicted.cpu().numpy())\n",
        "    true_list.append(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOMDbWCwWd4z",
        "outputId": "37e82636-142b-4b68-94d7-f35faf556d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 34 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list=list(np.concatenate(pred_list).flat)\n",
        "true_list=list(np.concatenate(true_list).flat)\n",
        "print(pred_list)\n",
        "print(true_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sftb6_TKYk1a",
        "outputId": "aa691855-2ef2-463d-b67d-a7e38a5360e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 8, 1, 9, 8, 4, 3, 0, 5, 2, 2, 1, 3, 7, 4, 8, 9, 7, 9, 2, 7, 7, 2, 4, 5, 1, 6, 3, 8, 9, 9, 7, 0, 4, 1, 1, 6, 8, 4, 1, 2, 7, 3, 9, 5, 8, 5, 8, 7, 3, 9, 6, 5, 2, 1, 9, 5, 9, 2, 8, 7, 0, 9, 4, 2, 2, 3, 6, 6, 4, 5, 5, 4, 3, 9, 4, 5, 6, 3, 1, 3, 3, 2, 8, 1, 2, 2, 8, 8, 6, 3, 7, 7, 9, 6, 9, 1, 1, 4, 5, 2, 5, 9, 1, 4, 6, 0, 6, 8, 6, 3, 2, 3, 6, 8, 7, 6, 4, 9, 1, 0, 3, 8, 6, 7, 3, 9, 9, 0, 4, 6, 8, 1, 0, 5, 5, 8, 7, 3, 7, 2, 3, 6, 8, 1, 9, 2, 4, 9, 3, 4, 2, 8, 1, 7, 0, 7, 3, 9, 6, 2, 9, 8, 2, 1, 6, 7, 1, 1, 6, 3, 3, 4, 3, 5, 9, 6, 5, 3, 7, 2, 6, 4, 7, 8, 6, 9, 2, 9, 4, 7, 1, 6, 3, 2, 1, 5, 5, 0, 6, 8, 6, 2, 7, 5, 9, 6, 9, 7, 6, 7, 2, 9, 1, 1, 8, 8, 5, 7, 4, 4, 7, 3, 1, 9, 3, 4, 2, 9, 5, 6, 9, 9, 6, 9, 6, 7, 6, 2, 8, 4, 1, 2, 7, 9, 8, 5, 9, 1, 3, 2, 4, 7, 2, 7, 3, 2, 8, 7, 7, 8, 0, 9, 1, 3, 9, 0, 0, 9, 2, 6, 7, 4, 7, 3, 9, 5, 9, 4, 8, 4, 2, 4, 6, 4, 9, 5, 5, 7, 0, 6, 9, 4, 8, 9, 2, 0, 7, 1, 1, 2, 6, 2, 4, 5, 7, 1, 7, 8, 4, 1, 7, 8, 8, 3, 9, 1, 6, 2, 8, 6, 4, 2, 8, 4, 1, 9, 5, 6, 8, 5, 1, 7, 4, 1, 2, 9, 2, 7, 3, 6, 2, 7, 8, 7, 3, 2, 6, 9, 7, 4, 3, 8, 7, 2, 0, 5, 3, 7, 5, 2, 7, 9, 6, 6, 5, 2, 1, 8, 4, 8, 4, 4, 9, 9, 9, 3, 9, 0, 0, 4, 5, 2, 1, 3, 7, 9, 2, 1, 8, 8, 7, 9, 7, 4, 6, 4, 4, 3, 1, 5, 3, 5, 6, 0, 6, 4, 9, 4, 1, 3, 1, 2, 3, 9, 8, 3, 8, 1, 6, 8, 3, 3, 2, 9, 9, 4, 5, 2, 7, 8, 8, 2, 5, 2, 3, 7, 1, 9, 5, 7, 0, 6, 0, 5, 9, 9, 4, 9, 8, 4, 7, 8, 8, 5, 2, 7, 2, 6, 4, 2, 4, 0, 7, 6, 0, 9, 1, 6, 5, 1, 3, 6, 5, 6, 4, 8, 3, 0, 6, 7, 6, 7, 0, 3, 9, 3, 8, 2, 1, 6, 2, 8, 5, 3, 9, 2, 1, 7, 0]\n",
            "[6, 8, 9, 9, 2, 6, 3, 0, 5, 2, 0, 4, 9, 3, 4, 0, 4, 1, 0, 5, 1, 3, 1, 4, 9, 1, 4, 1, 5, 4, 4, 7, 9, 0, 1, 1, 6, 5, 1, 7, 3, 0, 5, 9, 8, 8, 6, 8, 7, 7, 6, 0, 9, 1, 0, 1, 8, 7, 9, 7, 7, 6, 1, 9, 2, 2, 9, 3, 3, 0, 0, 5, 1, 3, 0, 2, 8, 9, 9, 7, 4, 4, 2, 8, 1, 2, 2, 2, 5, 4, 7, 7, 0, 0, 0, 5, 1, 7, 6, 5, 2, 4, 7, 5, 5, 8, 0, 7, 5, 3, 8, 2, 0, 7, 8, 1, 1, 6, 6, 0, 9, 4, 8, 6, 5, 8, 9, 3, 0, 4, 3, 6, 6, 0, 3, 4, 8, 7, 9, 0, 2, 3, 3, 6, 4, 7, 2, 1, 9, 7, 4, 2, 0, 0, 1, 0, 6, 4, 2, 3, 9, 5, 8, 2, 9, 3, 3, 9, 1, 6, 3, 9, 7, 6, 2, 9, 6, 8, 5, 7, 8, 4, 0, 3, 9, 4, 9, 2, 0, 6, 1, 6, 3, 5, 2, 7, 5, 5, 1, 5, 5, 6, 6, 6, 8, 1, 6, 4, 9, 9, 4, 2, 9, 8, 4, 3, 2, 8, 7, 7, 3, 0, 3, 1, 9, 8, 8, 5, 7, 5, 2, 3, 4, 7, 1, 1, 0, 7, 2, 8, 2, 7, 9, 7, 3, 4, 0, 9, 0, 1, 2, 4, 3, 5, 7, 8, 2, 8, 7, 7, 8, 6, 8, 1, 3, 8, 9, 5, 5, 3, 3, 5, 2, 4, 8, 0, 2, 5, 5, 8, 4, 2, 7, 4, 0, 0, 5, 6, 7, 2, 3, 2, 9, 6, 9, 5, 0, 4, 1, 9, 2, 7, 4, 7, 4, 7, 9, 6, 8, 4, 1, 7, 0, 5, 8, 8, 6, 4, 2, 0, 0, 1, 2, 8, 3, 7, 6, 4, 6, 6, 5, 1, 4, 8, 0, 2, 9, 2, 6, 1, 6, 2, 7, 5, 1, 3, 7, 6, 3, 3, 4, 3, 1, 5, 8, 9, 5, 3, 1, 5, 8, 0, 0, 6, 6, 3, 2, 1, 6, 1, 9, 3, 8, 0, 0, 9, 3, 4, 1, 3, 7, 5, 4, 1, 6, 2, 9, 5, 1, 0, 8, 3, 5, 6, 4, 6, 1, 1, 3, 3, 0, 4, 5, 1, 4, 6, 4, 9, 4, 4, 8, 4, 2, 6, 9, 8, 3, 8, 7, 6, 8, 3, 5, 2, 4, 3, 1, 5, 6, 6, 8, 8, 8, 3, 2, 6, 7, 0, 7, 5, 7, 4, 6, 8, 2, 5, 9, 5, 9, 8, 7, 7, 9, 3, 5, 2, 1, 2, 9, 1, 2, 7, 9, 7, 3, 0, 1, 5, 6, 2, 7, 2, 6, 5, 4, 8, 2, 0, 9, 8, 3, 4, 7, 9, 3, 0, 0, 2, 1, 0, 4, 5, 5, 8, 1, 9, 9, 1, 0, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "array=confusion_matrix(true_list, pred_list)\n",
        "print(array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG3WssbhYsRj",
        "outputId": "6d328d54-2d0c-46b1-f060-c9331d0083e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7  7  1  3  4  3  3  7  5 10]\n",
            " [ 2 15  3  4  9  0  3  8  1  5]\n",
            " [ 1  0 32  1  3  4  1  1  5  2]\n",
            " [ 1  1  2 14  3  3 11  8  2  5]\n",
            " [ 2  5  2  5 11  4  9  4  1  7]\n",
            " [ 1  2  6  4  3 16  1  3  8  6]\n",
            " [ 3  3  2  4  4  2 18  6  5  3]\n",
            " [ 0  8  1  3  7  0  5 20  1  5]\n",
            " [ 1  1  4  7  4  7  2  0 21  3]\n",
            " [ 7  5  4  5  2  2  3  1  3 18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dummy classifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "X = true_list\n",
        "y = true_list\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\")  #or most_frequent,stratified\n",
        "dummy_clf.fit(X, y)\n",
        "print(\"dummy classifier result\\n\")\n",
        "#print(dummy_clf.score(X, y))\n",
        "x_dummy=dummy_clf.predict(X)\n",
        "print(classification_report(true_list, x_dummy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHFnoksBgwbr",
        "outputId": "3b2a259f-cfa3-4a9d-dfa0-5b46805ddbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dummy classifier result\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      1.00      0.18        50\n",
            "           1       0.00      0.00      0.00        50\n",
            "           2       0.00      0.00      0.00        50\n",
            "           3       0.00      0.00      0.00        50\n",
            "           4       0.00      0.00      0.00        50\n",
            "           5       0.00      0.00      0.00        50\n",
            "           6       0.00      0.00      0.00        50\n",
            "           7       0.00      0.00      0.00        50\n",
            "           8       0.00      0.00      0.00        50\n",
            "           9       0.00      0.00      0.00        50\n",
            "\n",
            "    accuracy                           0.10       500\n",
            "   macro avg       0.01      0.10      0.02       500\n",
            "weighted avg       0.01      0.10      0.02       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the confusion_matrix, and classification_report\n",
        "import seaborn as sn\n",
        "df_cm = pd.DataFrame(array, range(10), range(10))\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
        "plt.xlabel('predicted label', fontsize=18)\n",
        "plt.ylabel('true label', fontsize=16)\n",
        "plt.show()\n",
        "print(\"\\n\\nclassification report:\\n\\n\"+classification_report(true_list, pred_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "KvLIW39ZajAj",
        "outputId": "c6f57e31-1fa3-475c-f8a7-7d9582276100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.14      0.19        50\n",
            "           1       0.32      0.30      0.31        50\n",
            "           2       0.56      0.64      0.60        50\n",
            "           3       0.28      0.28      0.28        50\n",
            "           4       0.22      0.22      0.22        50\n",
            "           5       0.39      0.32      0.35        50\n",
            "           6       0.32      0.36      0.34        50\n",
            "           7       0.34      0.40      0.37        50\n",
            "           8       0.40      0.42      0.41        50\n",
            "           9       0.28      0.36      0.32        50\n",
            "\n",
            "    accuracy                           0.34       500\n",
            "   macro avg       0.34      0.34      0.34       500\n",
            "weighted avg       0.34      0.34      0.34       500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}